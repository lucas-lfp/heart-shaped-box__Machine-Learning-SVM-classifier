{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0d6a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\foulo\\anaconda\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.text import Text\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro, mannwhitneyu, chi2_contingency, spearmanr\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, GridSearchCV, cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d9617d",
   "metadata": {},
   "source": [
    "<h1>CSS Stylesheet</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aee5ad",
   "metadata": {},
   "source": [
    "<style>\n",
    "    /* Define CSS styles here */\n",
    "    h1 {\n",
    "        color: blue;\n",
    "    }\n",
    "    p {\n",
    "        font-size: 16px;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    .custom-class {\n",
    "        background-color: yellow;\n",
    "        border: 1px solid black;\n",
    "        padding: 10px;\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839908ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 {\n",
       "    color: black;\n",
       "    font-family: 'Segoe UI', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif;\n",
       "    font-size: 35px !important;\n",
       "    padding-bottom: 10px;\n",
       "    padding-top: 10px;\n",
       "    border-bottom: 5px solid navy;\n",
       "    border-top: 5px solid navy;\n",
       "    font-variant: small-caps;\n",
       "    text-align: center;\n",
       "    margin-bottom: 25px;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "    color : dimgray;    \n",
       "}\n",
       "\n",
       ".all {\n",
       "/*This class is the default class for <div> so it does not interact with jupyter notebook structure */\n",
       "text-align: justify;\n",
       "font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "margin-right: 2px;\n",
       "}\n",
       "\n",
       ".titlediv {\n",
       "    /*This class is the default class for <div> so it does not interact with jupyter notebook structure */\n",
       "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "    border: 5px solid navy;\n",
       "    padding-top: 30px;\n",
       "    padding-bottom: 30px;\n",
       "    padding-left: 5px;\n",
       "    padding-right: 5px;\n",
       "    margin-bottom: 10px;\n",
       "    }\n",
       "\n",
       ".titlediv_2 {\n",
       "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "    text-align: center !important;\n",
       "    }\n",
       "\n",
       ".titlep {\n",
       "    font-size : 50px;\n",
       "    text-align: center !important;\n",
       "    font-variant: small-caps;\n",
       "}   \n",
       "\n",
       ".titlep_2 {\n",
       "    font-size : 30px;\n",
       "    text-align: center !important;\n",
       "    margin-top: 10px;\n",
       "}  \n",
       "\n",
       ".title_sp {\n",
       "    text-align: center !important;\n",
       "    font-size: 20px !important;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".signature {\n",
       "    margin-top: 60px;\n",
       "    padding-top: 15px;\n",
       "    border-top: 2px solid black;\n",
       "    text-align: right !important;\n",
       "    font-family:'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "}\n",
       "\n",
       ".obj {\n",
       "    text-align: justify;\n",
       "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "    margin-right: 100px;\n",
       "    border: 2px solid lightcoral;\n",
       "    padding: 15px;\n",
       "}\n",
       "\n",
       "p {\n",
       "    text-align: justify !important;\n",
       "}\n",
       "\n",
       ".intro {\n",
       "    font-style: italic;\n",
       "}\n",
       "\n",
       ".conclusion {\n",
       "    border: 5px solid navy;\n",
       "    margin-top: 50px;\n",
       "    padding: 10px;\n",
       "}\n",
       "\n",
       ".conclusion h3 {\n",
       "    color:black;\n",
       "    font-variant: small-caps;\n",
       "}\n",
       "\n",
       ".small-caps {\n",
       "    font-variant: small-caps;\n",
       "}\n",
       "\n",
       "table {\n",
       "    margin-right: 10px !important;\n",
       "    font-size: 14px !important;\n",
       "    text-align: center !important;\n",
       "}\n",
       "\n",
       ".table_1 {\n",
       "    border: 3px solid black !important;\n",
       "    width: 100% !important;\n",
       "}\n",
       "\n",
       ".table_1 td, .table_1, th {\n",
       "    border: 1px solid black;\n",
       "}\n",
       "\n",
       ".head_tr {\n",
       "    background-color: dimgray !important;\n",
       "    color: white;\n",
       "    border: 3px solid black !important;\n",
       "}\n",
       "\n",
       "th, td {\n",
       "    padding: 8px !important;\n",
       "    text-align: center !important;\n",
       "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "    border: 1.5px solid black; \n",
       "}\n",
       "\n",
       "\n",
       ".recap_table td, .recap_table th {\n",
       "    height: 50px;\n",
       "    width: 180px;\n",
       "}\n",
       "\n",
       ".recap_table th {\n",
       "    font-size: 18px;\n",
       "}\n",
       "\n",
       ".col_1 {\n",
       "    text-align: right !important;\n",
       "    width: 50 !important;\n",
       "}\n",
       "\n",
       ".col_group {\n",
       "    width: 200px;\n",
       "    background-color: #d4c9df !important;\n",
       "    text-align: center;\n",
       "    height: 5px !important;\n",
       "    font-size: 16 !important;\n",
       "    font-weight: bold;\n",
       "}\n",
       "\n",
       ".columns {\n",
       "    display: flex;\n",
       "    flex-wrap: wrap;\n",
       "}\n",
       "\n",
       ".column {\n",
       "    flex: 1;\n",
       "    padding: 10px;\n",
       "    text-align: justify;\n",
       "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
       "    margin-right: 50px;\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "css_style = \"\"\"\n",
    "h1 {\n",
    "    color: black;\n",
    "    font-family: 'Segoe UI', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif;\n",
    "    font-size: 35px !important;\n",
    "    padding-bottom: 10px;\n",
    "    padding-top: 10px;\n",
    "    border-bottom: 5px solid navy;\n",
    "    border-top: 5px solid navy;\n",
    "    font-variant: small-caps;\n",
    "    text-align: center;\n",
    "    margin-bottom: 25px;\n",
    "}\n",
    "\n",
    "h3 {\n",
    "    color : dimgray;    \n",
    "}\n",
    "\n",
    ".all {\n",
    "/*This class is the default class for <div> so it does not interact with jupyter notebook structure */\n",
    "text-align: justify;\n",
    "font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "margin-right: 2px;\n",
    "}\n",
    "\n",
    ".titlediv {\n",
    "    /*This class is the default class for <div> so it does not interact with jupyter notebook structure */\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    border: 5px solid navy;\n",
    "    padding-top: 30px;\n",
    "    padding-bottom: 30px;\n",
    "    padding-left: 5px;\n",
    "    padding-right: 5px;\n",
    "    margin-bottom: 10px;\n",
    "    }\n",
    "\n",
    ".titlediv_2 {\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    text-align: center !important;\n",
    "    }\n",
    "\n",
    ".titlep {\n",
    "    font-size : 50px;\n",
    "    text-align: center !important;\n",
    "    font-variant: small-caps;\n",
    "}   \n",
    "\n",
    ".titlep_2 {\n",
    "    font-size : 30px;\n",
    "    text-align: center !important;\n",
    "    margin-top: 10px;\n",
    "}  \n",
    "\n",
    ".title_sp {\n",
    "    text-align: center !important;\n",
    "    font-size: 20px !important;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".signature {\n",
    "    margin-top: 60px;\n",
    "    padding-top: 15px;\n",
    "    border-top: 2px solid black;\n",
    "    text-align: right !important;\n",
    "    font-family:'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "}\n",
    "\n",
    ".obj {\n",
    "    text-align: justify;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    margin-right: 100px;\n",
    "    border: 2px solid lightcoral;\n",
    "    padding: 15px;\n",
    "}\n",
    "\n",
    "p {\n",
    "    text-align: justify !important;\n",
    "}\n",
    "\n",
    ".intro {\n",
    "    font-style: italic;\n",
    "}\n",
    "\n",
    ".conclusion {\n",
    "    border: 5px solid navy;\n",
    "    margin-top: 50px;\n",
    "    padding: 10px;\n",
    "}\n",
    "\n",
    ".conclusion h3 {\n",
    "    color:black;\n",
    "    font-variant: small-caps;\n",
    "}\n",
    "\n",
    ".small-caps {\n",
    "    font-variant: small-caps;\n",
    "}\n",
    "\n",
    "table {\n",
    "    margin-right: 10px !important;\n",
    "    font-size: 14px !important;\n",
    "    text-align: center !important;\n",
    "}\n",
    "\n",
    ".table_1 {\n",
    "    border: 3px solid black !important;\n",
    "    width: 100% !important;\n",
    "}\n",
    "\n",
    ".table_1 td, .table_1, th {\n",
    "    border: 1px solid black;\n",
    "}\n",
    "\n",
    ".head_tr {\n",
    "    background-color: dimgray !important;\n",
    "    color: white;\n",
    "    border: 3px solid black !important;\n",
    "}\n",
    "\n",
    "th, td {\n",
    "    padding: 8px !important;\n",
    "    text-align: center !important;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    border: 1.5px solid black; \n",
    "}\n",
    "\n",
    "\n",
    ".recap_table td, .recap_table th {\n",
    "    height: 50px;\n",
    "    width: 180px;\n",
    "}\n",
    "\n",
    ".recap_table th {\n",
    "    font-size: 18px;\n",
    "}\n",
    "\n",
    ".col_1 {\n",
    "    text-align: right !important;\n",
    "    width: 50 !important;\n",
    "}\n",
    "\n",
    ".col_group {\n",
    "    width: 200px;\n",
    "    background-color: #d4c9df !important;\n",
    "    text-align: center;\n",
    "    height: 5px !important;\n",
    "    font-size: 16 !important;\n",
    "    font-weight: bold;\n",
    "}\n",
    "\n",
    ".columns {\n",
    "    display: flex;\n",
    "    flex-wrap: wrap;\n",
    "}\n",
    "\n",
    ".column {\n",
    "    flex: 1;\n",
    "    padding: 10px;\n",
    "    text-align: justify;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    margin-right: 50px;\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "HTML(f'<style>{css_style}</style>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "827f3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_origin = pd.read_csv('cardio_train.csv', sep = ';').set_index('id')  # The purpose of this df is to keep a version of the dataset, it should therefore not be modified.\n",
    "df_raw = pd.read_csv('clean_cvd.csv', sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9f3fe",
   "metadata": {},
   "source": [
    "<h4>Code for: Preliminary Work</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9229f0d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class = 'all'>\n",
       "    <h1>Preliminary Work</h1>\n",
       "    <p class = 'intro'>\n",
       "    This section provides an overview of the project, a summary of part I (data processing and\n",
       "    conclusions).\n",
       "    </p>\n",
       "    <p style = 'border-left: 3px solid silver; padding-left: 5px; font-size : 12.5px; font-style: italic; margin-left: 15px'>\n",
       "        Note: along the project, \"subject\" will refer to any individual in the cohort regardless of \n",
       "        their cardiovascular status; \"patient\" will refer to individual with cardiovascular disease and \"control\" to \n",
       "        individuals without cardiovascular disease.\n",
       "    </p>  \n",
       "    <h2>Project Overview</h2>\n",
       "    <p>\n",
       "        This project aims to compare classification models for detecting cardiovascular diseases.\n",
       "    </p>\n",
       "    <p>\n",
       "        <strong>Part I:</strong> This part focused on analysing a substantial dataset containing\n",
       "        information about individuals with and without cardiovascular diseases.\n",
       "        To prepare for the conception of machine learning models, the first step \n",
       "        was to create a set of data visualisations \n",
       "        to gain a general understanding of the dataset and extract meaningful insights.\n",
       "    </p>\n",
       "    <p>\n",
       "        <strong>Part II:</strong> This part focused on designing classification models. \n",
       "        Several models, namely Logistic Regression, Random Forest, SVM, KNN and AdaBoost were tested, \n",
       "        and their performances were compared. Eventually, SVM was selected for the project as it displayed the\n",
       "        highest accuracy and an acceptable recall.\n",
       "    </p>\n",
       "    <p>\n",
       "        <strong>Part III:</strong> This phase is dedicated to tuning the SVM model, in order to maximise its performances.\n",
       "        A first step will consist on assessing the impact of the main model parameters <i>ie</i><code>C</code>, \n",
       "        <code>gamma</code> and <code>kernel</code>. A second step will focus on reworking the training dataset, taking\n",
       "        conclusions from Part I and II into account.\n",
       "    </p>\n",
       "    <h3>Objectives</h3>\n",
       "    <p>\n",
       "        The objectives of <strong>Part III</strong> are as follows:\n",
       "    </p>\n",
       "    <ul>\n",
       "        <li>Determine the best parameter combination (<code>C</code>, <code>gamma</code> and <code>kernel</code>).</li>\n",
       "        <li>Evaluate the effect of data manipulation (feature engineering...) on the training dataset</li>\n",
       "        <li>Conclude on the model with best perfomances.</li>\n",
       "    </ul>\n",
       "    </p>\n",
       "    <h3>Material</h3>\n",
       "    <p>\n",
       "         Dataset used in this project is the Cardiovascular Disease dataset,\n",
       "          available on\n",
       "          <a href = 'https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset'>Kaggle</a>. \n",
       "          Libraries used include <b>Pandas</b>, <b>NumPy</b>, <b>Matplotlib</b>, \n",
       "         <b>Seaborn</b>, and <b>SciPy</b>. \n",
       "         The project is implemented in Python and designed as a Streamlit application, \n",
       "         with HTML and CSS used for formatting.\n",
       "    </p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class = 'all'>\n",
       "    <h2>Dataset Description</h2>\n",
       "    <p class = 'intro'>\n",
       "        The original dataset contained <b>70000</b> rows and <b>12</b> columns. \n",
       "        There was <b>no missing data</b> in the dataset. For more details on data processing \n",
       "        performed on the original dataset, \n",
       "        or if you wish to see the visualisations made, please refer to Part I of the project.\n",
       "    </p>\n",
       "    <h3>Summary of Data Processing from Part I:</h3>\n",
       "    <p>\n",
       "        <ul>\n",
       "            <li><code>age</code>: unit change from days to years.</li>\n",
       "            <li><code>gender</code>: renamed <code>sex</code> and modalities set to \"female\" and \n",
       "            \"male\" instead of \"1\" and \"2\" respectively.</li>\n",
       "            <li><code>height</code> and <code>weight</code>: removal of some extreme values.</li>\n",
       "            <li><code>ap_hi</code> and <code>ap_lo</code>: incorrect pairs of values were swapped.</li>\n",
       "        </ul>\n",
       "        This data processing led to a new dataset of <b>67189</b> rows and <b>18</b> columns.\n",
       "    </p>        \n",
       "    <h3>Features creation</h3>\n",
       "    <ul>\n",
       "        <li><code>bmi</code>: the body-mass index in kg/m² - <i>continuous</i></li>\n",
       "        <li><code>ap_m</code>: mean blood pressure, calculated as (Systolic Pressure + 2&times;Diastolic Pressure)/3  -\n",
       "        <i>continuous</i></li>\n",
       "        <li><code>ap_aha</code>: categorisation into 4 classes according to American Heart Association (AHA) criteria from the \n",
       "        <a href = 'https://www.ahajournals.org/doi/10.1161/HYP.0000000000000065'>2017 guidelines</a> - <i>categorical</i>\n",
       "        <ul>\n",
       "            <li>1: Normal - 2: Elevated - 3: Hypertension stage 1 - Hypertension stage 2</li>\n",
       "        </ul></li>\n",
       "            <li><code>lifestyle</code>: categorisation of subject's lifestyle habbits into 7 classes - <i>categorical</i>\n",
       "        <ul>\n",
       "            <li>0: Don't smoke, no alcool, active - 1: Smoker - 2: Drinks alcohol - 3: Not active - 4: Smoker and drinks alcohol - \n",
       "        5: Smoker and is not active - 6: Drinks alcohol and is not active - 7: Smoker, drinks alcohol and is not active</li>\n",
       "        </ul></li>\n",
       "        <li><code>healthy_ls</code> lifestyle considered \"healthy\", <i>ie</i> subjects that don't smoke, don't drink alcohol and\n",
       "        do exercise - <i>binary</i></li>\n",
       "    </ul>\n",
       "    </p>\n",
       "    <h3>Features of the Dataset</h3>\n",
       "    <table class = 'table_1'>\n",
       "        <tr>\n",
       "            <th>Feature</th>\n",
       "            <th>Description</th>\n",
       "            <th>Unit / Modalities</th>\n",
       "            <th style = 'width: 10%'>Data Type</th>\n",
       "            <th style = 'width: 30%'>Status</th>            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Age</b><br><code>age</code></td>\n",
       "            <td>The age of the subject</td>\n",
       "            <td>years</td>\n",
       "            <td>float</td>\n",
       "            <td>Reworked from the original dataset: unit change from days to years</td> \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Age Group</b><br><code>age_group</code></td>\n",
       "            <td>The 5-years age group of the subject</td>\n",
       "            <td>< 45<br>[46 - 50]<br>[51 - 55]<br>[56 - 60]<br>[61 - 65]</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Sex</b><br><code>sex</code></td>\n",
       "            <td>Sex of the subject</td>\n",
       "            <td>male<br>female</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Reworked from the original dataset: renamed <code>sex</code> and modalities set to \"female\" and \n",
       "            \"male\" instead of \"1\" and \"2\" respectively.</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Height</b><br><code>height</code></td>\n",
       "            <td>Height of the subject</td>\n",
       "            <td>cm</td>\n",
       "            <td>int</td>\n",
       "            <td>Reworked from the original dataset: some extreme values were removed (could be errors)</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Weight</b><br><code>weight</code></td>\n",
       "            <td>weight of the subject</td>\n",
       "            <td>kg</td>\n",
       "            <td>float</td>\n",
       "            <td>Reworked from the original dataset: some extreme values were removed (could be errors)</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Body-Mass Index</b><br><code>bmi</code></td>\n",
       "            <td>BMI of the subject defined as weight/height²</td>\n",
       "            <td>kg/m²</td>\n",
       "            <td>float</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Systolic blood pressure</b><br><code>ap_hi</code></td>\n",
       "            <td>Systolic blood pressure of the subject</td>\n",
       "            <td>mmHg</td>\n",
       "            <td>float</td>\n",
       "            <td>Reworked from the original dataset: error values were corrected/deleted</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Diastolic blood pressure</b><br><code>ap_lo</code></td>\n",
       "            <td>Diastolic blood pressure of the subject</td>\n",
       "            <td>mmHg</td>\n",
       "            <td>float</td>\n",
       "            <td>Reworked from the original dataset: error values were corrected/deleted</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Mean Blood Pressure</b><br><code>ap_m</code></td>\n",
       "            <td>Mean blood pressure of the subject, defined as (systolic + 2 &times diastolic) /3</td>\n",
       "            <td>mmHg</td>\n",
       "            <td>float</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>AHA Classification</b><br><code>ap_aha</code></td>\n",
       "            <td>Blood pressure class according to the American Heart Association's guidelines</td>\n",
       "            <td>Normal<br>Elevated<br>Hypertension Stage 1<br>Hypertension Stage 2</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>AHA classification</b><br><code>ap_aha</code></td>\n",
       "            <td>Blood pressure class according to the American Heart Association's guidelines</td>\n",
       "            <td>Normal<br>Elevated<br>Hypertension Stage 1<br>Hypertension Stage 2</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Cholesterol</b><br><code>cholesterol</code></td>\n",
       "            <td>Class for the cholesterol of the subject</td>\n",
       "            <td>Normal<br>Above normal<br>Well above normal</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Original</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Glucose</b><br><code>gluc</code></td>\n",
       "            <td>Class for the glucose level of the subject</td>\n",
       "            <td>Normal<br>Above normal<br>Well above normal</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Original</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Smoker</b><br><code>smoke</code></td>\n",
       "            <td>Smoker status of the subject</td>\n",
       "            <td>\"0\": no<br>\"1\": yes</td>\n",
       "            <td>binary</td>\n",
       "            <td>Original</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Alcohol intake</b><br><code>alco</code></td>\n",
       "            <td>Drinking status of the subject</td>\n",
       "            <td>\"0\": no<br>\"1\": yes</td>\n",
       "            <td>binary</td>\n",
       "            <td>Original</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Physical activity</b><br><code>active</code></td>\n",
       "            <td>Physical activity status of the subject</td>\n",
       "            <td>\"0\": no<br>\"1\": yes</td>\n",
       "            <td>binary</td>\n",
       "            <td>Original</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Lifestyle</b><br><code>lifestyle</code></td>\n",
       "            <td>Class for the lifestyle of the subject</td>\n",
       "            <td style = 'text-align: left !important'>\"0\": Don't smoke, no alcool, active<br>\"1\": Smoker<br>\"2\": Drinks alcohol<br>\"3\": Not active<br>\"4\": Smoker and drinks alcohol<br>\"5\": Smoker and is not active<br>\"6\": Drinks alcohol and is not active<br>\"7\": Smoker, drinks alcohol and is not active</td>\n",
       "            <td>categorical</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Healthy Lifestyle</b><br><code>healthy_ls</code></td>\n",
       "            <td>Defines if the subject do not smoke, do not drink, and report have physical activity</td>\n",
       "            <td>\"0\": no<br>\"1\": yes</td>\n",
       "            <td>binary</td>\n",
       "            <td>Created</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "    <h3>Dataset Head and Tail</h3>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bmi</th>\n",
       "      <th>ap_m</th>\n",
       "      <th>ap_aha</th>\n",
       "      <th>lifestyle</th>\n",
       "      <th>healthy_ls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50.357290</td>\n",
       "      <td>male</td>\n",
       "      <td>168</td>\n",
       "      <td>62.0</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[46 - 50]</td>\n",
       "      <td>21.967120</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.381246</td>\n",
       "      <td>female</td>\n",
       "      <td>156</td>\n",
       "      <td>85.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[51 - 55]</td>\n",
       "      <td>34.927679</td>\n",
       "      <td>106.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.627652</td>\n",
       "      <td>female</td>\n",
       "      <td>165</td>\n",
       "      <td>64.0</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[51 - 55]</td>\n",
       "      <td>23.507805</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.249144</td>\n",
       "      <td>male</td>\n",
       "      <td>169</td>\n",
       "      <td>82.0</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[46 - 50]</td>\n",
       "      <td>28.710479</td>\n",
       "      <td>116.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.841205</td>\n",
       "      <td>female</td>\n",
       "      <td>156</td>\n",
       "      <td>56.0</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[46 - 50]</td>\n",
       "      <td>23.011177</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age     sex  height  weight  ap_hi  ap_lo  cholesterol  gluc  smoke  \\\n",
       "0  50.357290    male     168    62.0    110     80            1     1      0   \n",
       "1  55.381246  female     156    85.0    140     90            3     1      0   \n",
       "2  51.627652  female     165    64.0    130     70            3     1      0   \n",
       "3  48.249144    male     169    82.0    150    100            1     1      0   \n",
       "4  47.841205  female     156    56.0    100     60            1     1      0   \n",
       "\n",
       "   alco  active  cardio  age_group        bmi        ap_m  ap_aha  lifestyle  \\\n",
       "0     0       1       0  [46 - 50]  21.967120   90.000000       3          0   \n",
       "1     0       1       1  [51 - 55]  34.927679  106.666667       4          0   \n",
       "2     0       0       1  [51 - 55]  23.507805   90.000000       3          3   \n",
       "3     0       1       1  [46 - 50]  28.710479  116.666667       4          0   \n",
       "4     0       0       0  [46 - 50]  23.011177   73.333333       1          3   \n",
       "\n",
       "   healthy_ls  \n",
       "0           1  \n",
       "1           1  \n",
       "2           0  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>age_group</th>\n",
       "      <th>bmi</th>\n",
       "      <th>ap_m</th>\n",
       "      <th>ap_aha</th>\n",
       "      <th>lifestyle</th>\n",
       "      <th>healthy_ls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67184</th>\n",
       "      <td>52.676249</td>\n",
       "      <td>male</td>\n",
       "      <td>168</td>\n",
       "      <td>76.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[51 - 55]</td>\n",
       "      <td>26.927438</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67185</th>\n",
       "      <td>61.878166</td>\n",
       "      <td>female</td>\n",
       "      <td>158</td>\n",
       "      <td>126.0</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[61 - 65]</td>\n",
       "      <td>50.472681</td>\n",
       "      <td>106.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67186</th>\n",
       "      <td>52.199863</td>\n",
       "      <td>male</td>\n",
       "      <td>183</td>\n",
       "      <td>105.0</td>\n",
       "      <td>180</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[51 - 55]</td>\n",
       "      <td>31.353579</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67187</th>\n",
       "      <td>61.412731</td>\n",
       "      <td>female</td>\n",
       "      <td>163</td>\n",
       "      <td>72.0</td>\n",
       "      <td>135</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[61 - 65]</td>\n",
       "      <td>27.099251</td>\n",
       "      <td>98.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67188</th>\n",
       "      <td>56.235455</td>\n",
       "      <td>female</td>\n",
       "      <td>170</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[56 - 60]</td>\n",
       "      <td>24.913495</td>\n",
       "      <td>93.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             age     sex  height  weight  ap_hi  ap_lo  cholesterol  gluc  \\\n",
       "67184  52.676249    male     168    76.0    120     80            1     1   \n",
       "67185  61.878166  female     158   126.0    140     90            2     2   \n",
       "67186  52.199863    male     183   105.0    180     90            3     1   \n",
       "67187  61.412731  female     163    72.0    135     80            1     2   \n",
       "67188  56.235455  female     170    72.0    120     80            2     1   \n",
       "\n",
       "       smoke  alco  active  cardio  age_group        bmi        ap_m  ap_aha  \\\n",
       "67184      1     0       1       0  [51 - 55]  26.927438   93.333333       3   \n",
       "67185      0     0       1       1  [61 - 65]  50.472681  106.666667       4   \n",
       "67186      0     1       0       1  [51 - 55]  31.353579  120.000000       4   \n",
       "67187      0     0       0       1  [61 - 65]  27.099251   98.333333       3   \n",
       "67188      0     0       1       0  [56 - 60]  24.913495   93.333333       3   \n",
       "\n",
       "       lifestyle  healthy_ls  \n",
       "67184          1           0  \n",
       "67185          0           1  \n",
       "67186          6           0  \n",
       "67187          3           0  \n",
       "67188          0           1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>bmi</th>\n",
       "      <th>ap_m</th>\n",
       "      <th>ap_aha</th>\n",
       "      <th>lifestyle</th>\n",
       "      <th>healthy_ls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "      <td>67189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>53.3</td>\n",
       "      <td>164.6</td>\n",
       "      <td>74.7</td>\n",
       "      <td>126.9</td>\n",
       "      <td>81.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>27.6</td>\n",
       "      <td>96.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.7</td>\n",
       "      <td>7.7</td>\n",
       "      <td>13.9</td>\n",
       "      <td>16.6</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>39.1</td>\n",
       "      <td>131.0</td>\n",
       "      <td>50.7</td>\n",
       "      <td>70.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.6</td>\n",
       "      <td>56.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>48.4</td>\n",
       "      <td>159.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>53.9</td>\n",
       "      <td>165.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>93.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.4</td>\n",
       "      <td>170.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.3</td>\n",
       "      <td>103.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.9</td>\n",
       "      <td>207.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>85.8</td>\n",
       "      <td>186.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age   height   weight    ap_hi    ap_lo  cholesterol     gluc  \\\n",
       "count  67189.0  67189.0  67189.0  67189.0  67189.0      67189.0  67189.0   \n",
       "mean      53.3    164.6     74.7    126.9     81.5          1.4      1.2   \n",
       "std        6.7      7.7     13.9     16.6      9.4          0.7      0.6   \n",
       "min       39.1    131.0     50.7     70.0     45.0          1.0      1.0   \n",
       "25%       48.4    159.0     65.0    120.0     80.0          1.0      1.0   \n",
       "50%       53.9    165.0     72.0    120.0     80.0          1.0      1.0   \n",
       "75%       58.4    170.0     82.0    140.0     90.0          2.0      1.0   \n",
       "max       64.9    207.0    200.0    240.0    182.0          3.0      3.0   \n",
       "\n",
       "         smoke     alco   active   cardio      bmi     ap_m   ap_aha  \\\n",
       "count  67189.0  67189.0  67189.0  67189.0  67189.0  67189.0  67189.0   \n",
       "mean       0.1      0.1      0.8      0.5     27.6     96.6      2.9   \n",
       "std        0.3      0.2      0.4      0.5      5.1     11.0      0.9   \n",
       "min        0.0      0.0      0.0      0.0     14.6     56.7      1.0   \n",
       "25%        0.0      0.0      1.0      0.0     23.9     93.3      3.0   \n",
       "50%        0.0      0.0      1.0      0.0     26.5     93.3      3.0   \n",
       "75%        0.0      0.0      1.0      1.0     30.3    103.3      3.0   \n",
       "max        1.0      1.0      1.0      1.0     85.8    186.7      4.0   \n",
       "\n",
       "       lifestyle  healthy_ls  \n",
       "count    67189.0     67189.0  \n",
       "mean         0.8         0.7  \n",
       "std          1.4         0.5  \n",
       "min          0.0         0.0  \n",
       "25%          0.0         0.0  \n",
       "50%          0.0         1.0  \n",
       "75%          1.0         1.0  \n",
       "max          7.0         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class = 'all'>\n",
       "    <h2>Conclusions from Part I</h2>\n",
       "    <h3>Major Findings</h3>\n",
       "    <p>\n",
       "        Several variables showed a strong correlation with the target feature: \n",
       "        prevalence of cardiovascular diseases increased with <b>age</b>, <b>BMI</b>\n",
       "        and <b>blood pressure</b>, and with levels of <b>glucose</b> and <b>cholesterol</b>\n",
       "        above normality.\n",
       "    </p>\n",
       "    <p>\n",
       "        <b>Sex</b> on the other hand did not correlate with the feature target, as the\n",
       "        prevalence of cardiovascular disease was very close to <b>50%</b> among \n",
       "        both males and females. Nevertheless, sex influenced several variables:\n",
       "        men had higher blood pressure, while women had slightly higher levels of\n",
       "        cholesterol. There was way more smokers among men and there were \n",
       "        also more likely to drink alcohol. Men were also taller than women, but\n",
       "        the distribution of BMI was comparable for both sexes.\n",
       "    </p>\n",
       "    <h3>Insights for the Model to be built</h3>\n",
       "    <p>\n",
       "        The analysis has highlighted several avenues for designing an effective model:\n",
       "        <ul>\n",
       "            <li><code>age</code>: aging has a major influence on health in general.\n",
       "            It could therefore be relevant to separate the training data in groups, either\n",
       "            two groups (with a cut-off around 50 years old) or even into smaller age\n",
       "            groups.</li>\n",
       "            <li><code>sex</code> did not correlate with the target feature, but did with\n",
       "            several other feature. It could be relevant to either rebalance the dataset to\n",
       "            have 50% males (<i>vs.</i> the 35.4%\n",
       "            in the original dataset), or to train separe models for males and females.</li>\n",
       "            <li><code>height</code>: this variable alone may not bring relevant information \n",
       "            (unlike <code>weight</code>) and could be dropped in favor of <code>bmi</code>.</li>\n",
       "            <li><code>weight</code> and <code>bmi</code>: extreme values (high values especially)\n",
       "            may not be representative, and could have a negative influence. It could be wise\n",
       "            to compare the performances of the model with and without these extreme values.</li>\n",
       "            <li><code>gluc</code> and <code>cholesterol</code>: in both case, majority of\n",
       "            subjects had normal values for these variables. Pooling modalities <i>Above normal</i>\n",
       "            and <i>Well above normal</i> could give more weight to these features. \n",
       "            <code>gluc</code> could also be dropped as it may not bring a lot of information,\n",
       "            overshadowed by <code>cholesterol</code></li>\n",
       "            <li><code>smoke</code>: it could be wise to drop this feature as it may be\n",
       "            biaised, as explained in the <i>Lifestyle</i> section.</li>\n",
       "        </ul>\n",
       "    </p>\n",
       "</div>\n",
       "<div class = 'all'>\n",
       "    <h2>Conclusions from Part II</h2>\n",
       "    <h3>Major Findings</h3>\n",
       "    <p>\n",
       "        Support Vector Machines presented the best accuracy (73.2%), a good rate of correct predictions for Class 0 (77.6%)\n",
       "        and an acceptable rate for Class 1 (68.8%).\n",
       "        <br>Random Forest had the highest rate of correct classification for class 1 (71.0%) despite a lower accuracy (70.8%) and would have make\n",
       "        a decent second choice. In the present context, we want to limit the number of wrong predictions for class 1. \n",
       "        Indeed, should an individual be incorrectly labeled as \"patient\", further diagnostic testing would soon reveal that this \n",
       "        individual do not have CV disease. On the other hand, misclassifiying a patient as not having CV disease may \n",
       "        take them out of medical care for a while and have consequences.\n",
       "    </p>\n",
       "    <p>\n",
       "        Across different models, the same features bore the highest importance: blood pressure-related features, \n",
       "        age, weight and cholesterol. Compared to those, features related to lifestyle, glucose levels and sex had little\n",
       "        to no impact.\n",
       "    </p>\n",
       "    <h3>Insights for the Model to be built</h3>\n",
       "    <ul>\n",
       "        <li>No model stood out regarding performances. While SVM seemed to be the best choice, it is unlikely that\n",
       "        parameter tuning will allow a major increase in performances.</li>\n",
       "        <li>A group of four to five features bore the models while the other brought limited information:\n",
       "        some feature engineering could be a key to model improvement.</li>\n",
       "    </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Preliminary Work</h1>\n",
    "    <p class = 'intro'>\n",
    "    This section provides an overview of the project, a summary of part I (data processing and\n",
    "    conclusions).\n",
    "    </p>\n",
    "    <p style = 'border-left: 3px solid silver; padding-left: 5px; font-size : 12.5px; font-style: italic; margin-left: 15px'>\n",
    "        Note: along the project, \"subject\" will refer to any individual in the cohort regardless of \n",
    "        their cardiovascular status; \"patient\" will refer to individual with cardiovascular disease and \"control\" to \n",
    "        individuals without cardiovascular disease.\n",
    "    </p>  \n",
    "    <h2>Project Overview</h2>\n",
    "    <p>\n",
    "        This project aims to compare classification models for detecting cardiovascular diseases.\n",
    "    </p>\n",
    "    <p>\n",
    "        <strong>Part I:</strong> This part focused on analysing a substantial dataset containing\n",
    "        information about individuals with and without cardiovascular diseases.\n",
    "        To prepare for the conception of machine learning models, the first step \n",
    "        was to create a set of data visualisations \n",
    "        to gain a general understanding of the dataset and extract meaningful insights.\n",
    "    </p>\n",
    "    <p>\n",
    "        <strong>Part II:</strong> This part focused on designing classification models. \n",
    "        Several models, namely Logistic Regression, Random Forest, SVM, KNN and AdaBoost were tested, \n",
    "        and their performances were compared. Eventually, SVM was selected for the project as it displayed the\n",
    "        highest accuracy and an acceptable recall.\n",
    "    </p>\n",
    "    <p>\n",
    "        <strong>Part III:</strong> This phase is dedicated to tuning the SVM model, in order to maximise its performances.\n",
    "        A first step will consist on assessing the impact of the main model parameters <i>ie</i><code>C</code>, \n",
    "        <code>gamma</code> and <code>kernel</code>. A second step will focus on reworking the training dataset, taking\n",
    "        conclusions from Part I and II into account.\n",
    "    </p>\n",
    "    <h3>Objectives</h3>\n",
    "    <p>\n",
    "        The objectives of <strong>Part III</strong> are as follows:\n",
    "    </p>\n",
    "    <ul>\n",
    "        <li>Determine the best parameter combination (<code>C</code>, <code>gamma</code> and <code>kernel</code>).</li>\n",
    "        <li>Evaluate the effect of data manipulation (feature engineering...) on the training dataset</li>\n",
    "        <li>Conclude on the model with best perfomances.</li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <h3>Material</h3>\n",
    "    <p>\n",
    "         Dataset used in this project is the Cardiovascular Disease dataset,\n",
    "          available on\n",
    "          <a href = 'https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset'>Kaggle</a>. \n",
    "          Libraries used include <b>Pandas</b>, <b>NumPy</b>, <b>Matplotlib</b>, \n",
    "         <b>Seaborn</b>, and <b>SciPy</b>. \n",
    "         The project is implemented in Python and designed as a Streamlit application, \n",
    "         with HTML and CSS used for formatting.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h2>Dataset Description</h2>\n",
    "    <p class = 'intro'>\n",
    "        The original dataset contained <b>{df_origin.shape[0]}</b> rows and <b>{df_origin.shape[1]}</b> columns. \n",
    "        There was <b>no missing data</b> in the dataset. For more details on data processing \n",
    "        performed on the original dataset, \n",
    "        or if you wish to see the visualisations made, please refer to Part I of the project.\n",
    "    </p>\n",
    "    <h3>Summary of Data Processing from Part I:</h3>\n",
    "    <p>\n",
    "        <ul>\n",
    "            <li><code>age</code>: unit change from days to years.</li>\n",
    "            <li><code>gender</code>: renamed <code>sex</code> and modalities set to \"female\" and \n",
    "            \"male\" instead of \"1\" and \"2\" respectively.</li>\n",
    "            <li><code>height</code> and <code>weight</code>: removal of some extreme values.</li>\n",
    "            <li><code>ap_hi</code> and <code>ap_lo</code>: incorrect pairs of values were swapped.</li>\n",
    "        </ul>\n",
    "        This data processing led to a new dataset of <b>{df_raw.shape[0]}</b> rows and <b>{df_raw.shape[1]}</b> columns.\n",
    "    </p>        \n",
    "    <h3>Features creation</h3>\n",
    "    <ul>\n",
    "        <li><code>bmi</code>: the body-mass index in kg/m² - <i>continuous</i></li>\n",
    "        <li><code>ap_m</code>: mean blood pressure, calculated as (Systolic Pressure + 2&times;Diastolic Pressure)/3  -\n",
    "        <i>continuous</i></li>\n",
    "        <li><code>ap_aha</code>: categorisation into 4 classes according to American Heart Association (AHA) criteria from the \n",
    "        <a href = 'https://www.ahajournals.org/doi/10.1161/HYP.0000000000000065'>2017 guidelines</a> - <i>categorical</i>\n",
    "        <ul>\n",
    "            <li>1: Normal - 2: Elevated - 3: Hypertension stage 1 - Hypertension stage 2</li>\n",
    "        </ul></li>\n",
    "            <li><code>lifestyle</code>: categorisation of subject's lifestyle habbits into 7 classes - <i>categorical</i>\n",
    "        <ul>\n",
    "            <li>0: Don't smoke, no alcool, active - 1: Smoker - 2: Drinks alcohol - 3: Not active - 4: Smoker and drinks alcohol - \n",
    "        5: Smoker and is not active - 6: Drinks alcohol and is not active - 7: Smoker, drinks alcohol and is not active</li>\n",
    "        </ul></li>\n",
    "        <li><code>healthy_ls</code> lifestyle considered \"healthy\", <i>ie</i> subjects that don't smoke, don't drink alcohol and\n",
    "        do exercise - <i>binary</i></li>\n",
    "    </ul>\n",
    "    </p>\n",
    "    <h3>Features of the Dataset</h3>\n",
    "    <table class = 'table_1'>\n",
    "        <tr>\n",
    "            <th>Feature</th>\n",
    "            <th>Description</th>\n",
    "            <th>Unit / Modalities</th>\n",
    "            <th style = 'width: 10%'>Data Type</th>\n",
    "            <th style = 'width: 30%'>Status</th>            \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Age</b><br><code>age</code></td>\n",
    "            <td>The age of the subject</td>\n",
    "            <td>years</td>\n",
    "            <td>float</td>\n",
    "            <td>Reworked from the original dataset: unit change from days to years</td> \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Age Group</b><br><code>age_group</code></td>\n",
    "            <td>The 5-years age group of the subject</td>\n",
    "            <td>< 45<br>[46 - 50]<br>[51 - 55]<br>[56 - 60]<br>[61 - 65]</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Sex</b><br><code>sex</code></td>\n",
    "            <td>Sex of the subject</td>\n",
    "            <td>male<br>female</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Reworked from the original dataset: renamed <code>sex</code> and modalities set to \"female\" and \n",
    "            \"male\" instead of \"1\" and \"2\" respectively.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Height</b><br><code>height</code></td>\n",
    "            <td>Height of the subject</td>\n",
    "            <td>cm</td>\n",
    "            <td>int</td>\n",
    "            <td>Reworked from the original dataset: some extreme values were removed (could be errors)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Weight</b><br><code>weight</code></td>\n",
    "            <td>weight of the subject</td>\n",
    "            <td>kg</td>\n",
    "            <td>float</td>\n",
    "            <td>Reworked from the original dataset: some extreme values were removed (could be errors)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Body-Mass Index</b><br><code>bmi</code></td>\n",
    "            <td>BMI of the subject defined as weight/height²</td>\n",
    "            <td>kg/m²</td>\n",
    "            <td>float</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Systolic blood pressure</b><br><code>ap_hi</code></td>\n",
    "            <td>Systolic blood pressure of the subject</td>\n",
    "            <td>mmHg</td>\n",
    "            <td>float</td>\n",
    "            <td>Reworked from the original dataset: error values were corrected/deleted</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Diastolic blood pressure</b><br><code>ap_lo</code></td>\n",
    "            <td>Diastolic blood pressure of the subject</td>\n",
    "            <td>mmHg</td>\n",
    "            <td>float</td>\n",
    "            <td>Reworked from the original dataset: error values were corrected/deleted</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Mean Blood Pressure</b><br><code>ap_m</code></td>\n",
    "            <td>Mean blood pressure of the subject, defined as (systolic + 2 &times diastolic) /3</td>\n",
    "            <td>mmHg</td>\n",
    "            <td>float</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>AHA Classification</b><br><code>ap_aha</code></td>\n",
    "            <td>Blood pressure class according to the American Heart Association's guidelines</td>\n",
    "            <td>Normal<br>Elevated<br>Hypertension Stage 1<br>Hypertension Stage 2</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>AHA classification</b><br><code>ap_aha</code></td>\n",
    "            <td>Blood pressure class according to the American Heart Association's guidelines</td>\n",
    "            <td>Normal<br>Elevated<br>Hypertension Stage 1<br>Hypertension Stage 2</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Cholesterol</b><br><code>cholesterol</code></td>\n",
    "            <td>Class for the cholesterol of the subject</td>\n",
    "            <td>Normal<br>Above normal<br>Well above normal</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Original</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Glucose</b><br><code>gluc</code></td>\n",
    "            <td>Class for the glucose level of the subject</td>\n",
    "            <td>Normal<br>Above normal<br>Well above normal</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Original</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Smoker</b><br><code>smoke</code></td>\n",
    "            <td>Smoker status of the subject</td>\n",
    "            <td>\"0\": no<br>\"1\": yes</td>\n",
    "            <td>binary</td>\n",
    "            <td>Original</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Alcohol intake</b><br><code>alco</code></td>\n",
    "            <td>Drinking status of the subject</td>\n",
    "            <td>\"0\": no<br>\"1\": yes</td>\n",
    "            <td>binary</td>\n",
    "            <td>Original</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Physical activity</b><br><code>active</code></td>\n",
    "            <td>Physical activity status of the subject</td>\n",
    "            <td>\"0\": no<br>\"1\": yes</td>\n",
    "            <td>binary</td>\n",
    "            <td>Original</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Lifestyle</b><br><code>lifestyle</code></td>\n",
    "            <td>Class for the lifestyle of the subject</td>\n",
    "            <td style = 'text-align: left !important'>\"0\": Don't smoke, no alcool, active<br>\"1\": Smoker<br>\"2\": Drinks alcohol<br>\"3\": Not active<br>\"4\": Smoker and drinks alcohol<br>\"5\": Smoker and is not active<br>\"6\": Drinks alcohol and is not active<br>\"7\": Smoker, drinks alcohol and is not active</td>\n",
    "            <td>categorical</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Healthy Lifestyle</b><br><code>healthy_ls</code></td>\n",
    "            <td>Defines if the subject do not smoke, do not drink, and report have physical activity</td>\n",
    "            <td>\"0\": no<br>\"1\": yes</td>\n",
    "            <td>binary</td>\n",
    "            <td>Created</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Dataset Head and Tail</h3>\n",
    "</div>\"\"\"))\n",
    "\n",
    "display(df_raw.head())\n",
    "display(df_raw.tail())\n",
    "\n",
    "display(df_raw.describe().round(1))\n",
    "\n",
    "display(HTML(\n",
    "f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h2>Conclusions from Part I</h2>\n",
    "    <h3>Major Findings</h3>\n",
    "    <p>\n",
    "        Several variables showed a strong correlation with the target feature: \n",
    "        prevalence of cardiovascular diseases increased with <b>age</b>, <b>BMI</b>\n",
    "        and <b>blood pressure</b>, and with levels of <b>glucose</b> and <b>cholesterol</b>\n",
    "        above normality.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Sex</b> on the other hand did not correlate with the feature target, as the\n",
    "        prevalence of cardiovascular disease was very close to <b>50%</b> among \n",
    "        both males and females. Nevertheless, sex influenced several variables:\n",
    "        men had higher blood pressure, while women had slightly higher levels of\n",
    "        cholesterol. There was way more smokers among men and there were \n",
    "        also more likely to drink alcohol. Men were also taller than women, but\n",
    "        the distribution of BMI was comparable for both sexes.\n",
    "    </p>\n",
    "    <h3>Insights for the Model to be built</h3>\n",
    "    <p>\n",
    "        The analysis has highlighted several avenues for designing an effective model:\n",
    "        <ul>\n",
    "            <li><code>age</code>: aging has a major influence on health in general.\n",
    "            It could therefore be relevant to separate the training data in groups, either\n",
    "            two groups (with a cut-off around 50 years old) or even into smaller age\n",
    "            groups.</li>\n",
    "            <li><code>sex</code> did not correlate with the target feature, but did with\n",
    "            several other feature. It could be relevant to either rebalance the dataset to\n",
    "            have 50% males (<i>vs.</i> the {df_raw['sex'].value_counts(normalize = True)['male']:.1%}\n",
    "            in the original dataset), or to train separe models for males and females.</li>\n",
    "            <li><code>height</code>: this variable alone may not bring relevant information \n",
    "            (unlike <code>weight</code>) and could be dropped in favor of <code>bmi</code>.</li>\n",
    "            <li><code>weight</code> and <code>bmi</code>: extreme values (high values especially)\n",
    "            may not be representative, and could have a negative influence. It could be wise\n",
    "            to compare the performances of the model with and without these extreme values.</li>\n",
    "            <li><code>gluc</code> and <code>cholesterol</code>: in both case, majority of\n",
    "            subjects had normal values for these variables. Pooling modalities <i>Above normal</i>\n",
    "            and <i>Well above normal</i> could give more weight to these features. \n",
    "            <code>gluc</code> could also be dropped as it may not bring a lot of information,\n",
    "            overshadowed by <code>cholesterol</code></li>\n",
    "            <li><code>smoke</code>: it could be wise to drop this feature as it may be\n",
    "            biaised, as explained in the <i>Lifestyle</i> section.</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "</div>\n",
    "<div class = 'all'>\n",
    "    <h2>Conclusions from Part II</h2>\n",
    "    <h3>Major Findings</h3>\n",
    "    <p>\n",
    "        Support Vector Machines presented the best accuracy (73.2%), a good rate of correct predictions for Class 0 (77.6%)\n",
    "        and an acceptable rate for Class 1 (68.8%).\n",
    "        <br>Random Forest had the highest rate of correct classification for class 1 (71.0%) despite a lower accuracy (70.8%) and would have make\n",
    "        a decent second choice. In the present context, we want to limit the number of wrong predictions for class 1. \n",
    "        Indeed, should an individual be incorrectly labeled as \"patient\", further diagnostic testing would soon reveal that this \n",
    "        individual do not have CV disease. On the other hand, misclassifiying a patient as not having CV disease may \n",
    "        take them out of medical care for a while and have consequences.\n",
    "    </p>\n",
    "    <p>\n",
    "        Across different models, the same features bore the highest importance: blood pressure-related features, \n",
    "        age, weight and cholesterol. Compared to those, features related to lifestyle, glucose levels and sex had little\n",
    "        to no impact.\n",
    "    </p>\n",
    "    <h3>Insights for the Model to be built</h3>\n",
    "    <ul>\n",
    "        <li>No model stood out regarding performances. While SVM seemed to be the best choice, it is unlikely that\n",
    "        parameter tuning will allow a major increase in performances.</li>\n",
    "        <li>A group of four to five features bore the models while the other brought limited information:\n",
    "        some feature engineering could be a key to model improvement.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\"\"\"))\n",
    "# Creating a class \"Parameter\" to access parameter-related informations\n",
    "class Parameter:\n",
    "    def __init__(self, name, full_name, unit, mod, mod_names):\n",
    "        self.name = name\n",
    "        self.full_name = full_name\n",
    "        self.unit = unit\n",
    "        self.label = f'{self.full_name} ({self.unit})'\n",
    "        self.mod = mod\n",
    "        self.mod_names = mod_names\n",
    "\n",
    "age = Parameter(\"age\", \"Age\", \"years\", None, None)\n",
    "sex = Parameter(\"sex\", \"Sex\", None, ['female', 'male'], ['Female', 'Male'])\n",
    "height = Parameter(\"height\", \"Height\", \"cm\", None, None)\n",
    "weight = Parameter(\"weight\", \"Weight\", \"kg\", None, None)\n",
    "ap_hi = Parameter(\"ap_hi\", \"Systolic Blood Pressure\", \"mmHg\", None, None)\n",
    "ap_lo = Parameter(\"ap_lo\", \"Diastolic Blood Pressure\", \"mmHg\", None, None)\n",
    "cholesterol = Parameter(\"cholesterol\", \"Cholesterol\", None, [\"1\", \"2\", \"3\"], ['Normal', 'Above normal', 'Well above normal'])\n",
    "gluc = Parameter(\"gluc\", \"Glucose\", None, [\"1\", \"2\", \"3\"], ['Normal', 'Above normal', 'Well above normal'])\n",
    "smoke = Parameter(\"smoke\", \"Tobbaco\", None, [\"0\", \"1\"], ['No', 'Yes'])\n",
    "alco = Parameter(\"alco\", \"Alcohol\", None, [\"0\", \"1\"], ['No', 'Yes'])\n",
    "active = Parameter(\"active\", \"Physical Activity\", None, [\"0\", \"1\"], ['No', 'Yes'])\n",
    "cardio = Parameter(\"cardio\", \"Cardiovascular Disease\", None, [\"0\", \"1\"], ['No', 'Yes'])\n",
    "bmi = Parameter(\"bmi\", \"BMI\", \"kg/m²\", None, None)\n",
    "ap_m = Parameter(\"ap_m\", \"Mean Blood Pressure\", \"mmHg\", None, None)\n",
    "ap_aha = Parameter(\"ap_aha\", \"Blood Pressure Status\", None, [\"1\", \"2\", \"3\", \"4\"], ['Normal', 'Elevated', 'Hypertension stage I', 'Hypertension stage II'])\n",
    "lifestyle = Parameter(\"lifestyle\", \"Lifestyle\", None, [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"],\n",
    "                      [\"Non-smoker, No alcohol, Active\", \"Smoker\", \"Alcohol\", \"Not Active\", \"Smoker & Alcohol\", \"Smoker & Not active\", \"Alcohol & Not active\", \"Smoker & Alcohol & Not active\"])\n",
    "healthy_ls = Parameter('healthy_ls', \"Healthy Lifestyle\", None, [\"0\", \"1\"], [\"no\", \"yes\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7750bb",
   "metadata": {},
   "source": [
    "<h4>Code for: Functions definition</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c14088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pval_txt(pval):\n",
    "    \"\"\"\n",
    "    Formats a p-value according to its value.\n",
    "\n",
    "    If the p-value is less than 0.0001, the function returns the string 'p < 10^-4'.\n",
    "    Otherwise, it returns the p-value rounded to 4 decimal places as a string, prefixed with 'p = '.\n",
    "\n",
    "    Args:\n",
    "        pval (float): The p-value to format.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted p-value.\n",
    "    \"\"\"\n",
    "    if pval < 0.0001:\n",
    "        return f'$\\mathit{{p}}$ < 10\\u207b\\u2074'\n",
    "    else:\n",
    "        return f'$\\mathit{{p}}$ = {pval:.4f}'\n",
    "    \n",
    "def pval_shapiro(df, var):\n",
    "    pval = shapiro(df[var.name])[1]\n",
    "    return f\"Normally distributed <br>{pval_txt(pval)}\" if pval >= 0.05 else f\"Not normally distributed <br>{pval_txt(pval)}\"\n",
    "\n",
    "def mean_sd_range(df, var):\n",
    "    return (f\"{np.mean(df[var.name]):.1f} ± {np.std(df[var.name]):.1f}\", f\"[{np.min(df[var.name]):.1f} - {np.max(df[var.name]):.1f}]\")\n",
    "\n",
    "def univar_cont(df, var_lst):\n",
    "    body = f\"\"\"\"\"\"\n",
    "    head = f\"\"\"\n",
    "    <table class = \"table_1\" style = \"width: 75% !important\">\n",
    "        <tr class = \"head_tr\">\n",
    "            <th>Parameter</th>\n",
    "            <th>Mean ± SD</th>\n",
    "            <th>Range</th>\n",
    "            <th>Normality</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for var in var_lst:\n",
    "        body += f\"\"\"\n",
    "        <tr>\n",
    "            <td>{var.full_name} ({var.unit})</td>\n",
    "            <td style = 'border-left: 1px dashed black'>{mean_sd_range(df, var)[0]}</td>\n",
    "            <td style = 'border-left: 1px dashed black'>{mean_sd_range(df, var)[1]}</td>\n",
    "            <td style = 'border-left: 1px dashed black'>{pval_shapiro(df, var)}</td>\n",
    "        </tr>\n",
    "        \"\"\"\n",
    "        \n",
    "    tail = f\"\"\"</table>\"\"\"\n",
    "    \n",
    "    table = head + body + tail\n",
    "    \n",
    "    display(HTML(table))\n",
    "\n",
    "def univar_cat(df, var_lst):\n",
    "    body = f\"\"\"\"\"\"\n",
    "    head = f\"\"\"\n",
    "    <table class = \"table_1\" style = \"width: 75% !important\">\n",
    "        <tr class = \"head_tr\">\n",
    "            <th>Parameter</th>\n",
    "            <th>Modalities</th>\n",
    "            <th>Observations (<i>n</i>)</th>\n",
    "            <th>Observations (%)</th>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    for var in var_lst:\n",
    "        body += f\"\"\"\n",
    "            <tr style = 'border-top: 1px solid black'>\n",
    "                <td rowspan = \"{len(var.mod)}\">{var.full_name}</td>\"\"\"\n",
    "        for mod, name in zip(var.mod, var.mod_names):\n",
    "            body += f\"\"\"\n",
    "                <td style = 'border-left: 1px dashed black'>{name}</td>\n",
    "                <td style = 'border-left: 1px dashed black'>{df[var.name].value_counts()[mod]}</td>\n",
    "                <td style = 'border-left: 1px dashed black'>{df[var.name].value_counts(normalize = True)[mod]:.1%}</td>\n",
    "            </tr>\n",
    "            \"\"\"\n",
    "        \n",
    "    tail = f\"\"\"</table>\"\"\"\n",
    "    \n",
    "    table = head + body + tail\n",
    "    \n",
    "    display(HTML(table))    \n",
    "    \n",
    "    \n",
    "def mwu_cardio(df, var):\n",
    "    \"\"\"\n",
    "    Computes a Mann-Whitney test for the difference in the input characteristic between patients with and without cardiovascular disease.\n",
    "\n",
    "    Args:\n",
    "    var: str\n",
    "    The name of the characteristic to compare between patients with and without cardiovascular disease.\n",
    "\n",
    "    Returns:\n",
    "    pval: float\n",
    "    The p-value of the Mann-Whitney U test.\n",
    "    \"\"\"\n",
    "    mwu, pval = mannwhitneyu(df[df['cardio'] == \"0\"][var.name], df[df['cardio'] == \"1\"][var.name])\n",
    "    return pval\n",
    "\n",
    "def mwu(df1, df2, var):\n",
    "\n",
    "    mwu, pval = mannwhitneyu(df1[var.name], df2[var.name])\n",
    "    return pval_txt(pval)\n",
    "\n",
    "def pp_3(df, var):\n",
    "    return f\"\"\"N: {df[var.name].value_counts(normalize = True)[\"1\"]:.1%}\n",
    "    <br>+: {df[var.name].value_counts(normalize = True)[\"2\"]:.1%}\n",
    "    <br>++: {df[var.name].value_counts(normalize = True)[\"3\"]:.1%}\n",
    "    \"\"\"\n",
    "\n",
    "def pp_3_cardio(df, var, card):\n",
    "    if card == \"0\":\n",
    "        df = df[df['cardio'] == \"0\"]\n",
    "    else:\n",
    "        df = df[df['cardio'] == \"1\"]\n",
    "    return f\"\"\"N: {df[var.name].value_counts(normalize = True)[\"1\"]:.1%}\n",
    "    <br>+: {df[var.name].value_counts(normalize = True)[\"2\"]:.1%}\n",
    "    <br>++: {df[var.name].value_counts(normalize = True)[\"3\"]:.1%}\n",
    "    \"\"\"\n",
    "\n",
    "def chi2_cardio(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the chi-squared statistic and p-value for a categorical variable in relation to 'cardio'.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the categorical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A text description of the p-value result.\n",
    "    \"\"\"\n",
    "    crosstab = pd.crosstab(df[var.name], df['cardio'])\n",
    "    chi2, pval, _, _ = chi2_contingency(crosstab)\n",
    "    return pval_txt(pval)\n",
    "\n",
    "def chi2_var(df, var1, var2):\n",
    "    \"\"\"\n",
    "    Calculates the chi-squared statistic and p-value for a contingency table of two categorical variables.\n",
    "\n",
    "    Parameters:\n",
    "    - var1 (str): The name of the first categorical variable in the DataFrame 'df'.\n",
    "    - var2 (str): The name of the second categorical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A text description of the p-value result.\n",
    "    \"\"\"\n",
    "    crosstab = pd.crosstab(df[var1.name], df[var2.name])\n",
    "    chi2, pval, _, _ = chi2_contingency(crosstab)\n",
    "    return pval_txt(pval)\n",
    "\n",
    "def mean_sd(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation for a numerical variable in the DataFrame 'df' \n",
    "    and return the results as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the numerical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string representing the mean and standard deviation in the format 'mean ± sd'.\n",
    "    \"\"\"\n",
    "    return f'{np.mean(df[var.name]):.1f} ± {np.std(df[var.name]):.1f}'\n",
    "\n",
    "def mean_sd_1(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation for a numerical variable in the subset of the DataFrame 'df' \n",
    "    where 'cardio' equals 1 and return the results as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the numerical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string representing the mean and standard deviation in the format 'mean ± sd'.\n",
    "    \"\"\"\n",
    "    return f'{np.mean(df[df.cardio == \"1\"][var.name]):.1f} ± {np.std(df[df.cardio == \"1\"][var.name]):.1f}'\n",
    "\n",
    "def mean_sd_0(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation for a numerical variable in the subset of the DataFrame 'df' \n",
    "    where 'cardio' equals 0 and return the results as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the numerical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string representing the mean and standard deviation in the format 'mean ± sd'.\n",
    "    \"\"\"\n",
    "    return f'{np.mean(df[df.cardio == \"0\"][var.name]):.1f} ± {np.std(df[df.cardio == \"0\"][var.name]):.1f}'\n",
    "\n",
    "def pp(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of positive occurrences (1) for a binary categorical variable in the DataFrame 'df' \n",
    "    and return the result as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the binary categorical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string representing the percentage of positive occurrences in the format 'xx.x%'.\n",
    "    \"\"\"\n",
    "    return f'{df[var.name].value_counts(normalize = True)[1]:.1%}'\n",
    "    \n",
    "def pp_0(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of positive occurrences (1) for a binary categorical variable in the subset of \n",
    "    the DataFrame 'df' where 'cardio' equals 0, and return the result as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the binary categorical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string representing the percentage of positive occurrences in the format 'xx.x%'.\n",
    "    \"\"\"\n",
    "    return f'{df[df.cardio == \"0\"][var.name].value_counts(normalize = True)[1]:.1%}'\n",
    "    \n",
    "def pp_1(df, var):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of positive occurrences (1) for a binary categorical variable in the subset of \n",
    "    the DataFrame 'df' where 'cardio' equals 1, and return the result as a formatted string.\n",
    "\n",
    "    Parameters:\n",
    "    - var (str): The name of the binary categorical variable in the DataFrame 'df'.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string representing the percentage of positive occurrences in the format 'xx.x%'.\n",
    "    \"\"\"\n",
    "    return f'{df[df.cardio == \"1\"][var.name].value_counts(normalize = True)[1]:.1%}'\n",
    "\n",
    "def min_max(df, var):\n",
    "    return f'[{np.min(df[var.name]):.1f} - {np.max(df[var.name]):.1f}]'\n",
    "\n",
    "def matrix_display(matrix):\n",
    "    \"\"\"\n",
    "    Displays a confusion matrix in a formatted HTML style.\n",
    "\n",
    "    Parameters:\n",
    "    - matrix (list of lists): A 2x2 confusion matrix containing True Negatives, False Positives,\n",
    "      False Negatives, and True Positives.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    return    f\"\"\" <table style = 'border 1px solid black; width : 70%; border: 1px solid black'>\n",
    "        <tr style = 'background-color: gray; color: white; border: 1px solid black'>\n",
    "            <th>Classification</th>\n",
    "            <th>Samples (<i>n</i>)</th>\n",
    "            <th>Samples (%)</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th colspan = \"3\" style = 'border: 1px solid black; background-color: lightcyan; font-variant: small-caps; font-weight:bold; text-align: left ! important; line-height: 0.6'>Class 0: Absence of CV disease</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>True Negatives</td>\n",
    "            <td>{matrix[0][0]}</td>\n",
    "            <td style = 'color: green; font-weight: bold'>{matrix[0][0]/matrix[0].sum():.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>False Positives</td>\n",
    "            <td>{matrix[0][1]}</td>\n",
    "            <td style = 'color: firebrick; font-weight: bold'>{matrix[0][1]/matrix[0].sum():.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th colspan = \"3\" style = 'border: 1px solid black; background-color: lightcyan; font-variant: small-caps; font-weight:bold; text-align: left ! important; line-height: 0.6'>Class 1: Presence of CV disease</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>True Positives</td>\n",
    "            <td>{matrix[1][1]}</td>\n",
    "            <td style = 'color: green; font-weight: bold'>{matrix[1][1]/matrix[1].sum():.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>False Negatives</td>\n",
    "            <td>{matrix[1][0]}</td>\n",
    "            <td style = 'color: firebrick; font-weight: bold'>{matrix[1][0]/matrix[1].sum():.1%}</td>\n",
    "        </tr>\n",
    "    </table>\"\"\"\n",
    "    \n",
    "def classification_plot(matrix):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (8, 6))\n",
    "\n",
    "    plt.subplots_adjust(wspace = 1)\n",
    "\n",
    "    ax[0].pie(matrix[0], labels = ['True Negatives', 'False Positives'], autopct = '%.0f%%', startangle=90, colors = ['palegreen', 'lightcoral'], wedgeprops = {'edgecolor' : 'black'}, textprops = {'fontsize' : '14'})\n",
    "    ax[0].set_title('Classification: Class 0', fontdict = fontdict_title)\n",
    "    ax[1].pie(matrix[1][::-1], labels = ['True Positives','False Negatives'], autopct = '%.0f%%', startangle=90, colors = ['palegreen', 'lightcoral'], wedgeprops = {'edgecolor' : 'black'}, textprops = {'fontsize' : '14'})\n",
    "    ax[1].set_title('Classification: Class 1', fontdict = fontdict_title)\n",
    "\n",
    "    plt.show()  \n",
    "\n",
    "def perf_barplot_with_crossval(classification_rep, df_cv):\n",
    "    \n",
    "    dfrep = pd.DataFrame(classification_rep).reset_index().rename(columns={'index': 'Metric'})\n",
    "    dfrep_m = pd.melt(dfrep[['Metric', '0.0', '1.0']], id_vars = \"Metric\", var_name = \"Class\", value_name = \"Score\")\n",
    "    dfrep_m2 = pd.merge(dfrep_m, dfrep[['Metric', 'accuracy', 'macro avg', 'weighted avg']], on = \"Metric\", how = \"left\")\n",
    "    dfrep_m2.loc[dfrep_m2[\"Metric\"] != \"support\", [\"Score\", \"macro avg\", \"weighted avg\"]] *= 100\n",
    "    dfrep_m2[\"accuracy\"] *= 100\n",
    "    \n",
    "    df_cv_melted = df_cv.melt(var_name='Metric', value_name='Value')\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize = (12,5))\n",
    "    plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "    sns.barplot(x = 'Metric', \n",
    "                y = \"Score\", \n",
    "                data = dfrep_m2.loc[dfrep_m2[\"Metric\"] != \"support\"], \n",
    "                hue = \"Class\",\n",
    "                palette = ['yellow', 'darkorchid'],\n",
    "                edgecolor = \"black\",\n",
    "                ax = ax[0],\n",
    "               )\n",
    "    \n",
    "    sns.barplot(x = 'Metric',\n",
    "                y = 'Value',\n",
    "                data = df_cv_melted.loc[df_cv_melted['Metric'] != \"AUC\"],\n",
    "                palette = ['mediumvioletred', 'palegreen', 'deepskyblue', 'coral', 'gainsboro'],\n",
    "                edgecolor = 'black',\n",
    "                ax = ax[1]\n",
    "               )\n",
    "    \n",
    "    for y in [50, 75, 90, 100]:\n",
    "        ax[0].axhline(y = y, linestyle = 'dotted', color = 'black')\n",
    "        ax[1].axhline(y = y, linestyle = 'dotted', color = 'black')\n",
    "        ax[1].text(3.6, y, f'{y}%')\n",
    "    \n",
    "    ax[0].set_xticklabels(ax[0].get_xticklabels(), fontdict = {\"size\" : \"12\"})\n",
    "    ax[1].set_xticklabels(ax[1].get_xticklabels(), fontdict = {\"size\" : \"12\"})\n",
    "    \n",
    "    ax[0].set_xlabel('Metrics', fontdict = fontdict_labels)\n",
    "    ax[1].set_xlabel('Metrics', fontdict = fontdict_labels)\n",
    "    \n",
    "    ax[0].set_ylim([0,105])\n",
    "    ax[1].set_ylim([0,105])\n",
    "    \n",
    "    ax[0].set_yticks(ticks = range(0,105,10), labels = range(0,105,10))\n",
    "    ax[1].set_yticks(ticks = range(0,105,10), labels = range(0,105,10))\n",
    "    \n",
    "    ax[0].set_ylabel('Score (%)', fontdict = fontdict_labels)\n",
    "    ax[1].set_ylabel('Mean Value for Class 1 (%)', fontdict = fontdict_labels)\n",
    "    \n",
    "    ax[0].set_title('Classification Performances', fontdict = fontdict_title)\n",
    "    ax[1].set_title('Cross Validation Results', fontdict = fontdict_title)\n",
    "    \n",
    "    handles, _ = ax[0].get_legend_handles_labels()\n",
    "    legend = ax[0].legend(handles=handles, labels=['Class 0', 'Class 1'], title= None, framealpha = 1, facecolor = 'white', edgecolor = 'black')\n",
    "    legend.get_title().set_fontsize('12')\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontsize('10')\n",
    "    \n",
    "    plt.show()    \n",
    "\n",
    "def perf_barplot(classification_rep):\n",
    "    \n",
    "    dfrep = pd.DataFrame(classification_rep).reset_index().rename(columns={'index': 'Metric'})\n",
    "    dfrep_m = pd.melt(dfrep[['Metric', '0.0', '1.0']], id_vars = \"Metric\", var_name = \"Class\", value_name = \"Score\")\n",
    "    dfrep_m2 = pd.merge(dfrep_m, dfrep[['Metric', 'accuracy', 'macro avg', 'weighted avg']], on = \"Metric\", how = \"left\")\n",
    "    dfrep_m2.loc[dfrep_m2[\"Metric\"] != \"support\", [\"Score\", \"macro avg\", \"weighted avg\"]] *= 100\n",
    "    dfrep_m2[\"accuracy\"] *= 100\n",
    "    \n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 1, figsize = (8,5))\n",
    "    plt.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "    sns.barplot(\n",
    "        x = 'Metric', \n",
    "        y = \"Score\", \n",
    "        data = dfrep_m2.loc[dfrep_m2[\"Metric\"] != \"support\"], \n",
    "        hue = \"Class\",\n",
    "        palette = ['yellow', 'darkorchid'],\n",
    "        edgecolor = \"black\",\n",
    "    )\n",
    "    \n",
    "    for y in [50, 75, 90, 100]:\n",
    "        ax.axhline(y = y, linestyle = 'dotted', color = 'black')\n",
    "        ax.text(2.55, y, f'{y}%')\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontdict = {\"size\" : \"12\"})\n",
    "    ax.set_xlabel('Metrics', fontdict = fontdict_labels)\n",
    "    ax.set_ylim([0,105])\n",
    "\n",
    "    \n",
    "    ax.set_yticks(ticks = range(0,105,10), labels = range(0,105,10))\n",
    "    ax.set_ylabel('Score (%)', fontdict = fontdict_labels)\n",
    "    ax.set_title('Classification Performances', fontdict = fontdict_title)\n",
    "    \n",
    "    handles, _ = ax.get_legend_handles_labels()\n",
    "    legend = ax.legend(handles=handles, labels=['Class 0', 'Class 1'], title= None, framealpha = 1, facecolor = 'white', edgecolor = 'black')\n",
    "    legend.get_title().set_fontsize('12')\n",
    "    for text in legend.get_texts():\n",
    "        text.set_fontsize('10')\n",
    "    \n",
    "    plt.show()    \n",
    "    \n",
    "def report_display(report):\n",
    "    \"\"\"\n",
    "    Displays a classification report in a formatted HTML style.\n",
    "\n",
    "    Parameters:\n",
    "    - report (dict): A dictionary containing classification report metrics, typically generated by\n",
    "      scikit-learn's `classification_report` function.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    display(HTML(f\"\"\"\n",
    "        <div class = 'all'>\n",
    "            <h3>Detailed Metrics</h3>\n",
    "            <h4><span style = 'color : indigo'>Class 0 (No Cardiovascular Disease):</span></h4>\n",
    "            <ul>\n",
    "                <li>Precision: <b>{report['0.0']['precision']:.1%}</b></li>\n",
    "                <li>Recall: <b>{report['0.0']['recall']:.1%}</b></li>\n",
    "                <li>f1-score: <b>{report['0.0']['f1-score']:.1%}</b></li>\n",
    "                <li>support: <b>{report['0.0']['support']}</b></li>\n",
    "            </ul>\n",
    "            <h4><span style = 'color : indigo'>Class 1 (Cardiovascular Disease):</span></h4>\n",
    "            <ul>\n",
    "                <li>Precision: <b>{report['1.0']['precision']:.1%}</b></li>\n",
    "                <li>Recall: <b>{report['1.0']['recall']:.1%}</b></li>\n",
    "                <li>f1-score: <b>{report['1.0']['f1-score']:.1%}</b></li>\n",
    "                <li>support: <b>{report['1.0']['support']}</b></li>\n",
    "            </ul>\n",
    "            <h4><span style = 'color : indigo'>Overall Model Performance:</span></h4>\n",
    "            <ul>\n",
    "                <li>Accuracy: <b>{report['accuracy']:.1%}</b></li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "    \n",
    "    \n",
    "def report_table(perf_report):\n",
    "    \"\"\"\n",
    "    Displays a classification report in a formatted HTML table.\n",
    "\n",
    "    Parameters:\n",
    "    - report (dict): A dictionary containing classification report metrics, typically generated by\n",
    "      scikit-learn's `classification_report` function.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    display(HTML(f\"\"\"\n",
    "        <div class = 'all'>\n",
    "            <h3>Detailed Metrics</h3>\n",
    "            <table style = 'border: 1px solid black; width: 70%'>\n",
    "                <tr style = 'border: 1px solid black; color: white; background-color: grey'>\n",
    "                    <th style = 'border-right: 1px solid black'>Metric</th>\n",
    "                    <th>Class 0 <br>Controls</th>\n",
    "                    <th>Class 1 <br>Patients</th>\n",
    "                    <th>Macro Average</th>\n",
    "                    <th>Weighted Average</th>\n",
    "                </tr>\n",
    "                <tr style = 'border-bottom: 1px solid black'>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Accuracy</b></td>\n",
    "                    <td colspan = \"4\"><b>{perf_report['accuracy']:.1%}</b></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Precision</b></td>\n",
    "                    <td>{perf_report['0.0']['precision']:.1%}</td>\n",
    "                    <td>{perf_report['1.0']['precision']:.1%}</td>\n",
    "                    <td>{perf_report['macro avg']['precision']:.1%}</td>\n",
    "                    <td>{perf_report['weighted avg']['precision']:.1%}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Recall</b></td>\n",
    "                    <td>{perf_report['0.0']['recall']:.1%}</td>\n",
    "                    <td>{perf_report['1.0']['recall']:.1%}</td>\n",
    "                    <td>{perf_report['macro avg']['recall']:.1%}</td>\n",
    "                    <td>{perf_report['weighted avg']['recall']:.1%}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>f1-score</b></td>\n",
    "                    <td>{perf_report['0.0']['f1-score']:.1%}</td>\n",
    "                    <td>{perf_report['1.0']['f1-score']:.1%}</td>\n",
    "                    <td>{perf_report['macro avg']['f1-score']:.1%}</td>\n",
    "                    <td>{perf_report['weighted avg']['f1-score']:.1%}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Support</b></td>\n",
    "                    <td>{perf_report['0.0']['support']}</td>\n",
    "                    <td>{perf_report['1.0']['support']}</td>\n",
    "                    <td>{perf_report['macro avg']['support']}</td>\n",
    "                    <td>{perf_report['weighted avg']['support']}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    \"\"\"))\n",
    "\n",
    "    \n",
    "def report_table_with_crossval(perf_report, cv_report):\n",
    "    display(HTML(f\"\"\"\n",
    "        <div class = 'all'>\n",
    "            <h3>Detailed Metrics</h3>\n",
    "            <table style = 'border: 1px solid black; width: 80%'>\n",
    "                <tr style = 'border: 1px solid black; color: white; background-color: grey'>\n",
    "                    <th style = 'border-right: 1px solid black'>Metric</th>\n",
    "                    <th>Class 0 <br>Controls</th>\n",
    "                    <th>Class 1 <br>Patients</th>\n",
    "                    <th>Macro Average</th>\n",
    "                    <th>Weighted Average</th>\n",
    "                    <th style = 'border-left: 1px dashed black'>Cross-Validation<br><i>n</i>=5</th>\n",
    "                </tr>\n",
    "                <tr style = 'border: 1px solid black'>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Accuracy</b></td>\n",
    "                    <td colspan = \"4\"><b>{perf_report['accuracy']:.1%}</b></td>\n",
    "                    <td style = 'border-left: 1px dashed black'>{np.mean(cv_report[\"Accuracy\"]):.1f} ± {np.std(cv_report[\"Accuracy\"]):.1f} %</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Precision</b></td>\n",
    "                    <td>{perf_report['0.0']['precision']:.1%}</td>\n",
    "                    <td>{perf_report['1.0']['precision']:.1%}</td>\n",
    "                    <td>{perf_report['macro avg']['precision']:.1%}</td>\n",
    "                    <td>{perf_report['weighted avg']['precision']:.1%}</td>\n",
    "                    <td style = 'border-left: 1px dashed black'>{np.mean(cv_report[\"Precision\"]):.1f} ± {np.std(cv_report[\"Precision\"]):.1f} %</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Recall</b></td>\n",
    "                    <td>{perf_report['0.0']['recall']:.1%}</td>\n",
    "                    <td>{perf_report['1.0']['recall']:.1%}</td>\n",
    "                    <td>{perf_report['macro avg']['recall']:.1%}</td>\n",
    "                    <td>{perf_report['weighted avg']['recall']:.1%}</td>\n",
    "                    <td style = 'border-left: 1px dashed black'>{np.mean(cv_report[\"Recall\"]):.1f} ± {np.std(cv_report[\"Recall\"]):.1f} %</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>f1-score</b></td>\n",
    "                    <td>{perf_report['0.0']['f1-score']:.1%}</td>\n",
    "                    <td>{perf_report['1.0']['f1-score']:.1%}</td>\n",
    "                    <td>{perf_report['macro avg']['f1-score']:.1%}</td>\n",
    "                    <td>{perf_report['weighted avg']['f1-score']:.1%}</td>\n",
    "                    <td style = 'border-left: 1px dashed black'>{np.mean(cv_report[\"f1-score\"]):.1f} ± {np.std(cv_report[\"f1-score\"]):.1f} %</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style = 'border-right: 1px solid black'><b>Support</b></td>\n",
    "                    <td>{perf_report['0.0']['support']}</td>\n",
    "                    <td>{perf_report['1.0']['support']}</td>\n",
    "                    <td>{perf_report['macro avg']['support']}</td>\n",
    "                    <td>{perf_report['weighted avg']['support']}</td>\n",
    "                    <td style = 'border-left: 1px dashed black'>N/A</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "        </div>\n",
    "    \"\"\"))\n",
    "    \n",
    "def feature_importance(df, var_column, coefs_columns, x_lim = [-2,9]):\n",
    "    display(HTML(\n",
    "    f\"\"\"\n",
    "    <div class = 'all'>\n",
    "        <h3>Feature Importance</h3>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    ))\n",
    "    df = df.sort_values(by=coefs_columns, ascending=False)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "    sns.barplot(\n",
    "        y = var_column, \n",
    "        x = coefs_columns,\n",
    "        data = df,\n",
    "        edgecolor = 'black', \n",
    "        ax = ax\n",
    "    )\n",
    "\n",
    "    ax.set_ylabel('Features', fontdict = fontdict_labels)\n",
    "\n",
    "    ax.set_xlim(x_lim)\n",
    "    ax.set_xticks(ticks = range(-2,10,1), labels = range(-2,10,1))\n",
    "    ax.set_xlabel('Feature Coefficient', fontdict = fontdict_labels)\n",
    "\n",
    "    ax.set_title(None)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6f21cb",
   "metadata": {},
   "source": [
    "<h4>Code for: Data viz tools</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2339b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilitaries for data viz\n",
    "fontdict_title = {'color' : 'navy', 'family' : 'Trebuchet MS', 'size' : 16, 'weight' : 'bold'}\n",
    "fontdict_labels = {'color': 'black', 'family': 'Trebuchet MS', 'size' : 14}\n",
    "palette_cardio = {\"0\":'lightcyan', \"1\": 'firebrick'}\n",
    "palette_sex = {\"female\": \"coral\", \"male\" : \"seagreen\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a909680",
   "metadata": {},
   "source": [
    "<h4>Code for: Preprocessing</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ed30028",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class = 'all'>\n",
       "    <h1>Parameter hypertuning</h1>\n",
       "    <h2>Datasets</h2>\n",
       "    <p>\n",
       "        The same dataset used for model screening in Part II will be used for parameter hypertuning. \n",
       "        It contains 67189 rows and 12 columns. Features are limited to the original features.\n",
       "    </p>\n",
       "    <h2>Preprocessing Operations</h2>\n",
       "    <h3>Feature Encoding</h3>\n",
       "    <p>\n",
       "        <code>sex</code> was droped after <code>female_sex</code> was created, were females are encoded <code>1</code>\n",
       "        and males <code>0</code>. There were no unordered categorical features left.\n",
       "        Ordered categorical features, <i>ie</i> <code>cholesterol</code> and <code>gluc</code> \n",
       "        were encoded using <code>LabelEncoder()</code>, as their modalities were originally passed as strings.\n",
       "    </p>\n",
       "    <h3>Normalisation</h3>\n",
       "    <p>\n",
       "        Among preprocessing techniques, normalisation aims at scaling data features to a specific range, \n",
       "        often between 0 and 1. It felt appropriate since the scale of the features of this dataset varies significantly, \n",
       "        with majority of features having a non-gaussian distribution. \n",
       "        Normalisation helps bring all features to a common scale, making them directly comparable.\n",
       "    </p>\n",
       "    <h3>Train and Test Datasets</h3>\n",
       "    <p>\n",
       "        A value of <code>0.3</code> was set for <code>test_size</code> parameter. The training datasets\n",
       "        has a shape of 47032 rows and 11 columns. Head and tail\n",
       "        of the training dataset are shown hereafter:\n",
       "    </p>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>female_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>62257</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.381579</td>\n",
       "      <td>0.082384</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.328467</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50733</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37820</th>\n",
       "      <td>0.68</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.075687</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29104</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.102478</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.255474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47060</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.216343</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.255474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    height    weight     ap_hi     ap_lo  cholesterol  gluc  smoke  \\\n",
       "62257  0.76  0.381579  0.082384  0.470588  0.328467          0.5   0.0    0.0   \n",
       "50733  0.64  0.315789  0.022103  0.117647  0.109489          0.5   0.0    0.0   \n",
       "37820  0.68  0.421053  0.075687  0.235294  0.109489          0.0   0.0    0.0   \n",
       "29104  0.64  0.486842  0.102478  0.294118  0.255474          0.0   0.0    0.0   \n",
       "47060  0.56  0.434211  0.216343  0.294118  0.255474          0.0   0.0    0.0   \n",
       "\n",
       "       alco  active  female_sex  \n",
       "62257   0.0     1.0         1.0  \n",
       "50733   0.0     1.0         1.0  \n",
       "37820   0.0     1.0         0.0  \n",
       "29104   0.0     1.0         1.0  \n",
       "47060   0.0     1.0         1.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>weight</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>female_sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5436</th>\n",
       "      <td>0.88</td>\n",
       "      <td>0.565789</td>\n",
       "      <td>0.042197</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36555</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51603</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.460526</td>\n",
       "      <td>0.189551</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.328467</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56692</th>\n",
       "      <td>0.56</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.169457</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30248</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.095780</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.255474</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age    height    weight     ap_hi     ap_lo  cholesterol  gluc  smoke  \\\n",
       "5436   0.88  0.565789  0.042197  0.352941  0.109489          0.0   0.0    0.0   \n",
       "36555  0.20  0.368421  0.022103  0.117647  0.109489          0.0   0.0    0.0   \n",
       "51603  0.48  0.460526  0.189551  0.352941  0.328467          0.0   0.0    1.0   \n",
       "56692  0.56  0.447368  0.169457  0.235294  0.182482          0.0   0.0    0.0   \n",
       "30248  0.76  0.500000  0.095780  0.294118  0.255474          0.0   0.0    0.0   \n",
       "\n",
       "       alco  active  female_sex  \n",
       "5436    0.0     1.0         0.0  \n",
       "36555   0.0     1.0         1.0  \n",
       "51603   1.0     1.0         0.0  \n",
       "56692   0.0     0.0         1.0  \n",
       "30248   0.0     1.0         0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('clean_cvd.csv', sep = \",\")            # The very same df that was used on Part I\n",
    "\n",
    "   \n",
    "# Dropping created features from Part I\n",
    "df = df.drop(['age_group', 'lifestyle', 'ap_m', 'ap_aha', 'healthy_ls', 'bmi'], axis = 1)\n",
    "\n",
    "# Turning Sex into a binary feature\n",
    "df['female_sex'] = df.apply(lambda row: 1 if row['sex'] == 'female' else 0, axis = 1)\n",
    "df = df.drop(['sex'], axis = 1)\n",
    "\n",
    "# Encoding ordered categorical features\n",
    "label_encoder = LabelEncoder()              \n",
    "df[\"cholesterol\"] = label_encoder.fit_transform(df[\"cholesterol\"])\n",
    "df[\"gluc\"] = label_encoder.fit_transform(df[\"gluc\"])\n",
    "\n",
    "# Setting datatypes\n",
    "df[['age', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio', 'female_sex']] = df[['age', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'cardio', 'female_sex']].astype('int64')\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "columns_df = df.columns          # Saving columns names before scaling\n",
    "df = scaler.fit_transform(df)\n",
    "df = pd.DataFrame(df, columns = columns_df)\n",
    "\n",
    "# Train and Test datasets\n",
    "X = df.drop(\"cardio\", axis = 1)\n",
    "y = df['cardio']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 46)\n",
    "\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Parameter hypertuning</h1>\n",
    "    <h2>Datasets</h2>\n",
    "    <p>\n",
    "        The same dataset used for model screening in Part II will be used for parameter hypertuning. \n",
    "        It contains {df.shape[0]} rows and {df.shape[1]} columns. Features are limited to the original features.\n",
    "    </p>\n",
    "    <h2>Preprocessing Operations</h2>\n",
    "    <h3>Feature Encoding</h3>\n",
    "    <p>\n",
    "        <code>sex</code> was droped after <code>female_sex</code> was created, were females are encoded <code>1</code>\n",
    "        and males <code>0</code>. There were no unordered categorical features left.\n",
    "        Ordered categorical features, <i>ie</i> <code>cholesterol</code> and <code>gluc</code> \n",
    "        were encoded using <code>LabelEncoder()</code>, as their modalities were originally passed as strings.\n",
    "    </p>\n",
    "    <h3>Normalisation</h3>\n",
    "    <p>\n",
    "        Among preprocessing techniques, normalisation aims at scaling data features to a specific range, \n",
    "        often between 0 and 1. It felt appropriate since the scale of the features of this dataset varies significantly, \n",
    "        with majority of features having a non-gaussian distribution. \n",
    "        Normalisation helps bring all features to a common scale, making them directly comparable.\n",
    "    </p>\n",
    "    <h3>Train and Test Datasets</h3>\n",
    "    <p>\n",
    "        A value of <code>0.3</code> was set for <code>test_size</code> parameter. The training datasets\n",
    "        has a shape of {X_train.shape[0]} rows and {X_train.shape[1]} columns. Head and tail\n",
    "        of the training dataset are shown hereafter:\n",
    "    </p>\n",
    "</div>\"\"\"))\n",
    "display(X_train.head())\n",
    "display(X_train.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495b956a",
   "metadata": {},
   "source": [
    "<h4>Code for: Creating a df that stores performances for different parameters combination</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9d7b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scale_gamma = 0.691955007227351\n",
    "# df_svm = pd.read_csv('df_svm.csv', sep = ';')\n",
    "\n",
    "def svm_model(C = 1, gamma = \"scale\", kernel = 'rbf', degree = 3):\n",
    "    svm = SVC(C = C, gamma = gamma, kernel = kernel, degree = degree)\n",
    "    svm.fit(X_train, y_train)\n",
    "    smv_pred = svm.predict(X_test)\n",
    "\n",
    "    scoring_metrics = ['accuracy', 'recall', 'precision', 'f1']\n",
    "    scores = cross_validate(svm, X, y, cv=3, scoring=scoring_metrics)\n",
    "    \n",
    "    results ={\n",
    "        \"C\": svm.get_params()['C'],\n",
    "        \"gamma\": svm.get_params()['gamma'],\n",
    "        \"kernel\": svm.get_params()['kernel'],\n",
    "        \"degree\": svm.get_params()['degree'],\n",
    "        \"fit_time\": np.mean(scores['fit_time']),\n",
    "        \"score_time\": np.mean(scores['score_time']),\n",
    "        \"accuracy\": np.mean(scores['test_accuracy']),\n",
    "        \"recall\": np.mean(scores['test_recall']),\n",
    "        \"precision\": np.mean(scores['test_precision']),\n",
    "        \"f1\": np.mean(scores['test_f1'])\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# This function as well as the following lines of code were used to created a csv file that stores the performances of the\n",
    "# hypertuning experiments. It should be uncommented out if more tests are required for the analysis.\n",
    "\n",
    "# for c_val in [0.01, 0.1, 1, 10]:\n",
    "#     C = c_val\n",
    "#     gamma = 1\n",
    "#     kernel = \"sigmoid\"\n",
    "#     degree = 3\n",
    "\n",
    "#     test_results = svm_model(C, gamma, kernel, degree)\n",
    "#     df_svm = df_svm.append(test_results, ignore_index = True)    \n",
    "\n",
    "# df_svm.to_csv('df_svm.csv', sep = ';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6871dd38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class = 'all'>\n",
       "Based on sample_id, <b>100.0%</b> of rows is unique.\n",
       "\n",
       "    <table class = 'table_1'>\n",
       "        <tr>\n",
       "            <th>Kernel</th>\n",
       "            <th>Experiments (<i>n</i>)</th>\n",
       "            <th>Accuracy</th>\n",
       "            <th>Recall</th>\n",
       "            <th>Precision</th>\n",
       "            <th>F1-score</th>            \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Linear</b></td>\n",
       "            <td>21</td>\n",
       "            <td>72.2% ± 0.3%</td>\n",
       "            <td>63.5% ± 0.1%</td>\n",
       "            <td>76.9% ± 0.5%</td>\n",
       "            <td>69.5% ± 0.3%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>RBF</b></td>\n",
       "            <td>21</td>\n",
       "            <td>71.2% ± 3.5%</td>\n",
       "            <td>64.8% ± 10.6%</td>\n",
       "            <td>74.5% ± 2.4%</td>\n",
       "            <td>68.6% ± 8.4%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Sigmoid</b></td>\n",
       "            <td>21</td>\n",
       "            <td>53.6% ± 10.7%</td>\n",
       "            <td>42.4% ± 19.8%</td>\n",
       "            <td>49.4% ± 20.3%</td>\n",
       "            <td>44.7% ± 21.0%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Polynomial</b><br>(all)</td>\n",
       "            <td>32</td>\n",
       "            <td>69.0% ± 7.6%</td>\n",
       "            <td>54.5% ± 22.0%</td>\n",
       "            <td>69.6% ± 22.4%</td>\n",
       "            <td>59.9% ± 22.8%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Polynomial</b><br>(degree = 2)</td>\n",
       "            <td>5</td>\n",
       "            <td>72.6% ± 0.5%</td>\n",
       "            <td>64.2% ± 1.0%</td>\n",
       "            <td>77.1% ± 0.3%</td>\n",
       "            <td>70.1% ± 0.7%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Polynomial</b><br>(degree = 3)</td>\n",
       "            <td>18</td>\n",
       "            <td>66.3% ± 9.3%</td>\n",
       "            <td>46.7% ± 26.8%</td>\n",
       "            <td>63.9% ± 28.6%</td>\n",
       "            <td>52.0% ± 27.9%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Polynomial</b><br>(degree = 4)</td>\n",
       "            <td>5</td>\n",
       "            <td>72.5% ± 0.6%</td>\n",
       "            <td>64.8% ± 2.2%</td>\n",
       "            <td>76.6% ± 0.4%</td>\n",
       "            <td>70.2% ± 1.2%</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>All</b></td>\n",
       "            <td>95</td>\n",
       "            <td>66.8% ± 9.9%</td>\n",
       "            <td>56.1% ± 18.6%</td>\n",
       "            <td>67.8% ± 19.1%</td>\n",
       "            <td>60.6% ± 19.4%</td>\n",
       "        </tr>\n",
       "    </table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div class = 'all'>\n",
       "    <table class = 'table_1'>\n",
       "        <tr>\n",
       "            <th>Kernel</th>\n",
       "            <th>Max Accuracy</th>\n",
       "            <th>Min Accuracy</th>\n",
       "            <th>Max Recall</th>\n",
       "            <th>Min Recall</th>          \n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Linear</b></td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>72.4%</b>\n",
       "                <br>- <code>C = 1.0</code>\n",
       "                <br>- <code>gamma = scale</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>71.7%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = scale</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>63.6%</b>\n",
       "                <br>- <code>C = 1.0</code>\n",
       "                <br>- <code>gamma = scale</code>\n",
       "            </td>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>63.3%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = scale</code>\n",
       "            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>RBF</b></td>\n",
       "            <td style = 'text-align: left !important; background-color: palegreen; border: 2px solid forestgreen'>- Value: <b>73.2%</b>\n",
       "                <br>- <code>C = 10.0</code>\n",
       "                <br>- <code>gamma = scale</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>57.1%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important; background-color: palegreen; border: 2px solid forestgreen'>- Value: <b>76.6%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 10</code>\n",
       "            </td>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>20.3%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Sigmoid</b></td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>72.3%</b>\n",
       "                <br>- <code>C = 10.0</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important; background-color: lightsalmon; border: 2px solid firebrick'>- Value: <b>38.9%</b>\n",
       "                <br>- <code>C = 10.0</code>\n",
       "                <br>- <code>gamma = 10</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>63.5%</b>\n",
       "                <br>- <code>C = 10.0</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>0.0%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>Polynomial</b><br>(degree = 3)</td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>73.2%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 10</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>50.0%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>67.4%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 10</code>\n",
       "            </td>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important; background-color: lightsalmon; border: 2px solid firebrick'>- Value: <b>0.0%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "            </td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td><b>All</b></td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>73.2%</b>\n",
       "                <br>- <code>C = 10.0</code>\n",
       "                <br>- <code>gamma = scale</code>\n",
       "                <br>- <code>kernel = rbf</code>               \n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>38.9%</b>\n",
       "                <br>- <code>C = 10.0</code>\n",
       "                <br>- <code>gamma = 10</code>\n",
       "                <br>- <code>kernel = sigmoid</code>                \n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>76.6%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 10</code>\n",
       "                <br>- <code>kernel = rbf</code>\n",
       "            </td>\n",
       "            </td>\n",
       "            <td style = 'text-align: left !important'>- Value: <b>0.0%</b>\n",
       "                <br>- <code>C = 0.01</code>\n",
       "                <br>- <code>gamma = 0.01</code>\n",
       "                <br>- <code>kernel = poly</code>\n",
       "            </td>\n",
       "        </tr>\n",
       "    <p>\n",
       "        <b>Linear</b> kernel displayed very limited variability for all metrics, suggesting parameter tuning had\n",
       "        little to no impact. Consequently, min and max values for both accuracy and recall were comparable. For both metrics, \n",
       "        <code>C = 1</code> and <code>gamma = \"scale\"</code> produced the best results.\n",
       "    </p>\n",
       "    <p>\n",
       "        <b>RBF</b> kernel led to the best values for both accuracy (<b>73.2%</b>) and recall\n",
       "        (<b>76.6%</b>). Interestingly, max value for accuracy was obtained with \n",
       "        <code>C = 10.0</code> and <code>gamma = \"scale\"</code> whereas best recall was obtained with\n",
       "        <code>C = 0.01</code> and <code>gamma = 10</code>, suggesting that finding balance between both <code>C</code>\n",
       "        and <code>gamma</code> is essential. Moderate variability was found with this kernel, and the lowest values for both\n",
       "        accuracy and recall were found with <code>C = 0.01</code> and <code>gamma = 0.01</code>.\n",
       "    </p>\n",
       "    <p>\n",
       "        <b>Polynomial</b> kernel (third degree) produced high variability, yet its max value for accuracy (<b>73.2%</b>)\n",
       "        was comparable to the max accuracy obtained with RBF. Max accuracy and max recall were both obtained with \n",
       "        <code>C = 0.01</code> and <code>gamma = 10</code>.\n",
       "    </p>\n",
       "\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_params = pd.read_csv('svm_screen_params.csv', sep = \";\")\n",
    "default_scale_gamma = 0.691955007227351\n",
    "df_params['degree'] = df_params.apply(lambda row: row['degree'] if row['kernel'] == 'poly' else np.nan, axis = 1)\n",
    "df_params['gamma_float'] = df_params.apply(lambda row: default_scale_gamma if row['gamma'] == 'scale' else row['gamma'], axis = 1)\n",
    "df_params['sample_id'] = df_params.apply(lambda row: f\"{row['C']}_{row['gamma']}_{row['kernel']}_{row['degree']}\", axis = 1)\n",
    "\n",
    "df_linear = df_params[df_params['kernel'] == 'linear']\n",
    "df_rbf = df_params[df_params['kernel'] == 'rbf']\n",
    "df_sigmoid = df_params[df_params['kernel'] == 'sigmoid']\n",
    "df_poly = df_params[df_params['kernel'] == 'poly']\n",
    "df_poly_2 = df_poly[df_poly['degree'] == 2]\n",
    "df_poly_3 = df_poly[df_poly['degree'] == 3]\n",
    "df_poly_4 = df_poly[df_poly['degree'] == 4]\n",
    "\n",
    "# Checking for duplicate experiments:\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "Based on sample_id, <b>{df_params['sample_id'].duplicated().value_counts(normalize = True)[False]:.1%}</b> of rows is unique.\n",
    "\n",
    "    <table class = 'table_1'>\n",
    "        <tr>\n",
    "            <th>Kernel</th>\n",
    "            <th>Experiments (<i>n</i>)</th>\n",
    "            <th>Accuracy</th>\n",
    "            <th>Recall</th>\n",
    "            <th>Precision</th>\n",
    "            <th>F1-score</th>            \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Linear</b></td>\n",
    "            <td>{len(df_linear)}</td>\n",
    "            <td>{np.mean(df_linear['accuracy']):.1%} ± {np.std(df_linear['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_linear['recall']):.1%} ± {np.std(df_linear['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_linear['precision']):.1%} ± {np.std(df_linear['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_linear['f1']):.1%} ± {np.std(df_linear['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>RBF</b></td>\n",
    "            <td>{len(df_rbf)}</td>\n",
    "            <td>{np.mean(df_rbf['accuracy']):.1%} ± {np.std(df_rbf['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_rbf['recall']):.1%} ± {np.std(df_rbf['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_rbf['precision']):.1%} ± {np.std(df_rbf['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_rbf['f1']):.1%} ± {np.std(df_rbf['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Sigmoid</b></td>\n",
    "            <td>{len(df_sigmoid)}</td>\n",
    "            <td>{np.mean(df_sigmoid['accuracy']):.1%} ± {np.std(df_sigmoid['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_sigmoid['recall']):.1%} ± {np.std(df_sigmoid['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_sigmoid['precision']):.1%} ± {np.std(df_sigmoid['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_sigmoid['f1']):.1%} ± {np.std(df_sigmoid['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Polynomial</b><br>(all)</td>\n",
    "            <td>{len(df_poly)}</td>\n",
    "            <td>{np.mean(df_poly['accuracy']):.1%} ± {np.std(df_poly['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_poly['recall']):.1%} ± {np.std(df_poly['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_poly['precision']):.1%} ± {np.std(df_poly['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_poly['f1']):.1%} ± {np.std(df_poly['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Polynomial</b><br>(degree = 2)</td>\n",
    "            <td>{len(df_poly_2)}</td>\n",
    "            <td>{np.mean(df_poly_2['accuracy']):.1%} ± {np.std(df_poly_2['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_2['recall']):.1%} ± {np.std(df_poly_2['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_2['precision']):.1%} ± {np.std(df_poly_2['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_2['f1']):.1%} ± {np.std(df_poly_2['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Polynomial</b><br>(degree = 3)</td>\n",
    "            <td>{len(df_poly_3)}</td>\n",
    "            <td>{np.mean(df_poly_3['accuracy']):.1%} ± {np.std(df_poly_3['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_3['recall']):.1%} ± {np.std(df_poly_3['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_3['precision']):.1%} ± {np.std(df_poly_3['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_3['f1']):.1%} ± {np.std(df_poly_3['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Polynomial</b><br>(degree = 4)</td>\n",
    "            <td>{len(df_poly_4)}</td>\n",
    "            <td>{np.mean(df_poly_4['accuracy']):.1%} ± {np.std(df_poly_4['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_4['recall']):.1%} ± {np.std(df_poly_4['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_4['precision']):.1%} ± {np.std(df_poly_4['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_poly_4['f1']):.1%} ± {np.std(df_poly_4['f1']):.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>All</b></td>\n",
    "            <td>{len(df_params)}</td>\n",
    "            <td>{np.mean(df_params['accuracy']):.1%} ± {np.std(df_params['accuracy']):.1%}</td>\n",
    "            <td>{np.mean(df_params['recall']):.1%} ± {np.std(df_params['recall']):.1%}</td>\n",
    "            <td>{np.mean(df_params['precision']):.1%} ± {np.std(df_params['precision']):.1%}</td>\n",
    "            <td>{np.mean(df_params['f1']):.1%} ± {np.std(df_params['f1']):.1%}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "</div>\"\"\"))\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <table class = 'table_1'>\n",
    "        <tr>\n",
    "            <th>Kernel</th>\n",
    "            <th>Max Accuracy</th>\n",
    "            <th>Min Accuracy</th>\n",
    "            <th>Max Recall</th>\n",
    "            <th>Min Recall</th>          \n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Linear</b></td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_linear['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_linear.loc[df_linear['accuracy'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_linear.loc[df_linear['accuracy'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_linear['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_linear.loc[df_linear['accuracy'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_linear.loc[df_linear['accuracy'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_linear['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_linear.loc[df_linear['recall'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_linear.loc[df_linear['recall'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_linear['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_linear.loc[df_linear['recall'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_linear.loc[df_linear['recall'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>RBF</b></td>\n",
    "            <td style = 'text-align: left !important; background-color: palegreen; border: 2px solid forestgreen'>- Value: <b>{np.max(df_rbf['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_rbf.loc[df_rbf['accuracy'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_rbf.loc[df_rbf['accuracy'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_rbf['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_rbf.loc[df_rbf['accuracy'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_rbf.loc[df_rbf['accuracy'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important; background-color: palegreen; border: 2px solid forestgreen'>- Value: <b>{np.max(df_rbf['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_rbf.loc[df_rbf['recall'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_rbf.loc[df_rbf['recall'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_rbf['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_rbf.loc[df_rbf['recall'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_rbf.loc[df_rbf['recall'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Sigmoid</b></td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_sigmoid['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_sigmoid.loc[df_sigmoid['accuracy'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_sigmoid.loc[df_sigmoid['accuracy'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important; background-color: lightsalmon; border: 2px solid firebrick'>- Value: <b>{np.min(df_sigmoid['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_sigmoid.loc[df_sigmoid['accuracy'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_sigmoid.loc[df_sigmoid['accuracy'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_sigmoid['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_sigmoid.loc[df_sigmoid['recall'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_sigmoid.loc[df_sigmoid['recall'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_sigmoid['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_sigmoid.loc[df_sigmoid['recall'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_sigmoid.loc[df_sigmoid['recall'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Polynomial</b><br>(degree = 3)</td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_poly_3['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_poly_3.loc[df_poly_3['accuracy'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_poly_3.loc[df_poly_3['accuracy'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_poly_3['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_poly_3.loc[df_poly_3['accuracy'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_poly_3.loc[df_poly_3['accuracy'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_poly_3['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_poly_3.loc[df_poly_3['recall'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_poly_3.loc[df_poly_3['recall'].idxmax(), 'gamma']}</code>\n",
    "            </td>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important; background-color: lightsalmon; border: 2px solid firebrick'>- Value: <b>{np.min(df_poly_3['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_poly_3.loc[df_poly_3['recall'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_poly_3.loc[df_poly_3['recall'].idxmin(), 'gamma']}</code>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>All</b></td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_params['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_params.loc[df_params['accuracy'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_params.loc[df_params['accuracy'].idxmax(), 'gamma']}</code>\n",
    "                <br>- <code>kernel = {df_params.loc[df_params['accuracy'].idxmax(), 'kernel']}</code>               \n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_params['accuracy']):.1%}</b>\n",
    "                <br>- <code>C = {df_params.loc[df_params['accuracy'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_params.loc[df_params['accuracy'].idxmin(), 'gamma']}</code>\n",
    "                <br>- <code>kernel = {df_params.loc[df_params['accuracy'].idxmin(), 'kernel']}</code>                \n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.max(df_params['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_params.loc[df_params['recall'].idxmax(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_params.loc[df_params['recall'].idxmax(), 'gamma']}</code>\n",
    "                <br>- <code>kernel = {df_params.loc[df_params['recall'].idxmax(), 'kernel']}</code>\n",
    "            </td>\n",
    "            </td>\n",
    "            <td style = 'text-align: left !important'>- Value: <b>{np.min(df_params['recall']):.1%}</b>\n",
    "                <br>- <code>C = {df_params.loc[df_params['recall'].idxmin(), 'C']}</code>\n",
    "                <br>- <code>gamma = {df_params.loc[df_params['recall'].idxmin(), 'gamma']}</code>\n",
    "                <br>- <code>kernel = {df_params.loc[df_params['recall'].idxmin(), 'kernel']}</code>\n",
    "            </td>\n",
    "        </tr>\n",
    "    <p>\n",
    "        <b>Linear</b> kernel displayed very limited variability for all metrics, suggesting parameter tuning had\n",
    "        little to no impact. Consequently, min and max values for both accuracy and recall were comparable. For both metrics, \n",
    "        <code>C = 1</code> and <code>gamma = \"scale\"</code> produced the best results.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>RBF</b> kernel led to the best values for both accuracy (<b>{np.max(df_rbf['accuracy']):.1%}</b>) and recall\n",
    "        (<b>{np.max(df_rbf['recall']):.1%}</b>). Interestingly, max value for accuracy was obtained with \n",
    "        <code>C = 10.0</code> and <code>gamma = \"scale\"</code> whereas best recall was obtained with\n",
    "        <code>C = 0.01</code> and <code>gamma = 10</code>, suggesting that finding balance between both <code>C</code>\n",
    "        and <code>gamma</code> is essential. Moderate variability was found with this kernel, and the lowest values for both\n",
    "        accuracy and recall were found with <code>C = 0.01</code> and <code>gamma = 0.01</code>.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>Polynomial</b> kernel (third degree) produced high variability, yet its max value for accuracy (<b>{np.max(df_poly_3['accuracy']):.1%}</b>)\n",
    "        was comparable to the max accuracy obtained with RBF. Max accuracy and max recall were both obtained with \n",
    "        <code>C = 0.01</code> and <code>gamma = 10</code>.\n",
    "    </p>\n",
    "\n",
    "</div>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fccead30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>degree</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>gamma_float</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>scale</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.582913</td>\n",
       "      <td>34.957230</td>\n",
       "      <td>0.727470</td>\n",
       "      <td>0.676955</td>\n",
       "      <td>0.752633</td>\n",
       "      <td>0.712790</td>\n",
       "      <td>0.691955</td>\n",
       "      <td>0.1_scale_rbf_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.348680</td>\n",
       "      <td>45.388657</td>\n",
       "      <td>0.686303</td>\n",
       "      <td>0.589275</td>\n",
       "      <td>0.730681</td>\n",
       "      <td>0.652396</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1_0.01_rbf_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.195986</td>\n",
       "      <td>38.511891</td>\n",
       "      <td>0.721160</td>\n",
       "      <td>0.645613</td>\n",
       "      <td>0.760088</td>\n",
       "      <td>0.698186</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1_0.1_rbf_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.757723</td>\n",
       "      <td>35.197943</td>\n",
       "      <td>0.727128</td>\n",
       "      <td>0.680501</td>\n",
       "      <td>0.750090</td>\n",
       "      <td>0.713603</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1_1_rbf_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>rbf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.601656</td>\n",
       "      <td>35.074196</td>\n",
       "      <td>0.721457</td>\n",
       "      <td>0.720393</td>\n",
       "      <td>0.721578</td>\n",
       "      <td>0.720981</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1_10_rbf_nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      C  gamma kernel  degree   fit_time  score_time  accuracy    recall  \\\n",
       "1   0.1  scale    rbf     NaN  39.582913   34.957230  0.727470  0.676955   \n",
       "51  0.1   0.01    rbf     NaN  47.348680   45.388657  0.686303  0.589275   \n",
       "55  0.1    0.1    rbf     NaN  42.195986   38.511891  0.721160  0.645613   \n",
       "59  0.1      1    rbf     NaN  39.757723   35.197943  0.727128  0.680501   \n",
       "63  0.1     10    rbf     NaN  41.601656   35.074196  0.721457  0.720393   \n",
       "\n",
       "    precision        f1 gamma_float          sample_id  \n",
       "1    0.752633  0.712790    0.691955  0.1_scale_rbf_nan  \n",
       "51   0.730681  0.652396        0.01   0.1_0.01_rbf_nan  \n",
       "55   0.760088  0.698186         0.1    0.1_0.1_rbf_nan  \n",
       "59   0.750090  0.713603           1      0.1_1_rbf_nan  \n",
       "63   0.721578  0.720981          10     0.1_10_rbf_nan  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rbf[df_rbf['C'] == 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bed8264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>gamma</th>\n",
       "      <th>kernel</th>\n",
       "      <th>degree</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>gamma_float</th>\n",
       "      <th>sample_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.01</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.008044</td>\n",
       "      <td>20.195658</td>\n",
       "      <td>0.476998</td>\n",
       "      <td>0.472367</td>\n",
       "      <td>0.476344</td>\n",
       "      <td>0.474339</td>\n",
       "      <td>0.691955</td>\n",
       "      <td>0.01_scale_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.10</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.764808</td>\n",
       "      <td>17.635636</td>\n",
       "      <td>0.466669</td>\n",
       "      <td>0.465068</td>\n",
       "      <td>0.466143</td>\n",
       "      <td>0.465589</td>\n",
       "      <td>0.691955</td>\n",
       "      <td>0.1_scale_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.00</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.455408</td>\n",
       "      <td>17.166292</td>\n",
       "      <td>0.465895</td>\n",
       "      <td>0.465872</td>\n",
       "      <td>0.465477</td>\n",
       "      <td>0.465656</td>\n",
       "      <td>0.691955</td>\n",
       "      <td>1.0_scale_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.00</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.913778</td>\n",
       "      <td>17.105072</td>\n",
       "      <td>0.465910</td>\n",
       "      <td>0.465842</td>\n",
       "      <td>0.465489</td>\n",
       "      <td>0.465647</td>\n",
       "      <td>0.691955</td>\n",
       "      <td>10.0_scale_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100.00</td>\n",
       "      <td>scale</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.895561</td>\n",
       "      <td>17.269096</td>\n",
       "      <td>0.465880</td>\n",
       "      <td>0.465872</td>\n",
       "      <td>0.465463</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0.691955</td>\n",
       "      <td>100.0_scale_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>0.01</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.720114</td>\n",
       "      <td>29.770131</td>\n",
       "      <td>0.500305</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000119</td>\n",
       "      <td>10</td>\n",
       "      <td>0.01_10_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.10</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.423001</td>\n",
       "      <td>30.964403</td>\n",
       "      <td>0.491167</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.230682</td>\n",
       "      <td>0.015434</td>\n",
       "      <td>10</td>\n",
       "      <td>0.1_10_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.00</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.734435</td>\n",
       "      <td>27.639077</td>\n",
       "      <td>0.408981</td>\n",
       "      <td>0.197736</td>\n",
       "      <td>0.341734</td>\n",
       "      <td>0.250496</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0_10_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>10.00</td>\n",
       "      <td>10</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.876516</td>\n",
       "      <td>19.503241</td>\n",
       "      <td>0.388739</td>\n",
       "      <td>0.382422</td>\n",
       "      <td>0.386882</td>\n",
       "      <td>0.384633</td>\n",
       "      <td>10</td>\n",
       "      <td>10.0_10_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.177115</td>\n",
       "      <td>18.809328</td>\n",
       "      <td>0.500439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01_0.01_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.038752</td>\n",
       "      <td>16.928890</td>\n",
       "      <td>0.657890</td>\n",
       "      <td>0.493192</td>\n",
       "      <td>0.734832</td>\n",
       "      <td>0.590216</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1_0.01_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.615941</td>\n",
       "      <td>14.328433</td>\n",
       "      <td>0.716605</td>\n",
       "      <td>0.632683</td>\n",
       "      <td>0.759838</td>\n",
       "      <td>0.690453</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.0_0.01_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.418411</td>\n",
       "      <td>12.715644</td>\n",
       "      <td>0.723496</td>\n",
       "      <td>0.634977</td>\n",
       "      <td>0.771131</td>\n",
       "      <td>0.696455</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0_0.01_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.710675</td>\n",
       "      <td>17.836728</td>\n",
       "      <td>0.655390</td>\n",
       "      <td>0.487204</td>\n",
       "      <td>0.733525</td>\n",
       "      <td>0.585493</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.01_0.1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.881985</td>\n",
       "      <td>15.041913</td>\n",
       "      <td>0.715861</td>\n",
       "      <td>0.631938</td>\n",
       "      <td>0.758941</td>\n",
       "      <td>0.689639</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1_0.1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.622714</td>\n",
       "      <td>10.598490</td>\n",
       "      <td>0.673131</td>\n",
       "      <td>0.629167</td>\n",
       "      <td>0.689523</td>\n",
       "      <td>0.657932</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0_0.1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>54.233410</td>\n",
       "      <td>7.706038</td>\n",
       "      <td>0.626933</td>\n",
       "      <td>0.624877</td>\n",
       "      <td>0.627062</td>\n",
       "      <td>0.625954</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0_0.1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.01</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.602404</td>\n",
       "      <td>20.416548</td>\n",
       "      <td>0.467636</td>\n",
       "      <td>0.465991</td>\n",
       "      <td>0.467091</td>\n",
       "      <td>0.466526</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01_1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.395068</td>\n",
       "      <td>18.554543</td>\n",
       "      <td>0.464124</td>\n",
       "      <td>0.463578</td>\n",
       "      <td>0.463643</td>\n",
       "      <td>0.463596</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1_1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.606448</td>\n",
       "      <td>18.319930</td>\n",
       "      <td>0.463930</td>\n",
       "      <td>0.463727</td>\n",
       "      <td>0.463475</td>\n",
       "      <td>0.463587</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0_1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>10.00</td>\n",
       "      <td>1</td>\n",
       "      <td>sigmoid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.163846</td>\n",
       "      <td>18.296715</td>\n",
       "      <td>0.463960</td>\n",
       "      <td>0.463697</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.463585</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0_1_sigmoid_nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C  gamma   kernel  degree   fit_time  score_time  accuracy    recall  \\\n",
       "10    0.01  scale  sigmoid     NaN  67.008044   20.195658  0.476998  0.472367   \n",
       "11    0.10  scale  sigmoid     NaN  58.764808   17.635636  0.466669  0.465068   \n",
       "12    1.00  scale  sigmoid     NaN  52.455408   17.166292  0.465895  0.465872   \n",
       "13   10.00  scale  sigmoid     NaN  51.913778   17.105072  0.465910  0.465842   \n",
       "14  100.00  scale  sigmoid     NaN  50.895561   17.269096  0.465880  0.465872   \n",
       "79    0.01     10  sigmoid     NaN  67.720114   29.770131  0.500305  0.000060   \n",
       "80    0.10     10  sigmoid     NaN  72.423001   30.964403  0.491167  0.007984   \n",
       "81    1.00     10  sigmoid     NaN  65.734435   27.639077  0.408981  0.197736   \n",
       "82   10.00     10  sigmoid     NaN  44.876516   19.503241  0.388739  0.382422   \n",
       "83    0.01   0.01  sigmoid     NaN  48.177115   18.809328  0.500439  0.000000   \n",
       "84    0.10   0.01  sigmoid     NaN  46.038752   16.928890  0.657890  0.493192   \n",
       "85    1.00   0.01  sigmoid     NaN  41.615941   14.328433  0.716605  0.632683   \n",
       "86   10.00   0.01  sigmoid     NaN  39.418411   12.715644  0.723496  0.634977   \n",
       "87    0.01    0.1  sigmoid     NaN  51.710675   17.836728  0.655390  0.487204   \n",
       "88    0.10    0.1  sigmoid     NaN  47.881985   15.041913  0.715861  0.631938   \n",
       "89    1.00    0.1  sigmoid     NaN  60.622714   10.598490  0.673131  0.629167   \n",
       "90   10.00    0.1  sigmoid     NaN  54.233410    7.706038  0.626933  0.624877   \n",
       "91    0.01      1  sigmoid     NaN  84.602404   20.416548  0.467636  0.465991   \n",
       "92    0.10      1  sigmoid     NaN  66.395068   18.554543  0.464124  0.463578   \n",
       "93    1.00      1  sigmoid     NaN  61.606448   18.319930  0.463930  0.463727   \n",
       "94   10.00      1  sigmoid     NaN  61.163846   18.296715  0.463960  0.463697   \n",
       "\n",
       "    precision        f1 gamma_float                sample_id  \n",
       "10   0.476344  0.474339    0.691955   0.01_scale_sigmoid_nan  \n",
       "11   0.466143  0.465589    0.691955    0.1_scale_sigmoid_nan  \n",
       "12   0.465477  0.465656    0.691955    1.0_scale_sigmoid_nan  \n",
       "13   0.465489  0.465647    0.691955   10.0_scale_sigmoid_nan  \n",
       "14   0.465463  0.465649    0.691955  100.0_scale_sigmoid_nan  \n",
       "79   0.150000  0.000119          10      0.01_10_sigmoid_nan  \n",
       "80   0.230682  0.015434          10       0.1_10_sigmoid_nan  \n",
       "81   0.341734  0.250496          10       1.0_10_sigmoid_nan  \n",
       "82   0.386882  0.384633          10      10.0_10_sigmoid_nan  \n",
       "83   0.000000  0.000000        0.01    0.01_0.01_sigmoid_nan  \n",
       "84   0.734832  0.590216        0.01     0.1_0.01_sigmoid_nan  \n",
       "85   0.759838  0.690453        0.01     1.0_0.01_sigmoid_nan  \n",
       "86   0.771131  0.696455        0.01    10.0_0.01_sigmoid_nan  \n",
       "87   0.733525  0.585493         0.1     0.01_0.1_sigmoid_nan  \n",
       "88   0.758941  0.689639         0.1      0.1_0.1_sigmoid_nan  \n",
       "89   0.689523  0.657932         0.1      1.0_0.1_sigmoid_nan  \n",
       "90   0.627062  0.625954         0.1     10.0_0.1_sigmoid_nan  \n",
       "91   0.467091  0.466526           1       0.01_1_sigmoid_nan  \n",
       "92   0.463643  0.463596           1        0.1_1_sigmoid_nan  \n",
       "93   0.463475  0.463587           1        1.0_1_sigmoid_nan  \n",
       "94   0.463500  0.463585           1       10.0_1_sigmoid_nan  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c1a01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78685b0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85521649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b41b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e51b487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89bc1f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c721093",
   "metadata": {},
   "source": [
    "<h4>Code for: Logistic Regression - Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8778273",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state = 41)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "lr_params = lr.get_params()\n",
    "\n",
    "lr_fi = pd.DataFrame({\n",
    "    'coef' : lr.coef_[0],\n",
    "    'var': X.columns.tolist()\n",
    "})\n",
    "\n",
    "lr_report = classification_report(y_test, lr_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5f280",
   "metadata": {},
   "source": [
    "<h4>Code for: Logistic Regression - Report</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daeafdc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Logistic Regression</h1>\n",
    "    <p class = 'intro'>\n",
    "        Logistic Regression is a widely used machine learning model for binary classification tasks. It models \n",
    "        the probability that a given input data point belongs to a particular class (in the present case, whether a patient \n",
    "        has cardiovascular disease or not). The output of Logistic Regression is a probability score between 0 and 1, \n",
    "        which is then thresholded to make a binary prediction.\n",
    "    </p>\n",
    "    <h4>Key Points</h4>\n",
    "    <p style='border-left: 3px solid silver; padding-left: 5px; font-size: 12.5px; margin-left: 15px'>\n",
    "        <strong>Linear Decision Boundary:</strong> Logistic Regression creates a linear decision boundary to separate the two classes. This decision boundary is a hyperplane in the feature space.\n",
    "        <br><strong>Probability Output:</strong> Instead of direct class prediction, Logistic Regression outputs probabilities. These probabilities represent the likelihood of the input belonging to the positive class.\n",
    "        <br><strong>Log-Odds Transformation:</strong> Logistic Regression uses the log-odds (logit) transformation to model the relationship between the input features and the log-odds of the event occurring.\n",
    "        <br><strong>Sigmoid Activation:</strong> The logistic function (sigmoid) is applied to the linear combination of input features. This function maps any real-valued number to the range [0, 1].\n",
    "        <br><strong>Maximum Likelihood Estimation:</strong> Logistic Regression estimates parameters using maximum likelihood, aiming to maximize the likelihood of the observed class labels given the input features.\n",
    "        <br><strong>Regularization:</strong> To prevent overfitting, regularization terms like L1 or L2 regularization can be added to the logistic regression cost function.\n",
    "    </p>\n",
    "    <h2>Default Model</h2>\n",
    "    <h3>Parameters</h3>\n",
    "    <table class = 'table_1' style = 'width: 100% !important'>\n",
    "        <tr>\n",
    "            <th style = 'width: 15%'>Parameter</th>\n",
    "            <th style = 'width: 5%'>Value</th>\n",
    "            <th>Definition</th>\n",
    "            <th>Effect</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Inverse of Regularisation Strength</b><br><code>C</code></td>\n",
    "            <td>{lr_params['C']}</td>\n",
    "            <td style = 'text-align: justify !important'>The regularisation parameter <code>C</code> is the inverse of the regularisation strength. It controls the trade-off between fitting the training data well and preventing overfitting.</td>\n",
    "            <td style = 'text-align: justify !important'>Smaller values of <code>C</code> lead to stronger regularisation, encouraging the model to generalize better but possibly underfit. Larger values of C reduce regularisation, allowing the model to fit the training data more closely but risking overfitting.\n",
    "                <br><br><i>Note: <code>C=0.1</code> indicates strong regularisation, while <code>C=10</code> implies weaker regularisation.</i></td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Regularisation Type</b><br><code>penalty</code></td>\n",
    "            <td>{lr_params['penalty']}</td>\n",
    "            <td style = 'text-align: justify !important'>The penalty parameter determines the type of regularisation applied to the model. It can be either <code>\"l1\"</code> for L1 regularisation (lasso) or <code>\"l2\"</code> for L2 regularisation (ridge).</td>\n",
    "            <td style = 'text-align: justify !important'>L1 regularisation tends to produce sparse coefficients (some coefficients become exactly zero), leading to feature selection. L2 regularisation penalises large coefficients, encouraging smaller but non-zero coefficients for all features.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Maximum Number of Iterations</b><br><code>max_iter</code></td>\n",
    "            <td>{lr_params['max_iter']}</td>\n",
    "            <td style = 'text-align: justify !important'>The maximum number of iterations is the maximum number of iterations for the solver to converge (reach a solution).</td>\n",
    "            <td style = 'text-align: justify !important'>If the solver doesn't converge within the specified number of iterations, it might indicate that the model hasn't found a solution. Increasing the number of iterations may help in such cases.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Optimisation Algorithm</b><br><code>solver</code></td>\n",
    "            <td>{lr_params['solver']}</td>\n",
    "            <td style = 'text-align: justify !important'>The solver parameter specifies the optimisation algorithm to use in fitting the logistic regression model.</td>\n",
    "            <td style = 'text-align: justify !important'>Different solvers have different properties and may perform better or worse depending on the dataset size and characteristics. For example, <code>liblinear</code> is suitable for small datasets, while <code>saga</code> is useful for large datasets with a large number of features.\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Classification Results</h3>\n",
    "    {matrix_display(pd.crosstab(y_test, lr_pred))}\n",
    "</div>\n",
    "\"\"\"))\n",
    "classification_plot(pd.crosstab(y_test, lr_pred))        \n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Metrics</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "perf_barplot(lr_report)\n",
    "report_table(lr_report)\n",
    "\n",
    "display(HTML(\n",
    "f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Feature Importance</h3>\n",
    "</div>\n",
    "\"\"\"\n",
    "))\n",
    "lr_fi = lr_fi.sort_values(by='coef', ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "sns.barplot(\n",
    "    y = 'var', \n",
    "    x = 'coef',\n",
    "    data = lr_fi,\n",
    "    edgecolor = 'black', \n",
    "    ax = ax\n",
    ")\n",
    "\n",
    "ax.axvline(x = 1, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 2, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 9, linestyle = 'dotted', color = 'black')\n",
    "\n",
    "ax.set_ylabel('Features', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_xlim(-1,10)\n",
    "ax.set_xticks(ticks = range(-1,11,1), labels = range(-1,11,1))\n",
    "ax.set_xlabel('Feature Coefficient', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_title(None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50103e6",
   "metadata": {},
   "source": [
    "<h4>Code for: Random Forest - Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb425916",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=41)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "rf_params = rf.get_params()\n",
    "\n",
    "rf_fi = pd.DataFrame({\n",
    "    'importance': rf.feature_importances_,\n",
    "    'var': X.columns.tolist()\n",
    "})\n",
    "\n",
    "rf_report = classification_report(y_test, rf_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7df8ecf",
   "metadata": {},
   "source": [
    "<h4>Code for: Random Forest - Report</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d4b49f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Random Forest</h1>\n",
    "    <p class = 'intro'>\n",
    "        The Random Forest algorithm is an ensemble learning technique widely used for both classification and regression \n",
    "        tasks. It is an ensemble of decision trees, where multiple decision trees work together to make predictions.\n",
    "        Random Forest builds multiple decision trees independently and combines their predictions through a voting or averaging mechanism.\n",
    "    </p>\n",
    "    <h4>Key Points</h4>\n",
    "    <p style='border-left: 3px solid silver; padding-left: 5px; font-size: 12.5px; margin-left: 15px'>\n",
    "        <strong>Bootstrap Sampling:</strong> It starts by creating random subsets of the training data through bootstrap sampling (randomly selecting data points with replacement).\n",
    "        <br><strong>Feature Randomization:</strong> At each node of each tree, only a random subset of features is considered for splitting. This introduces diversity among the trees.\n",
    "        <br><strong>Aggregation:</strong> For classification tasks, Random Forest typically uses a majority voting mechanism to make predictions. For regression tasks, it averages the predictions of individual trees.\n",
    "    </p>\n",
    "    <h2>Default Model</h2>\n",
    "    <h3>Parameters</h3>\n",
    "    <table class = 'table_1' style = 'width: 100% !important'>\n",
    "        <tr>\n",
    "            <th style = 'width: 20%'>Parameter</th>\n",
    "            <th style = 'width: 5%'>Value</th>\n",
    "            <th>Definition</th>\n",
    "            <th>Effect</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Number of Trees</b><br><code>n_estimators</code></td>\n",
    "            <td>{rf_params['n_estimators']}</td>\n",
    "            <td style='text-align: justify !important'>The number of decision trees that will be built during training.</td>\n",
    "            <td style='text-align: justify !important'>Increasing the number of trees generally improves performance but comes at the cost of increased computation time. A common starting point is around 100 trees.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Maximum Depth of Trees</b><br><code>max_depth</code></td>\n",
    "            <td>{rf_params['max_depth']}</td>\n",
    "            <td style='text-align: justify !important'>The maximum number of nodes or levels a decision tree has from the root node to the deepest leaf node.</td>\n",
    "            <td style='text-align: justify !important'>Controlling the depth helps prevent overfitting. Smaller values of <code>max_depth</code> lead to simpler trees and better generalization.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Minimum Samples to Split</b><br><code>min_samples_split</code></td>\n",
    "            <td>{rf_params['min_samples_split']}</td>\n",
    "            <td style='text-align: justify !important'>The minimum number of samples required to split an internal node during the construction of a decision tree.</td>\n",
    "            <td style='text-align: justify !important'>Higher values of <code>min_samples_split</code> can result in more robust models by preventing the creation of small leaf nodes.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Minimum Samples per Leaf</b><br><code>min_samples_leaf</code></td>\n",
    "            <td>{rf_params['min_samples_leaf']}</td>\n",
    "            <td style='text-align: justify !important'>The minimum number of samples required to be at a leaf node. This parameter influences the size of the leaves in the decision trees.</td>\n",
    "            <td style='text-align: justify !important'>Controlling leaf size helps prevent overfitting. Larger values of <code>min_samples_leaf</code> result in simpler trees.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Maximum Features</b><br><code>max_features</code></td>\n",
    "            <td>{rf_params['max_features']}</td>\n",
    "            <td style='text-align: justify !important'>The number of features to consider when looking for the best split in each decision tree.</td>\n",
    "            <td style='text-align: justify !important'>This parameter controls the randomness of each tree. Using fewer features can lead to decorrelated trees, improving ensemble performance.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Classification Results</h3>\n",
    "    {matrix_display(pd.crosstab(y_test, rf_pred))}\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "classification_plot(pd.crosstab(y_test, rf_pred))        \n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Metrics</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "perf_barplot(rf_report)\n",
    "report_table(rf_report)\n",
    "\n",
    "display(HTML(\n",
    "f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Feature Importance</h3>\n",
    "</div>\n",
    "\"\"\"\n",
    "))\n",
    "        \n",
    "rf_fi = rf_fi.sort_values(by = 'importance', ascending = False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "sns.barplot(\n",
    "    y = 'var', \n",
    "    x = 'importance',\n",
    "    data = rf_fi,\n",
    "    edgecolor = 'black', \n",
    "    ax = ax\n",
    ")\n",
    "\n",
    "ax.axvline(x = 0.1, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 0.2, linestyle = 'dotted', color = 'black')\n",
    "#ax.axvline(x = 0.3, linestyle = 'dotted', color = 'black')\n",
    "\n",
    "ax.set_ylabel('Features', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_xlim(0,0.3)\n",
    "ax.set_xticks(ticks = [x/10 for x in range(0,4,1)], labels = [x/10 for x in range(0,4,1)])\n",
    "ax.set_xlabel('Feature Coefficient', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_title(None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7225ab7",
   "metadata": {},
   "source": [
    "<h4>Code for: Support Vector Machines - Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde9517",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(random_state = 46)\n",
    "svm.fit(X_train, y_train)\n",
    "svm_pred = svm.predict(X_test)\n",
    "svm_params = svm.get_params()\n",
    "svm_report = classification_report(y_test, svm_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e1e9a3",
   "metadata": {},
   "source": [
    "<h4>Code for: Support Vector Machines - Report</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f78505",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Support Vector Machine</h1>\n",
    "    <p class = 'intro'>\n",
    "        The Support Vector Machine is a powerful supervised machine learning algorithm used for classification and \n",
    "        regression tasks. It is particularly effective in situations where data may not be linearly separable, \n",
    "        making it a versatile choice for various applications. SVM works by finding the hyperplane that maximizes \n",
    "        the margin between different classes of data points. It aims to create a decision boundary that effectively \n",
    "        separates data points into distinct categories while maximizing the margin between the classes.\n",
    "    </p>\n",
    "    <h2>Default Model</h2>\n",
    "    <h3>Parameters</h3>\n",
    "    <table class = 'table_1' style = 'width: 100% !important'>\n",
    "        <tr>\n",
    "            <th style = 'width: 20%'>Parameter</th>\n",
    "            <th style = 'width: 5%'>Value</th>\n",
    "            <th>Definition</th>\n",
    "            <th>Effect</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Regularisation</b><br><code>C</code></td>\n",
    "            <td>{svm_params['C']}</td>\n",
    "            <td style='text-align: justify !important'>C controls the trade-off between achieving a low training error and a low testing error.</td>\n",
    "            <td style='text-align: justify !important'>A small C encourages a larger-margin hyperplane, possibly allowing some training points to be misclassified. A large C penalizes misclassifications more heavily, leading to a smaller-margin hyperplane. Higher values of C may result in a more complex decision boundary, potentially leading to overfitting.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Kernel</b><br><code>kernel</code></td>\n",
    "            <td>{svm_params['kernel']}</td>\n",
    "            <td style='text-align: justify !important'>Specifies the type of hyperplane used for separation. Common choices include linear, polynomial, radial basis function (RBF), and sigmoid kernels.</td>\n",
    "            <td style='text-align: justify !important'>The choice of the kernel determines the shape of the decision boundary. Experimenting with different kernels is essential to find the one that works best for your data.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Gamma</b><br><code>gamma</code></td>\n",
    "            <td>{svm_params['gamma']}</td>\n",
    "            <td style='text-align: justify !important'>Parameter for non-linear hyperplanes. It defines how far the influence of a single training example reaches. Low values mean a far reach, and high values mean a limited reach.</td>\n",
    "            <td style='text-align: justify !important'>Smaller values of gamma lead to a more generalized solution, while larger values make the model more sensitive to the training data (potentially leading to overfitting).</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Decision Function Shape</b><br><code>decision_function_shape</code></td>\n",
    "            <td>{svm_params['decision_function_shape']}</td>\n",
    "            <td style='text-align: justify !important'>determines whether to use a one-vs-one (<code>'ovo'</code>) or one-vs-the-rest (<code>'ovr'</code>) strategy for multi-class classification.</td>\n",
    "            <td style='text-align: justify !important'>The choice of strategy can affect the computational efficiency and performance, especially in the case of a large number of classes.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Classification Results</h3>\n",
    "    {matrix_display(pd.crosstab(y_test, svm_pred))}\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "classification_plot(pd.crosstab(y_test, svm_pred))        \n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Metrics</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "perf_barplot(svm_report)\n",
    "report_table(svm_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3525c4",
   "metadata": {},
   "source": [
    "<h4>Code for: K-Nearest Neighbors - Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4006ca86",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(X_test)\n",
    "knn_params = knn.get_params()\n",
    "knn_report = classification_report(y_test, knn_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c441924",
   "metadata": {},
   "source": [
    "<h4>Code for: K-Nearest Neighbors - Report</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7cdf75",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>K-Nearest Neighbors</h1>\n",
    "    <p class = 'intro'>\n",
    "        The k-Nearest Neighbors algorithm is a versatile supervised machine learning approach commonly used for \n",
    "        classification and regression tasks. It stands out for its simplicity and effectiveness, making it applicable \n",
    "        across various domains. KNN operates by identifying the k-nearest data points to a given test point and making \n",
    "        predictions based on the majority class (for classification) or average (for regression) of those neighbors. \n",
    "        It doesn't assume a specific underlying functional form and can adapt to different data distributions.\n",
    "    </p>\n",
    "    <h2>Default Model</h2>\n",
    "    <h3>Parameters</h3>\n",
    "    <table class = 'table_1' style = 'width: 100% !important'>\n",
    "        <tr>\n",
    "            <th style = 'width: 15%'>Parameter</th>\n",
    "            <th style = 'width: 10%'>Value</th>\n",
    "            <th>Definition</th>\n",
    "            <th>Effect</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Neighbors</b><br><code>n_neighbors</code></td>\n",
    "            <td>{knn_params['n_neighbors']}</td>\n",
    "            <td style='text-align: justify !important'>The number of neighbors to consider when making predictions.</td>\n",
    "            <td style='text-align: justify !important'>Smaller values make the model more sensitive to noise in the data, potentially leading to overfitting. Larger values provide a smoother decision boundary but may lead to underfitting.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Weights</b><br><code>weights</code></td>\n",
    "            <td>{knn_params['weights']}</td>\n",
    "            <td style='text-align: justify !important'>weight given to each neighbor when making predictions. Options include 'uniform' (all neighbors have equal weight) and 'distance' (closer neighbors have more influence).</td>\n",
    "            <td style='text-align: justify !important'>Choosing 'distance' may be useful when you expect nearer neighbors to have a greater impact on the prediction.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Algorithm</b><br><code>algorithm</code></td>\n",
    "            <td>{knn_params['algorithm']}</td>\n",
    "            <td style='text-align: justify !important'>Specifies the algorithm used to compute the nearest neighbors. Options include 'auto', 'ball_tree', 'kd_tree', and 'brute'. The 'auto' option selects the most appropriate algorithm based on the values passed to fit.</td>\n",
    "            <td style='text-align: justify !important'>Different algorithms may have different performance characteristics based on the dataset size and dimensionality.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Power Parameter</b><br><code>p</code></td>\n",
    "            <td>{knn_params['p']}</td>\n",
    "            <td style='text-align: justify !important'>The power parameter for the Minkowski distance metric.</td>\n",
    "            <td style='text-align: justify !important'>When <code>p</code> is set to 1, it is equivalent to using the Manhattan distance (L1 norm). When <code>p</code> is set to 2, it is equivalent to using the Euclidean distance (L2 norm). he choice of distance metric influences how distances are calculated and can affect the model's sensitivity to different features.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Leaf Size</b><br><code>leaf_size</code></td>\n",
    "            <td>{knn_params['leaf_size']}</td>\n",
    "            <td style='text-align: justify !important'>The number of points at which the algorithm switches to brute-force search. It influences the trade-off between the time complexity of the search and the space complexity of the data structure.</td>\n",
    "            <td style='text-align: justify !important'>Smaller values may lead to faster queries but require more memory. Larger values reduce memory requirements but may increase query time.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Classification Results</h3>\n",
    "    {matrix_display(pd.crosstab(y_test, knn_pred))}\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "classification_plot(pd.crosstab(y_test, knn_pred))        \n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Metrics</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "perf_barplot(knn_report)\n",
    "report_table(knn_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10f46d",
   "metadata": {},
   "source": [
    "<h4>Code for: AdaBoost - Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "adb = AdaBoostClassifier()\n",
    "adb.fit(X_train, y_train)\n",
    "adb_pred = adb.predict(X_test)\n",
    "\n",
    "adb_params = adb.get_params()\n",
    "\n",
    "adb_report = classification_report(y_test, adb_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298e3003",
   "metadata": {},
   "source": [
    "<h4>Code for: AdaBoost - Report</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a6669",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>AdaBoost</h1>\n",
    "    <p class = 'intro'>\n",
    "        AdaBoost, short for Adaptive Boosting, is a popular ensemble learning technique used for classification and \n",
    "        regression tasks. It is designed to improve the accuracy of weak learners (classifiers) by combining their \n",
    "        predictions into a strong ensemble model. AdaBoost works by sequentially training multiple weak classifiers on \n",
    "        the dataset. Each weak classifier focuses on the data points that were misclassified by the previous ones. \n",
    "        The algorithm assigns more weight to misclassified data points, allowing subsequent classifiers to pay more \n",
    "        attention to them. The final prediction is a weighted combination of the weak classifiers.\n",
    "    </p>\n",
    "    <h4>Key Points</h4>\n",
    "    <p style = 'border-left: 3px solid silver; padding-left: 5px; font-size : 12.5px; margin-left: 15px'>\n",
    "        <strong>Sequential Learning:</strong> Weak learners are trained sequentially, and their performance guides subsequent iterations.\n",
    "        <br><strong>Weighted Data:</strong> AdaBoost assigns different weights to data points, emphasizing the importance of misclassified samples.\n",
    "        <br><strong>Weighted Voting:</strong> The final prediction is a weighted vote of all weak classifiers, with more accurate classifiers having a greater influence.\n",
    "    </p>\n",
    "    <h2>Default Model</h2>\n",
    "    <h3>Parameters</h3>\n",
    "    <table class = 'table_1' style = 'width: 100% !important'>\n",
    "        <tr>\n",
    "            <th style = 'width: 15%'>Parameter</th>\n",
    "            <th style = 'width: 10%'>Value</th>\n",
    "            <th>Definition</th>\n",
    "            <th>Effect</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Weak Learners</b><br><code>n_estimators</code></td>\n",
    "            <td>{adb_params['n_estimators']}</td>\n",
    "            <td style='text-align: justify !important'>The number of weak learners (e.g., decision trees) to be combined in the ensemble. </td>\n",
    "            <td style='text-align: justify !important'>Increasing the number of estimators may improve performance up to a certain point but could lead to overfitting if set too high. There's also a trade-off between computational cost and gains in accuracy.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Learning Rate</b><br><code>learning_rate</code></td>\n",
    "            <td>{adb_params['learning_rate']}</td>\n",
    "            <td style='text-align: justify !important'>The contribution of each weak learner.</td>\n",
    "            <td style='text-align: justify !important'>Smaller values may improve generalization but require a higher number of estimators.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Base Estimator</b><br><code>base_estimator</code></td>\n",
    "            <td>{adb_params['base_estimator']}</td>\n",
    "            <td style='text-align: justify !important'>The weak learner used as the base model. If set to None, a decision tree with a depth of 1 <code>(DecisionTreeClassifier(max_depth=1))</code> is used by default.</td>\n",
    "            <td style='text-align: justify !important'>The choice of the base estimator influences the flexibility of the ensemble. A shallow tree is often used to prevent overfitting.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Algorithm</b><br><code>algorithm</code></td>\n",
    "            <td>{adb_params['algorithm']}</td>\n",
    "            <td style='text-align: justify !important'>The boosting algorithm to use. <code>'SAMME'</code> (Stagewise Additive Modeling using a Multiclass Exponential loss function) or <code>'SAMME.R'</code> (Real). <code>'SAMME.R'</code> is recommended for better performance.</td>\n",
    "            <td style='text-align: justify !important'><code>'SAMME.R'</code> often converges faster and provides better accuracy compared to <code>'SAMME'</code>, especially when weak learners can assign class probabilities.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Classification Results</h3>\n",
    "    {matrix_display(pd.crosstab(y_test, adb_pred))}\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "classification_plot(pd.crosstab(y_test, adb_pred))        \n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Metrics</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "perf_barplot(adb_report)\n",
    "report_table(adb_report)\n",
    "\n",
    "adb_fi = pd.DataFrame({'var': X.columns, 'importance' : adb.feature_importances_})\n",
    "\n",
    "adb_fi = adb_fi.sort_values(by = 'importance', ascending = False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "sns.barplot(\n",
    "    y = 'var', \n",
    "    x = 'importance',\n",
    "    data = adb_fi,\n",
    "    edgecolor = 'black', \n",
    "    ax = ax\n",
    ")\n",
    "\n",
    "ax.axvline(x = 0.1, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 0.2, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 0.3, linestyle = 'dotted', color = 'black')\n",
    "\n",
    "ax.set_ylabel('Features', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_xlim(0,0.4)\n",
    "ax.set_xticks(ticks = [x/10 for x in range(0,5,1)], labels = [x/10 for x in range(0,5,1)])\n",
    "ax.set_xlabel('Feature Coefficient', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_title(None)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d1840",
   "metadata": {},
   "source": [
    "<h4>Code for: ?</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d0377",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ml = pd.DataFrame()\n",
    "reports = lr_report, rf_report, svm_report, knn_report, adb_report\n",
    "names = [\"lr_\", \"rf_\", \"svm_\", \"knn_\", \"adb_\"]\n",
    "classification = ['0.0', '1.0']\n",
    "\n",
    "for report, name in zip(reports, names):    \n",
    "    for i in ['0.0', '1.0']:\n",
    "        report[i]['accuracy'] = report['accuracy']\n",
    "        report[i]['class'] = i[0]\n",
    "        name_index = name + i[0]\n",
    "        scores_ml = pd.concat([scores_ml, pd.DataFrame(report[i], index = [name_index])])\n",
    "\n",
    "scores_ml = scores_ml.reset_index().rename(columns = {'index' : 'model'}).drop(['support'], axis = 1)\n",
    "\n",
    "labels_ml = pd.DataFrame()\n",
    "preds = [lr_pred, rf_pred, svm_pred, knn_pred, adb_pred]\n",
    "full_names = ['Logistic Regression', 'Random Forest', 'SVM', 'K-Nearest Neighbors', 'AdaBoost']\n",
    "\n",
    "\n",
    "for pred, name, report, full_name in zip(preds, names, reports, full_names):\n",
    "    name_index = name[:-1]\n",
    "    recap = pd.DataFrame({\n",
    "            'TN': pd.crosstab(y_test, pred, normalize='index').iloc[0, 0],\n",
    "            'FP': pd.crosstab(y_test, pred, normalize='index').iloc[0, 1],\n",
    "            'FN': pd.crosstab(y_test, pred, normalize='index').iloc[1, 0],\n",
    "            'TP': pd.crosstab(y_test, pred, normalize='index').iloc[1, 1],\n",
    "            'accuracy': report['accuracy'],\n",
    "            'full_name': full_name\n",
    "        }, index=[name_index])\n",
    "    labels_ml = pd.concat([labels_ml, recap])\n",
    "    \n",
    "\n",
    "labels_ml = labels_ml.reset_index().rename(columns = {'index' : 'model'})\n",
    "\n",
    "\n",
    "scores_ml\n",
    "labels_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a3bfc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Synthesis</h1>\n",
    "    <p class = 'intro'>\n",
    "        Now that all models have been trained and tested, with their performances recorded, we may select the model that \n",
    "        feels the most suitable for this classification project.\n",
    "    </p>\n",
    "    <h4>Reminder: Metrics Signification</h4>\n",
    "    <p style='border-left: 3px solid silver; padding-left: 5px; font-size: 12.5px; margin-left: 15px'>\n",
    "        <strong>Accuracy:</strong> Measures the overall correctness of the model by calculating the ratio of correctly predicted instances to the total instances. It provides a general assessment of model performance.\n",
    "        <br><strong>Precision:</strong> Indicates the accuracy of positive predictions. It is the ratio of correctly predicted positive observations to the total predicted positives. Precision is relevant when the cost of false positives is high.\n",
    "        <br><strong>Recall:</strong> Measures the ability of the model to capture all the relevant instances, specifically the ratio of correctly predicted positive observations to the total actual positives. Recall is important when the cost of false negatives is high.\n",
    "        <br><strong>F1 Score:</strong> Combines precision and recall into a single metric, providing a balance between the two. It is the harmonic mean of precision and recall, offering a comprehensive evaluation of the model's performance.\n",
    "    </p>\n",
    "    <h3>Correct Predictions</h3>\n",
    "    \n",
    "\n",
    "</div>\n",
    "\"\"\"))\n",
    "head = f\"\"\"\n",
    "    <table class = 'table_1'>\n",
    "        <tr>\n",
    "            <th style = 'border-right: 1px solid black; width: 20%'>Model</th>\n",
    "            <th style = 'border-right: 1px solid black'>Class 0 - <i>Absence of CV Disease</i></th>\n",
    "            <th> Class 1 - <i>Presence of CV Disease</i></th>\n",
    "            <th style = 'border-left: 1px solid black'>Accuracy</th>\n",
    "        </tr>\n",
    "\"\"\"\n",
    "body = f\"\"\"\"\"\"\n",
    "\n",
    "for key, value in {'Logistic Regression' : 'lr', 'Random Forest' : 'rf', 'Support Vector Machines': 'svm', 'K-Nearest Neighbors': 'knn', 'AdaBoost': 'adb'}.items():\n",
    "        body += f\"\"\"\n",
    "            <tr>\n",
    "                <td style = 'text-align: right !important'><b>{key}</b></td>\"\"\"\n",
    "        \n",
    "        for label in ['TN', 'TP', 'accuracy']:\n",
    "            val = labels_ml[labels_ml['model'] == value][label].values[0]\n",
    "            if val == np.max(labels_ml[label]):\n",
    "                body += f\"\"\"<td style = 'color: forestgreen; font-weight: bold'>{val:.1%}</td>\"\"\"\n",
    "            \n",
    "            elif val == np.min(labels_ml[label]):\n",
    "                    body += f\"\"\"<td style = 'color: firebrick; font-weight: bold'>{val:.1%}</td>\"\"\"\n",
    "                    \n",
    "            else:\n",
    "                body += f\"\"\"<td>{val:.1%}</td>\"\"\"\n",
    "        body += f\"\"\"</tr>\"\"\"\n",
    "             \n",
    "tail = f\"\"\"\n",
    "    </table>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    {head+body+tail}\n",
    "    <h3>Performances</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "# Score df for class 0\n",
    "scores_0 = scores_ml[scores_ml['class'] == \"0\"].drop('class', axis = 1)\n",
    "scores_0['model'] = scores_0['model'].apply(lambda x: x[:-2])\n",
    "\n",
    "# Score df for class 1\n",
    "scores_1 = scores_ml[scores_ml['class'] == \"1\"].drop('class', axis = 1)\n",
    "scores_1['model'] = scores_1['model'].apply(lambda x: x[:-2])\n",
    "\n",
    "# scores_0_melt = pd.melt(scores_ml[scores_ml['class'] == \"0\"].drop(['class', 'accuracy'], axis = 1), id_vars = ['model'], var_name = ['metric'], value_name = 'score')\n",
    "# scores_0_melt[\"model\"] = scores_0[\"model\"].apply(lambda row: row[:-2])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (15,10))\n",
    "spec = gridspec.GridSpec(nrows = 2, ncols = 3)\n",
    "\n",
    "x_ticks_models = {\n",
    "    'lr': 'Logistic\\nRegression',\n",
    "    'rf' : 'Random\\nForest',\n",
    "    'svm': 'SVM',\n",
    "    'knn': 'K-Nearest\\nNeighbors',\n",
    "    'adb': 'AdaBoost'\n",
    "}\n",
    "\n",
    "palette_models = {\n",
    "    'lr': 'skyblue',\n",
    "    'rf' : 'lawngreen',\n",
    "    'svm': 'yellow',\n",
    "    'knn': 'sandybrown',\n",
    "    'adb': 'pink'\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "for df in [scores_0, scores_1]:\n",
    "    for param, param_spec in {\"precision\": spec[i,0], \"recall\": spec[i,1], \"f1-score\": spec[i,2]}.items():\n",
    "        ax = fig.add_subplot(param_spec)\n",
    "\n",
    "        sns.barplot(\n",
    "            y = param, \n",
    "            x = \"model\", \n",
    "            data = df.sort_values(by = param, ascending = False), \n",
    "            edgecolor = 'black', \n",
    "            ax = ax, \n",
    "            palette= palette_models\n",
    "        )\n",
    "\n",
    "        ax.set_ylim(0,1)\n",
    "        ax.set_yticks(ticks = [x/10 for x in range(0,11,1)], labels = range(0,110,10))\n",
    "        if param_spec == spec[0,0]:\n",
    "            ax.set_ylabel('Value for Class 0 (%)', fontdict = fontdict_labels)\n",
    "        elif param_spec == spec[1,0]:\n",
    "            ax.set_ylabel('Value for Class 1 (%)', fontdict = fontdict_labels)\n",
    "        else:\n",
    "            ax.set_ylabel(None)\n",
    "\n",
    "        ax.set_xticklabels(x_ticks_models[label.get_text()] for label in ax.get_xticklabels())\n",
    "        ax.set_xlabel(None)\n",
    "        \n",
    "        if df.equals(scores_0):\n",
    "            ax.set_title(param.capitalize(), fontdict = fontdict_title)\n",
    "        else:\n",
    "            ax.set_title(None)\n",
    "        \n",
    "        # Setting the % value on top of each bar, the first bar in bold.\n",
    "        first_bar = 0    \n",
    "        for container in ax.containers:\n",
    "            for bar in container.patches:\n",
    "                x_bar = bar.get_x()\n",
    "                y_bar = bar.get_height()\n",
    "                if first_bar == 0:\n",
    "                    ax.text(x_bar, y_bar+0.03, f\"{y_bar:.1%}\", size = 12, fontweight = 'bold')\n",
    "                else:\n",
    "                    ax.text(x_bar, y_bar+0.03, f\"{y_bar:.1%}\", size = 12)\n",
    "                first_bar += 1\n",
    "                             \n",
    "    i += 1\n",
    "\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Analysis</h3>\n",
    "    <p>\n",
    "        For this project, we tested five of the commonly machine learning models used for classification\n",
    "        tasks, namely <b>Logistic Regression</b>, <b>Random Forest</b>, <b>Support Vector Machines</b>, \n",
    "        <b>K-Nearest Neighbors</b> and <b>AdaBoost</b>.\n",
    "        These models were trained on the same dataset ({X_train.shape[0]} rows and {X_train.shape[1]} columns) and their\n",
    "        performances were recorded. \n",
    "        Overall, all models presented comparable performances, with accuracies ranging from \n",
    "        <b>{np.min(labels_ml['accuracy']):.1%}</b> for \n",
    "        <b>{labels_ml.loc[labels_ml['accuracy'].idxmin(), 'full_name']}</b>\n",
    "        to <b>{np.max(labels_ml['accuracy']):.1%}</b> for \n",
    "        <b>{labels_ml.loc[labels_ml['accuracy'].idxmax(), 'full_name']}</b>\n",
    "    </p>\n",
    "    <p>\n",
    "        Regarding <b>Class 0</b>, <i>ie</i> absence of cardiovascular disease, \n",
    "        <b>{labels_ml.loc[labels_ml['TN'].idxmax(), \"full_name\"]}</b> was the best model, with \n",
    "        <b>{np.max(labels_ml['TN']):.1%}</b> of correct classifications. <b>Logistic Regression</b> and <b>SVM</b>\n",
    "        also performed well, with {labels_ml[labels_ml['model'] == 'lr']['TN'].values[0]:.1%} and \n",
    "        {labels_ml[labels_ml['model'] == 'svm']['TN'].values[0]:.1%} of correct classifications, respectively. Conversely,\n",
    "        <b>Random Forest</b> and <b>KNN</b> achieved only <b>{labels_ml[labels_ml['model'] == 'rf']['TN'].values[0]:.1%}</b>\n",
    "        and <b>{labels_ml[labels_ml['model'] == 'knn']['TN'].values[0]:.1%}</b> of correct classification, respectively.\n",
    "    </p>\n",
    "    <p>\n",
    "        <b>{labels_ml.loc[labels_ml['TP'].idxmax(), 'full_name']}</b> however reached the highest rate\n",
    "        (<b>{labels_ml.loc[labels_ml['TP'].idxmax(), 'TP']:.1%}</b>) of correct classifications for <b>Class 1</b>\n",
    "        (<i>ie</i> presence of Cardiovascular disease). \n",
    "        <b>KNN</b> ({labels_ml.loc[labels_ml['model'] == 'knn', 'TP'].values[0]:.1%}),\n",
    "        <b>SVM</b> ({labels_ml.loc[labels_ml['model'] == 'svm', 'TP'].values[0]:.1%}) and\n",
    "        <b>Logistic Regression</b> ({labels_ml.loc[labels_ml['model'] == 'lr', 'TP'].values[0]:.1%}) had comparable results,\n",
    "        while <b>AdaBoost</b> provided only <b>{labels_ml.loc[labels_ml['TP'].idxmin(), 'TP']:.1%}</b> of correct classifications.\n",
    "    </p>\n",
    "    <p>\n",
    "        All things considered, Support Vector Machines will be selected for this project, as it presents the best accuracy\n",
    "        (<b>{labels_ml.loc[labels_ml['accuracy'].idxmax(), 'accuracy']:.1%}</b>), a good rate of correct predictions\n",
    "        for Class 0 (<b>{labels_ml[labels_ml['model'] == 'svm']['TN'].values[0]:.1%}</b>) and an acceptable rate for Class 1\n",
    "        (<b>{labels_ml[labels_ml['model'] == 'svm']['TP'].values[0]:.1%}</b>).\n",
    "    </p>\n",
    "    <p>\n",
    "        It should be kept in mind that Random Forest would have make a relevant choice as well, with the highest rate\n",
    "        of correct classification for class 1 (<b>{labels_ml.loc[labels_ml['TP'].idxmax(), 'TP']:.1%}</b>) despite a lower\n",
    "        accuracy (<b>{labels_ml[labels_ml['model'] == 'rf']['accuracy'].values[0]:.1%}</b>). In the present context, \n",
    "        we want to limit the number of wrong predictions for class 1. Indeed, should an individual be incorrectly labeled as \"patient\", further diagnostic\n",
    "        testing would soon reveal that this individual do not have CV disease. On the other hand, misclassifiying a patient as\n",
    "        not having CV disease may take them out of medical care for a wild and have consequences.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4868509",
   "metadata": {},
   "source": [
    "<h4>Code for: SVM - Parameters Exploration</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec6ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_scale_gamma = 0.691955007227351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a439a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f7b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce9c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_1 = SVC()\n",
    "svm_1.fit(X_train, y_train)\n",
    "svm_pred_1 = svm_1.predict(X_test)\n",
    "\n",
    "svm_params_1 = svm_1.get_params()\n",
    "svm_report_1 = classification_report(pd.crosstab(y_test, svm_pred_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edda64c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add54279",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37407d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5a8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a8267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6abdd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08135d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "xgb_params = xgb.get_params()\n",
    "\n",
    "xgb_report = classification_report(y_test, y_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a2583",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>XGBoost</h1>\n",
    "    <p class = 'intro'>\n",
    "        XGBoost, short for Extreme Gradient Boosting, is a powerful and efficient machine learning algorithm known \n",
    "        for its exceptional performance in various data science competitions and real-world applications. \n",
    "        It is an ensemble learning method based on decision trees and gradient boosting. XGBoost builds an ensemble of \n",
    "        decision trees sequentially, with each tree aiming to correct the errors of the previous ones. \n",
    "        It combines their predictions to make a final prediction. XGBoost introduces several innovations to \n",
    "        improve accuracy and efficiency:</p>\n",
    "    <h4>Key Points</h4>\n",
    "    <p style = 'border-left: 3px solid silver; padding-left: 5px; font-size : 12.5px; margin-left: 15px'>\n",
    "        <strong>Gradient Boosting:</strong> XGBoost uses gradient boosting, which minimizes the loss function by iteratively adding new trees. It pays more attention to the samples that are difficult to predict.\n",
    "        <br><strong>Regularization:</strong> XGBoost incorporates L1 and L2 regularization terms into the objective function to prevent overfitting.\n",
    "        <br><strong>Handling Missing Data:</strong> XGBoost can naturally handle missing data, making it suitable for datasets with incomplete information.\n",
    "        <br><strong>Parallel Processing:</strong> It efficiently utilizes parallel processing, speeding up training times.\n",
    "    </p>\n",
    "    <h2>Default Model</h2>\n",
    "    <h3>Parameters</h3>\n",
    "    <table class='table_1' style='width: 100% !important'>\n",
    "        <tr>\n",
    "            <th style='width: 15%'>Parameter</th>\n",
    "            <th style='width: 10%'>Value</th>\n",
    "            <th>Definition</th>\n",
    "            <th>Effect</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Number of Estimators</b><br><code>n_estimators</code></td>\n",
    "            <td>{xgb_params['n_estimators']}</td>\n",
    "            <td style='text-align: justify !important'>The number of boosting rounds or weak learners (trees) to be combined in the ensemble.</td>\n",
    "            <td style='text-align: justify !important'>Increasing the number of estimators may improve performance up to a certain point but could lead to overfitting if set too high. There's also a trade-off between computational cost and gains in accuracy.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Learning Rate</b><br><code>learning_rate</code></td>\n",
    "            <td>{xgb_params['learning_rate']}</td>\n",
    "            <td style='text-align: justify !important'>The step size shrinkage used in update to prevent overfitting.</td>\n",
    "            <td style='text-align: justify !important'>Smaller values may improve generalization but require a higher number of estimators. It controls the contribution of each tree to the final prediction.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Booster Type</b><br><code>booster</code></td>\n",
    "            <td>{xgb_params['booster']}</td>\n",
    "            <td style='text-align: justify !important'>The type of boosting model to be used. Common values are 'gbtree' (tree-based models), 'gblinear' (linear models), and 'dart' (Dropouts meet Multiple Additive Regression Trees).</td>\n",
    "            <td style='text-align: justify !important'>The booster type influences the learning process and the structure of weak learners. 'gbtree' is commonly used for its effectiveness.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Column Subsampling</b><br><code>colsample_bytree</code></td>\n",
    "            <td>{xgb_params['colsample_bytree']}</td>\n",
    "            <td style='text-align: justify !important'>Fraction of features to be randomly sampled for each tree.</td>\n",
    "            <td style='text-align: justify !important'>Controlling the fraction of features used helps prevent overfitting by introducing diversity among the trees.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>Maximum Tree Depth</b><br><code>max_depth</code></td>\n",
    "            <td>{xgb_params['max_depth']}</td>\n",
    "            <td style='text-align: justify !important'>The maximum depth of each tree in the ensemble.</td>\n",
    "            <td style='text-align: justify !important'>A deeper tree can capture more complex patterns but may lead to overfitting. It's a crucial parameter to control the complexity of the model.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h3>Classification Results</h3>\n",
    "    {matrix_display(pd.crosstab(y_test, y_pred))}\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "classification_plot(pd.crosstab(y_test, y_pred))        \n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Metrics</h3>\n",
    "</div>\n",
    "\"\"\"))\n",
    "\n",
    "perf_barplot(xgb_report)\n",
    "report_table(xgb_report)\n",
    "\n",
    "xgb_fi = pd.DataFrame({'var': X.columns, 'importance' : xgb.feature_importances_})\n",
    "\n",
    "xgb_fi = xgb_fi.sort_values(by = 'importance', ascending = False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "sns.barplot(\n",
    "    y = 'var', \n",
    "    x = 'importance',\n",
    "    data = xgb_fi,\n",
    "    edgecolor = 'black', \n",
    "    ax = ax\n",
    ")\n",
    "\n",
    "ax.axvline(x = 0.1, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 0.2, linestyle = 'dotted', color = 'black')\n",
    "ax.axvline(x = 0.5, linestyle = 'dotted', color = 'black')\n",
    "\n",
    "ax.set_ylabel('Features', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_xlim(0,0.6)\n",
    "ax.set_xticks(ticks = [x/10 for x in range(0,7,1)], labels = [x/10 for x in range(0,7,1)])\n",
    "ax.set_xlabel('Feature Coefficient', fontdict = fontdict_labels)\n",
    "\n",
    "ax.set_title(None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed05f660",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e803bf0",
   "metadata": {},
   "source": [
    "<div class = 'all'>\n",
    "<h2>Overview of Decision Trees:</h2>\n",
    "<p>A Decision Tree is a supervised machine learning algorithm that can be used for both classification and regression tasks. In the context of your project, where the target variable is \"cardio\" (indicating the presence or absence of cardiovascular disease), we will focus on classification Decision Trees.</p>\n",
    "\n",
    "<h3>How Decision Trees Work:</h3>\n",
    "<p>A Decision Tree divides the dataset into subsets based on the values of input features, creating a tree-like structure of decisions. Each internal node of the tree represents a feature, each branch represents a decision based on that feature, and each leaf node represents a class label (in your case, \"cardio\" class) or a regression value.</p>\n",
    "\n",
    "<p>The process of building a Decision Tree involves selecting the best feature at each step to split the data, based on criteria such as Gini impurity or information gain. The tree continues to split until a stopping criterion is met, such as a maximum tree depth or a minimum number of samples per leaf.</p>\n",
    "\n",
    "<h2>Pros of Using Decision Trees for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Interpretability:</strong> Decision Trees are easy to understand and interpret, making them suitable for explaining the model's decisions to non-technical stakeholders, such as healthcare professionals.</li>\n",
    "    <li><strong>Feature Importance:</strong> Decision Trees can provide information about feature importance, helping you identify which health-related factors contribute the most to cardiovascular disease prediction.</li>\n",
    "    <li><strong>Non-Linear Relationships:</strong> Decision Trees can capture non-linear relationships between input features and the target variable, which may be important in medical diagnosis.</li>\n",
    "    <li><strong>Handling Missing Values:</strong> Decision Trees can handle missing data without requiring extensive preprocessing, which is beneficial if your dataset contains incomplete information.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Cons of Using Decision Trees for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Overfitting:</strong> Decision Trees are prone to overfitting, especially when the tree becomes too deep or complex. Overfitting may lead to poor generalization to unseen data.</li>\n",
    "    <li><strong>Instability:</strong> Small variations in the data can result in significantly different trees, making Decision Trees less stable compared to some other algorithms.</li>\n",
    "    <li><strong>Not Ideal for Highly Imbalanced Data:</strong> If your dataset has a severe class imbalance (e.g., many more instances of one class than the other), Decision Trees may produce biased results.</li>\n",
    "    <li><strong>Difficulty with Complex Relationships:</strong> Decision Trees may struggle to model complex relationships that involve multiple interacting features.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion:</h2>\n",
    "<p>Decision Trees offer a transparent and interpretable approach to classification tasks like cardiovascular disease prediction. They can provide valuable insights into feature importance and non-linear relationships in your data. However, to mitigate overfitting and instability, you may need to apply techniques like pruning or ensemble methods, such as Random Forests or Gradient Boosting, which can enhance the performance of Decision Trees while maintaining their interpretability.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30b69e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6f6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe5bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed5cf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df15beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepration of 2 dfs\n",
    "df = pd.read_csv('clean_cvd.csv', sep = \",\")            # The very same df that was used on Part I\n",
    "df1 = pd.read_csv('clean_cvd.csv', sep = \",\")            # Modified df taking Part I's suggestions into account\n",
    "\n",
    "def df_set_dtypes(df):\n",
    "    df[['smoke', 'alco', 'active', 'cardio', 'cholesterol', 'gluc','ap_aha', 'lifestyle', 'healthy_ls']] = df[['smoke', 'alco', 'active', 'cardio', 'cholesterol', 'gluc','ap_aha', 'lifestyle', 'healthy_ls']].astype('str') \n",
    "    df[['sex', 'smoke', 'alco', 'active', 'cardio', 'lifestyle', 'healthy_ls']] = df[['sex', 'smoke', 'alco', 'active', 'cardio', 'lifestyle', 'healthy_ls']].astype('category')\n",
    "    cat_gluc_chol = pd.CategoricalDtype(categories = [\"1\", \"2\", \"3\"], ordered = True)\n",
    "    df[['cholesterol', 'gluc']] = df[['cholesterol', 'gluc']].astype(cat_gluc_chol)\n",
    "    cat_aha = pd.CategoricalDtype(categories = [\"1\", \"2\", \"3\", \"4\"], ordered = True)\n",
    "    df['ap_aha'] = df['ap_aha'].astype(cat_aha)\n",
    "    cat_lifestyle = pd.CategoricalDtype(categories = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"], ordered = False)\n",
    "    df['lifestyle'] = df['lifestyle'].astype(cat_lifestyle)\n",
    "    return df\n",
    "\n",
    "df_raw = df_set_dtypes(df_raw)\n",
    "df = df_set_dtypes(df)\n",
    "df1 = df_set_dtypes(df1)\n",
    "\n",
    "df1 = df1.drop(['height', 'gluc', 'smoke'], axis = 1)  # REFLEXIONNER SUR BLOUDE PRECHEURE \n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "w_max = np.percentile(df['weight'], 97.5)\n",
    "w_min = np.percentile(df['weight'], 2.5)\n",
    "\n",
    "df1 = df1[(df1['weight'] <= w_max) & (df1['weight'] >= w_min)]     # Dropping values outside of weight's 95% CI\n",
    "\n",
    "df1_m = df1[df1['sex'] == \"male\"]\n",
    "df1_f = df1[df1['sex'] == \"female\"]\n",
    "\n",
    "frac = len(df1_m)/len(df1_f)\n",
    "\n",
    "df1_f = df1_f.sample(frac = frac, random_state = 41)\n",
    "\n",
    "df1 = pd.concat([df1_m, df1_f])\n",
    "\n",
    "def preprocessing_df(df, gluc = \"no\"):\n",
    "    \n",
    "    # Getting dummies for unordered categorical features\n",
    "    df = pd.get_dummies(df, columns = ['sex'])\n",
    "    df = df.drop(['age_group', 'lifestyle', 'ap_m', 'ap_aha'], axis = 1)\n",
    "    \n",
    "    # Encoding ordered categorical features\n",
    "    label_encoder = LabelEncoder()              \n",
    "    df[\"cholesterol\"] = label_encoder.fit_transform(df[\"cholesterol\"])\n",
    "    #df[\"ap_aha\"] = label_encoder.fit_transform(df[\"ap_aha\"])\n",
    "    \n",
    "    if gluc == \"yes\":\n",
    "        df[\"gluc\"] = label_encoder.fit_transform(df[\"gluc\"])\n",
    "\n",
    "    # Min-Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    columns_df = df.columns          # Saving columns names before scaling\n",
    "    df = scaler.fit_transform(df)\n",
    "    df = pd.DataFrame(df, columns = columns_df)\n",
    "\n",
    "    # Train and Test datasets\n",
    "    X = df.drop(\"cardio\", axis = 1)\n",
    "    y = df['cardio']\n",
    "     \n",
    "    return X, y\n",
    "\n",
    "X, y = preprocessing_df(df, gluc = \"yes\")\n",
    "\n",
    "#X1, y1 = preprocessing_df(df1)\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Preprocessing</h1>\n",
    "    \n",
    "    <p class = 'intro'>\n",
    "        Following data processing and data viz realisation in Part I, the dataset now contains {df.shape[0]} rows\n",
    "        and {df.shape[1]} columns. This section will cover the preprocessing needed to have dataset fit for machine learning : categorical encoding, normalisation and train/test splitting.\n",
    "    </p>\n",
    "    <h2>Datasets</h2>\n",
    "    <p>\n",
    "        Two datasets were considered for machine learning. Dataset named <code>df0</code> is the dataset used in Part I,\n",
    "        containing all columns previously described. A second dataset, <code>df1</code>, was design according to Part I's\n",
    "        conclusions, therefore:\n",
    "        <ul>\n",
    "            <li><code>height</code>, <code>gluc</code> and <code>smoke</code> were dropped</li>\n",
    "            <li>extreme values for <code>weight</code> were dropped (<i>ie</i>: values outside of the 95% CI)</li>\n",
    "            <li>Entries from female subjects were randomly dropped so <code>sex</code> could be balanced</li>\n",
    "        </ul>\n",
    "        Consequently, this second dataset comprised <b>{df1.shape[0]} rows</b> and <b>{df1.shape[1]} columns</b>. \n",
    "        BMI ranged from <b>{np.min(df1['bmi']):.1f}</b> to <b>{np.max(df1['bmi']):.1f}</b> kg/m² (compared to\n",
    "        {np.min(df['bmi']):.1f} to {np.max(df['bmi']):.1f} kg/m² in <code>df0</code>) and the proportion of patients\n",
    "        among females was not impacted by sex-balancing; \n",
    "        <b>{df1[df1['sex'] == 'female']['cardio'].value_counts(normalize=True)[\"1\"]:.1%}</b> in \n",
    "        <code>df1</code> <i>vs</i> {df[df['sex'] == 'female']['cardio'].value_counts(normalize=True)[\"1\"]:.1%} \n",
    "        in <code>df0</code>.\n",
    "    </p>\n",
    "    <p>\n",
    "        Modelling will be performed on <code>df0</code> and <code>df1</code> only, to avoid training too many models.\n",
    "        Extra datasets shall be trained of the most performant model:\n",
    "        <ul>\n",
    "            <li>Separate models for males and females</li>\n",
    "            <li>Models based on age</li>\n",
    "            <li>If relevant: models dedicated to a specific age range for both sex</li>\n",
    "        </ul>\n",
    "    </p>\n",
    "    <h2>Preprocessing Operations</h2>\n",
    "    <h3>Feature Encoding</h3>\n",
    "    <p>\n",
    "        Unordered categorical features, ie <code>sex</code>, <code>age_group</code>, and <code>lifestyle</code> were\n",
    "        encoded using <code>pd.get_dummies()</code>.\n",
    "        Ordered categorical features, <i>ie</i> <code>cholesterol</code>, <code>gluc</code> and <code>ap_aha</code>, \n",
    "        were encoded using <code>LabelEncoder()</code>, as their modalities were originally passed as strings.\n",
    "    </p>\n",
    "    <h3>Normalisation</h3>\n",
    "    <p>\n",
    "        Among preprocessing techniques, normalisation aims at scaling data features to a specific range, \n",
    "        often between 0 and 1. It felt appropriate since the scale of the features of this dataset varies significantly, \n",
    "        with majority of features having a non-gaussian distribution. \n",
    "        Normalisation helps bring all features to a common scale, making them directly comparable.\n",
    "    </p>\n",
    "    <h3>Train and Test Datasets</h3>\n",
    "</div>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ae9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc403482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a69a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb83fd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c92b6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0cc0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h1>Logistic Regression:</h1>\n",
    "    <p class = 'intro'>A Decision Tree is a supervised machine learning algorithm that can be used for \n",
    "    both classification and regression tasks. In the context of your project, where the target variable is \"cardio\" \n",
    "    (indicating the presence or absence of cardiovascular disease), we will focus on classification Decision Trees..\n",
    "    </p>\n",
    "    <h2>Pros and Cons:</h2>\n",
    "    <h3>Pros</h3>\n",
    "    <ul>\n",
    "        <li><b>Interpretability:</b> Logistic Regression provides interpretable results. The impact of each feature \n",
    "        on the predicted probability is made easy to understand: it produces coefficients for each feature,\n",
    "        allowing the identification of which features are significant.</li>\n",
    "        <li><b>Efficiency:</b> Logistic Regression is computationally efficient and can handle large datasets with \n",
    "        relatively low computational resources. It trains quickly and can be a good baseline model.</li>\n",
    "        <li><b>Well-suited for Linear Relationships:</b> When the relationship between features and the target variable \n",
    "        is approximately linear, Logistic Regression can perform well. It's effective in capturing linear patterns in \n",
    "        the data.</li>\n",
    "        <li><b>Low Risk of Overfitting:</b> Logistic Regression is less prone to overfitting, especially when there's \n",
    "        not a vast amount of data. Regularization techniques like L1 (Lasso) and L2 (Ridge) can be applied to control \n",
    "        overfitting further.</li>\n",
    "    </ul>\n",
    "\n",
    "    <h3>Cons:</h3>\n",
    "    <ul>\n",
    "        <li><b>Limited Complexity:</b> Logistic Regression is a linear model, which means it assumes a \n",
    "        linear relationship between the input features and the log-odds of the target variable. If your data contains \n",
    "        complex, nonlinear relationships, Logistic Regression may not capture them well.</li>\n",
    "        <li><b>Assumes Independence of Features:</b> Logistic Regression assumes that features are independent \n",
    "        of each other. If there is multicollinearity (high correlation) among your features, it can \n",
    "        affect the model's performance and interpretation.</li>\n",
    "        <li><b>Not Ideal for Imbalanced Data:</b> In case of a severe class imbalance,\n",
    "        Logistic Regression might not perform well without proper balancing techniques or modifications.</li>\n",
    "        <li><b>No Probabilistic Scores:</b> While Logistic Regression provides probability scores, they can sometimes be poorly \n",
    "        calibrated, meaning the predicted probabilities may not reflect the true likelihood of an event. \n",
    "        Calibrating the model may be necessary for certain applications.</li>\n",
    "    </ul>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89516e39",
   "metadata": {},
   "source": [
    "<div class = 'all'>\n",
    "<h2>Overview of Random Forest:</h2>\n",
    "<p>The Random Forest algorithm is an ensemble learning technique widely used for both classification and regression tasks. It is an ensemble of decision trees, where multiple decision trees work together to make predictions.</p>\n",
    "\n",
    "<h3>How Random Forest Works:</h3>\n",
    "<p>Random Forest builds multiple decision trees independently and combines their predictions through a voting or averaging mechanism.</p>\n",
    "<ul>\n",
    "    <li><strong>Bootstrap Sampling:</strong> It starts by creating random subsets of the training data through bootstrap sampling (randomly selecting data points with replacement).</li>\n",
    "    <li><strong>Feature Randomization:</strong> At each node of each tree, only a random subset of features is considered for splitting. This introduces diversity among the trees.</li>\n",
    "    <li><strong>Aggregation:</strong> For classification tasks, Random Forest typically uses a majority voting mechanism to make predictions. For regression tasks, it averages the predictions of individual trees.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Pros of Using Random Forest for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>High Accuracy:</strong> Random Forest is known for its high predictive accuracy and generalization performance. It often outperforms single decision trees.</li>\n",
    "    <li><strong>Robustness:</strong> It is less prone to overfitting compared to individual decision trees, making it suitable for complex datasets.</li>\n",
    "    <li><strong>Feature Importance:</strong> Random Forest provides a measure of feature importance, helping you identify which health-related factors are most influential in predicting cardiovascular disease.</li>\n",
    "    <li><strong>Handles Missing Data:</strong> Random Forest can handle datasets with missing values without requiring extensive preprocessing.</li>\n",
    "    <li><strong>Reduced Risk of Bias:</strong> It mitigates the bias that can be present in imbalanced datasets by considering class proportions during training.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Cons of Using Random Forest for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Complexity:</strong> Random Forest models can be computationally intensive and may require longer training times, especially with a large number of trees.</li>\n",
    "    <li><strong>Less Interpretability:</strong> While Random Forest can provide feature importance, the model's ensemble nature makes it less interpretable than individual decision trees.</li>\n",
    "    <li><strong>Hyperparameter Tuning:</strong> Proper hyperparameter tuning is essential for optimal performance, and it may require experimentation.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion:</h2>\n",
    "<p>Random Forest is a powerful ensemble learning method that can provide high accuracy and robust predictions for the classification of cardiovascular disease. Its ability to handle complex datasets and mitigate overfitting makes it a strong candidate for your project. However, it's important to carefully tune hyperparameters and interpret feature importance to make the most of this model. The trade-offs between accuracy, interpretability, and computational resources should be considered based on your project's specific objectives and constraints.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865acddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_rf = accuracy_score(y_test, y_pred)\n",
    "confusion_rf = confusion_matrix(y_test, y_pred)\n",
    "classification_rep_rf = classification_report(y_test, y_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4c980",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h2>Random Forest</h2>\n",
    "</div>\n",
    "\"\"\"))\n",
    "matrix_display(confusion_rf)\n",
    "report_table(classification_rep_rf)\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Interpretation</h3>\n",
    "    <p>\n",
    "    The Random Forest model achieves an accuracy of approximately {classification_rep_rf['accuracy']:.3f}, \n",
    "    indicating that it correctly classifies about {classification_rep_rf['accuracy']:.1%} of instances in the dataset.\n",
    "    For Class 0 (No Cardiovascular Disease), the model has good precision ({classification_rep_rf['0.0']['precision']:.3f})\n",
    "    and recall ({classification_rep_rf['0.0']['recall']:.3f}), resulting in an F1-Score of \n",
    "    {classification_rep_rf['0.0']['f1-score']:.3f}. This suggests a balanced performance in terms of minimizing both \n",
    "    false positives and false negatives for this class.\n",
    "    For Class 1 (Cardiovascular Disease), the model has slightly lower precision \n",
    "    ({classification_rep_rf['1.0']['precision']:.3f}) but still reasonable recall \n",
    "    ({classification_rep_rf['1.0']['recall']:.3f}), resulting in an F1-Score of {classification_rep_rf['1.0']['f1-score']:.3f}.\n",
    "    This indicates that the model correctly identifies many true cases while maintaining a relatively low false positive rate.\n",
    "    The macro and weighted average metrics show consistent performance across classes, with F1-Scores around \n",
    "    {classification_rep_rf['weighted avg']['f1-score']:.3f}.\n",
    "    Overall, the Random Forest model provides a good level of predictive performance for the task of cardiovascular\n",
    "    disease prediction. It offers a balanced trade-off between precision and recall for both classes. \n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2f91c5",
   "metadata": {},
   "source": [
    "<h2>Overview of Support Vector Machine (SVM):</h2>\n",
    "<p>The Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. It is particularly effective in situations where data may not be linearly separable, making it a versatile choice for various applications.</p>\n",
    "\n",
    "<h3>How SVM Works:</h3>\n",
    "<p>SVM works by finding the hyperplane that maximizes the margin between different classes of data points. It aims to create a decision boundary that effectively separates data points into distinct categories while maximizing the margin between the classes.</p>\n",
    "<ul>\n",
    "    <li><strong>Kernel Trick:</strong> SVM can utilize kernel functions (e.g., linear, polynomial, radial basis function) to transform data into higher-dimensional space, where it may become linearly separable.</li>\n",
    "    <li><strong>Support Vectors:</strong> SVM focuses on support vectors, which are data points closest to the decision boundary. These support vectors play a crucial role in defining the optimal hyperplane.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Pros of Using SVM for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Effective in High-Dimensional Spaces:</strong> SVM can handle datasets with a large number of features, making it suitable for complex healthcare datasets.</li>\n",
    "    <li><strong>Robust to Outliers:</strong> SVM is less sensitive to outliers because it focuses on support vectors that have a significant influence on the decision boundary.</li>\n",
    "    <li><strong>Can Handle Non-Linear Data:</strong> By using kernel functions, SVM can effectively handle non-linear data, potentially improving predictive accuracy.</li>\n",
    "    <li><strong>Maximizes Margin:</strong> SVM aims to find the optimal margin between classes, which can result in better generalization to unseen data.</li>\n",
    "    <li><strong>Well-Suited for Binary Classification:</strong> SVM is naturally designed for binary classification tasks, making it appropriate for predicting cardiovascular disease (presence or absence).</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Cons of Using SVM for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Computationally Intensive:</strong> SVM can be computationally intensive, especially with large datasets and complex kernels. Training may take longer compared to some other algorithms.</li>\n",
    "    <li><strong>Hyperparameter Tuning:</strong> Achieving optimal performance with SVM often requires tuning hyperparameters like the choice of kernel and regularization parameter, which may require experimentation.</li>\n",
    "    <li><strong>Interpretability:</strong> SVM models can be less interpretable compared to decision trees or logistic regression, making it challenging to understand the factors driving predictions.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion:</h2>\n",
    "<p>SVM is a versatile and effective machine learning algorithm for binary classification tasks like predicting cardiovascular disease. Its ability to handle high-dimensional and potentially non-linear data, along with its robustness to outliers, makes it a strong candidate for your project. However, it's important to consider the computational resources required for training and fine-tuning, as well as the interpretability of the model. Ultimately, the choice of SVM or other algorithms should align with your specific project objectives and constraints.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3deaf0f",
   "metadata": {},
   "source": [
    "<h2>Overview of k-Nearest Neighbors (KNN):</h2>\n",
    "<p>The k-Nearest Neighbors (KNN) algorithm is a versatile supervised machine learning approach commonly used for classification and regression tasks. It stands out for its simplicity and effectiveness, making it applicable across various domains.</p>\n",
    "\n",
    "<h3>How KNN Works:</h3>\n",
    "<p>KNN operates by identifying the k-nearest data points to a given test point and making predictions based on the majority class (for classification) or average (for regression) of those neighbors. It doesn't assume a specific underlying functional form and can adapt to different data distributions.</p>\n",
    "<ul>\n",
    "    <li><strong>Distance Metric:</strong> KNN relies on distance metrics (e.g., Euclidean distance) to measure proximity between data points. The choice of distance metric influences the model's sensitivity to different features.</li>\n",
    "    <li><strong>Hyperparameter K:</strong> The parameter k determines the number of neighbors considered during predictions. Adjusting k can impact the model's bias-variance trade-off.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Pros of Using KNN for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Simple and Intuitive:</strong> KNN is easy to understand and implement, making it a good choice for projects where interpretability is important.</li>\n",
    "    <li><strong>Adaptable to Data Distribution:</strong> KNN doesn't make strong assumptions about the underlying data distribution, allowing it to handle diverse datasets effectively.</li>\n",
    "    <li><strong>Non-Parametric:</strong> Being a non-parametric method, KNN doesn't make assumptions about the form of the underlying model, making it suitable for various types of data.</li>\n",
    "    <li><strong>Effective for Small Datasets:</strong> KNN can perform well with smaller datasets, and its performance can improve with increased data size.</li>\n",
    "    <li><strong>No Training Phase:</strong> KNN doesn't have a traditional training phase; it memorizes the training data and makes predictions on the fly.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Cons of Using KNN for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Computational Cost during Prediction:</strong> Calculating distances for each prediction can be computationally expensive, especially with large datasets.</li>\n",
    "    <li><strong>Sensitive to Irrelevant Features:</strong> KNN can be sensitive to irrelevant or noisy features, and feature scaling may be necessary.</li>\n",
    "    <li><strong>Requires Optimal k:</strong> Selecting the appropriate value for k is crucial, and an improper choice may lead to overfitting or underfitting.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion:</h2>\n",
    "<p>KNN is a flexible and straightforward machine learning algorithm suitable for various projects, including binary classification tasks. Its adaptability to different data distributions, simplicity, and effectiveness make it a valuable choice. However, it's essential to consider computational costs, especially with larger datasets, and to carefully choose hyperparameters for optimal performance. Ultimately, the suitability of KNN or other algorithms depends on your specific project requirements and constraints.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45dbf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel = 'linear', C = 1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae28f8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_svm = accuracy_score(y_test, y_pred)\n",
    "confusion_svm = confusion_matrix(y_test, y_pred)\n",
    "classification_rep_svm = classification_report(y_test, y_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1129ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h2>Support Machine Vector</h2>\n",
    "</div>\n",
    "\"\"\"))\n",
    "matrix_display(confusion_svm)\n",
    "report_table(classification_rep_svm)\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Interpretation</h3>\n",
    "    <p>\n",
    "    The SVM model achieves an accuracy of approximately {classification_rep_svm['accuracy']:.3f}, indicating that\n",
    "    it correctly classifies about {classification_rep_svm['accuracy']:.1%} of instances in the dataset.\n",
    "    For Class 0 (No Cardiovascular Disease), the model has good precision\n",
    "    ({classification_rep_svm['0.0']['precision']:.3f}) and high recall ({classification_rep_svm['0.0']['recall']:.3f}),\n",
    "    resulting in an F1-Score of {classification_rep_svm['0.0']['f1-score']:.3f}. \n",
    "    This suggests a balanced performance in terms of minimizing both false positives and false negatives for this class.\n",
    "    For Class 1 (Cardiovascular Disease), the model has higher precision ({classification_rep_svm['1.0']['precision']:.3f})\n",
    "    but lower recall ({classification_rep_svm['1.0']['recall']:.3f}), \n",
    "    resulting in an F1-Score of {classification_rep_svm['1.0']['f1-score']:.3f}. This indicates that the model correctly \n",
    "    identifies many true cases but may miss some positive cases.\n",
    "    The macro and weighted average metrics show consistent performance across classes,\n",
    "    with F1-Scores around {classification_rep_svm['weighted avg']['f1-score']:.3f} and precision-recall balance.\n",
    "    Overall, the SVM model provides a good level of predictive performance for the task of cardiovascular disease \n",
    "    prediction. It demonstrates a trade-off between precision and recall, making it suitable for \n",
    "    situations where both minimizing false positives and false negatives are important.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6a4dd",
   "metadata": {},
   "source": [
    "<h2>Overview of AdaBoost:</h2>\n",
    "<p>AdaBoost, short for Adaptive Boosting, is a popular ensemble learning technique used for classification and regression tasks. It is designed to improve the accuracy of weak learners (classifiers) by combining their predictions into a strong ensemble model.</p>\n",
    "\n",
    "<h3>How AdaBoost Works:</h3>\n",
    "<p>AdaBoost works by sequentially training multiple weak classifiers on the dataset. Each weak classifier focuses on the data points that were misclassified by the previous ones. The algorithm assigns more weight to misclassified data points, allowing subsequent classifiers to pay more attention to them. The final prediction is a weighted combination of the weak classifiers.</p>\n",
    "<ul>\n",
    "    <li><strong>Sequential Learning:</strong> Weak learners are trained sequentially, and their performance guides subsequent iterations.</li>\n",
    "    <li><strong>Weighted Data:</strong> AdaBoost assigns different weights to data points, emphasizing the importance of misclassified samples.</li>\n",
    "    <li><strong>Weighted Voting:</strong> The final prediction is a weighted vote of all weak classifiers, with more accurate classifiers having a greater influence.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Pros of Using AdaBoost for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Improved Accuracy:</strong> AdaBoost combines the predictions of multiple weak learners to create a strong ensemble, often leading to higher accuracy compared to individual classifiers.</li>\n",
    "    <li><strong>Handles Non-Linearity:</strong> AdaBoost can capture non-linear relationships between features and the target variable, which can be beneficial for complex datasets.</li>\n",
    "    <li><strong>Feature Importance:</strong> The algorithm can provide insights into feature importance, helping you identify the most relevant factors in cardiovascular disease prediction.</li>\n",
    "    <li><strong>Less Prone to Overfitting:</strong> AdaBoost is less prone to overfitting compared to complex models, as it focuses on misclassified data points.</li>\n",
    "    <li><strong>Adaptive:</strong> AdaBoost adapts to difficult-to-classify instances by assigning higher weights to them, potentially improving model performance.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Cons of Using AdaBoost for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Sensitive to Noisy Data:</strong> AdaBoost can be sensitive to noisy data and outliers, as it assigns more weight to misclassified points, which may include noisy samples.</li>\n",
    "    <li><strong>Computationally Intensive:</strong> Training multiple weak learners sequentially can be computationally intensive, especially with large datasets.</li>\n",
    "    <li><strong>Interpretability:</strong> AdaBoost models may be less interpretable than simpler models like logistic regression.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion:</h2>\n",
    "<p>AdaBoost is a robust ensemble learning algorithm known for its ability to improve classification accuracy, even with weak base classifiers. It is a suitable choice for binary classification tasks like predicting cardiovascular disease. However, it's important to be aware of its sensitivity to noisy data and the computational resources required for training. Additionally, feature importance analysis can provide valuable insights into the factors influencing predictions. Depending on your specific project objectives and constraints, AdaBoost can be an effective tool for accurate disease prediction.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768143cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(), n_estimators = 50)\n",
    "adaboost.fit(X_train, y_train)\n",
    "y_pred = adaboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c511857",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_ada = accuracy_score(y_test, y_pred)\n",
    "confusion_ada = confusion_matrix(y_test, y_pred)\n",
    "classification_rep_ada = classification_report(y_test, y_pred, output_dict = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346d30fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h2>AdaBoost</h2>\n",
    "</div>\n",
    "\"\"\"))\n",
    "matrix_display(confusion_ada)\n",
    "report_table(classification_rep_ada)\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Interpretation</h3>\n",
    "    <p>\n",
    "    The AdaBoost model achieves an accuracy of approximately {classification_rep_ada['accuracy']:.3f}, \n",
    "    indicating that it correctly classifies about {classification_rep_ada['accuracy']:.1%} of instances in the dataset.\n",
    "    For Class 0 (No Cardiovascular Disease), the model has a precision of {classification_rep_ada['0.0']['precision']:.3f}\n",
    "    and recall of {classification_rep_ada['0.0']['recall']:.3f}, resulting in an F1-Score of\n",
    "    {classification_rep_ada['1.0']['f1-score']:.3f}. This suggests a balanced performance in terms of minimizing both \n",
    "    false positives and false negatives for this class.\n",
    "    For Class 1 (Cardiovascular Disease), the model has similar precision ({classification_rep_ada['1.0']['precision']:.3f}) \n",
    "    and recall ({classification_rep_ada['1.0']['recall']:.3f}), resulting in an F1-Score of {classification_rep_ada['1.0']['f1-score']:.3f}.\n",
    "    This indicates that the model correctly identifies many true cases but may miss some positive cases.\n",
    "    The macro and weighted average metrics show consistent performance across classes, with F1-Scores around\n",
    "    {classification_rep_ada['weighted avg']['f1-score']:.3f} and a good balance between precision and recall.\n",
    "    Overall, the AdaBoost model provides a satisfactory level of predictive performance for the task of \n",
    "    cardiovascular disease prediction. It demonstrates a trade-off between precision and recall, making it \n",
    "    suitable for situations where both minimizing false positives and false negatives are important. \n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2491be3",
   "metadata": {},
   "source": [
    "<div class = 'all'>\n",
    "<h2>Overview of XGBoost:</h2>\n",
    "<p>XGBoost, short for Extreme Gradient Boosting, is a powerful and efficient machine learning algorithm known for its exceptional performance in various data science competitions and real-world applications. It is an ensemble learning method based on decision trees and gradient boosting.</p>\n",
    "\n",
    "<h3>How XGBoost Works:</h3>\n",
    "<p>XGBoost builds an ensemble of decision trees sequentially, with each tree aiming to correct the errors of the previous ones. It combines their predictions to make a final prediction. XGBoost introduces several innovations to improve accuracy and efficiency:</p>\n",
    "<ul>\n",
    "    <li><strong>Gradient Boosting:</strong> XGBoost uses gradient boosting, which minimizes the loss function by iteratively adding new trees. It pays more attention to the samples that are difficult to predict.</li>\n",
    "    <li><strong>Regularization:</strong> XGBoost incorporates L1 and L2 regularization terms into the objective function to prevent overfitting.</li>\n",
    "    <li><strong>Handling Missing Data:</strong> XGBoost can naturally handle missing data, making it suitable for datasets with incomplete information.</li>\n",
    "    <li><strong>Parallel Processing:</strong> It efficiently utilizes parallel processing, speeding up training times.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Pros of Using XGBoost for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>High Accuracy:</strong> XGBoost is known for its outstanding predictive performance and the ability to handle complex datasets.</li>\n",
    "    <li><strong>Regularization:</strong> It offers built-in regularization techniques to prevent overfitting, which is critical for healthcare datasets.</li>\n",
    "    <li><strong>Feature Importance:</strong> XGBoost provides feature importance scores, allowing you to identify the most influential factors in cardiovascular disease prediction.</li>\n",
    "    <li><strong>Handles Non-Linearity:</strong> XGBoost can capture non-linear relationships between features and the target variable, potentially improving model accuracy.</li>\n",
    "    <li><strong>Flexibility:</strong> It supports both classification and regression tasks, making it adaptable to various data science projects.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Cons of Using XGBoost for Your Project:</h2>\n",
    "<ul>\n",
    "    <li><strong>Complexity:</strong> XGBoost models can be more complex and require careful tuning of hyperparameters to achieve optimal performance.</li>\n",
    "    <li><strong>Computational Resources:</strong> Training XGBoost models can be resource-intensive, especially when using a large number of trees and features.</li>\n",
    "    <li><strong>Interpretability:</strong> Like other ensemble methods, XGBoost models may be less interpretable than simpler models like logistic regression.</li>\n",
    "</ul>\n",
    "\n",
    "<h2>Conclusion:</h2>\n",
    "<p>XGBoost is a state-of-the-art ensemble learning algorithm that excels in predictive accuracy and is well-suited for binary classification tasks like predicting cardiovascular disease. Its ability to handle non-linearity, feature importance analysis, and built-in regularization make it a strong candidate for your project. However, it's important to carefully tune hyperparameters, consider computational resources, and explore feature importance for model interpretation. Depending on your specific project objectives and constraints, XGBoost can be an excellent choice for accurate disease prediction.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eae7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost = xgb.XGBClassifier(n_estimators = 100, max_depth = 3, learning_rate = 0.1)\n",
    "xgboost.fit(X_train, y_train)\n",
    "y_pred = xgboost.predict(X_test)\n",
    "\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred)\n",
    "confusion_xgb = confusion_matrix(y_test, y_pred)\n",
    "classification_rep_xgb = classification_report(y_test, y_pred, output_dict = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca69c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h2>XGBoost</h2>\n",
    "</div>\n",
    "\"\"\"))\n",
    "matrix_display(confusion_xgb)\n",
    "report_table(classification_rep_xgb)\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <h3>Interpretation</h3>\n",
    "    <p>\n",
    "    The XGBoost model achieves an accuracy of approximately {classification_rep_xgb['accuracy']:.3f}, \n",
    "    indicating that it correctly classifies about {classification_rep_xgb['accuracy']:.1%} of instances in the dataset.\n",
    "    For Class 0 (No Cardiovascular Disease), the model has a precision of {classification_rep_xgb['0.0']['precision']:.3f}\n",
    "    and recall of {classification_rep_xgb['0.0']['recall']:.3f}, resulting in an F1-Score of \n",
    "    {classification_rep_xgb['1.0']['f1-score']:.3f}. This suggests a balanced performance in terms of minimizing \n",
    "    both false positives and false negatives for this class.\n",
    "    For Class 1 (Cardiovascular Disease), the model has a precision of {classification_rep_xgb['1.0']['precision']:.3f}\n",
    "    and recall of {classification_rep_xgb['1.0']['recall']:.3f}, resulting in an F1-Score of \n",
    "    {classification_rep_xgb['1.0']['f1-score']:.3f}. This indicates that the model correctly identifies many true cases but\n",
    "    may miss some positive cases.\n",
    "    The macro and weighted average metrics show consistent performance across classes, with F1-Scores around \n",
    "    {classification_rep_xgb['weighted avg']['f1-score']:.3f} and a good balance between precision and recall.\n",
    "    Overall, the XGBoost model provides a satisfactory level of predictive performance \n",
    "    for the task of cardiovascular disease prediction. It demonstrates a trade-off between precision and recall, \n",
    "    making it suitable for situations where both minimizing false positives and false negatives are important. \n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a3683",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th>Model</th>\n",
    "            <th>True Negatives</th>\n",
    "            <th>False Positives</th>\n",
    "            <th>False Negatives</th>\n",
    "            <th>True Positives</th>\n",
    "            <th>Accuracy</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Logistic Regression</td>\n",
    "            <td>{confusion_lr[0][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_lr[0][1]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_lr[1][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_lr[1][1]/len(X_test):.1%}</td>\n",
    "            <td>{accuracy_lr:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Decision Tree</td>\n",
    "            <td>{confusion_dt[0][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_dt[0][1]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_dt[1][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_dt[1][1]/len(X_test):.1%}</td>\n",
    "            <td>{accuracy_dt:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Random Forest</td>\n",
    "            <td>{confusion_rf[0][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_rf[0][1]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_rf[1][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_rf[1][1]/len(X_test):.1%}</td>\n",
    "            <td>{accuracy_rf:.1%}</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <td>Support Vector Machine</td>\n",
    "            <td>{confusion_svm[0][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_svm[0][1]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_svm[1][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_svm[1][1]/len(X_test):.1%}</td>\n",
    "            <td>{accuracy_svm:.1%}</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <td>AdaBoost</td>\n",
    "            <td>{confusion_ada[0][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_ada[0][1]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_ada[1][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_ada[1][1]/len(X_test):.1%}</td>\n",
    "            <td>{accuracy_ada:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>XGBoost</td>\n",
    "            <td>{confusion_xgb[0][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_xgb[0][1]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_xgb[1][0]/len(X_test):.1%}</td>\n",
    "            <td>{confusion_xgb[1][1]/len(X_test):.1%}</td>\n",
    "            <td>{accuracy_xgb:.1%}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\"\"\"))\n",
    "\n",
    "display(HTML(f\"\"\"\n",
    "<div class = 'all'>\n",
    "    <table>\n",
    "        <tr>\n",
    "            <th></th>\n",
    "            <th colspan = \"3\">Class 0</th>\n",
    "            <th colspan = \"3\">Class 1</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Model</th>\n",
    "            <th>Precision</th>\n",
    "            <th>Recall</th>\n",
    "            <th>F1-Score</th>\n",
    "            <th>Precision</th>\n",
    "            <th>Recall</th>\n",
    "            <th>F1-Score</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Logistic Regression</td>\n",
    "            <td>{classification_rep_lr['0.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_lr['0.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_lr['0.0']['f1-score']:.1%}</td>\n",
    "            <td>{classification_rep_lr['1.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_lr['1.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_lr['1.0']['f1-score']:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Decision Tree</td>\n",
    "            <td>{classification_rep_dt['0.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_dt['0.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_dt['0.0']['f1-score']:.1%}</td>\n",
    "            <td>{classification_rep_dt['1.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_dt['1.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_dt['1.0']['f1-score']:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Random Forest</td>\n",
    "            <td>{classification_rep_rf['0.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_rf['0.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_rf['0.0']['f1-score']:.1%}</td>\n",
    "            <td>{classification_rep_rf['1.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_rf['1.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_rf['1.0']['f1-score']:.1%}</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <td>Support Vector Machine</td>\n",
    "            <td>{classification_rep_svm['0.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_svm['0.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_svm['0.0']['f1-score']:.1%}</td>\n",
    "            <td>{classification_rep_svm['1.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_svm['1.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_svm['1.0']['f1-score']:.1%}</td>\n",
    "        </tr>  \n",
    "        <tr>\n",
    "            <td>AdaBoost</td>\n",
    "            <td>{classification_rep_ada['0.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_ada['0.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_ada['0.0']['f1-score']:.1%}</td>\n",
    "            <td>{classification_rep_ada['1.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_ada['1.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_ada['1.0']['f1-score']:.1%}</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>XGBoost</td>\n",
    "            <td>{classification_rep_xgb['0.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_xgb['0.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_xgb['0.0']['f1-score']:.1%}</td>\n",
    "            <td>{classification_rep_xgb['1.0']['precision']:.1%}</td>\n",
    "            <td>{classification_rep_xgb['1.0']['recall']:.1%}</td>\n",
    "            <td>{classification_rep_xgb['1.0']['f1-score']:.1%}</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b0ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame(classification_rep_lr)\n",
    "df_lr['model'] = 'Logistic Regression'\n",
    "df_dt = pd.DataFrame(classification_rep_dt)\n",
    "df_dt['model'] = 'Decision Tree'\n",
    "df_rf = pd.DataFrame(classification_rep_rf)\n",
    "df_rf['model'] = 'Random Forest'\n",
    "df_svm = pd.DataFrame(classification_rep_svm)\n",
    "df_svm['model'] = 'Support Vector Machine'\n",
    "df_ada = pd.DataFrame(classification_rep_ada)\n",
    "df_ada['model'] = 'AdaBoost'\n",
    "df_xgb = pd.DataFrame(classification_rep_xgb)\n",
    "df_xgb['model'] = 'XGBoost'\n",
    "\n",
    "dfs_to_concat = [df_lr, df_dt, df_rf, df_svm, df_ada, df_xgb]\n",
    "\n",
    "df_models = pd.concat(dfs_to_concat, axis=0)\n",
    "df_models.reset_index(drop=False, inplace=True)\n",
    "df_models.rename(columns={'index': 'metric', '0.0' : 'class_0', '1.0' : 'class_1'}, inplace = True)\n",
    "df_models = df_models[df_models['metric'] != 'support']\n",
    "\n",
    "df_accuracy = pd.DataFrame({'metric' : 'accuracy',\n",
    "                            'class_0' : [accuracy_lr, accuracy_dt, accuracy_rf, accuracy_svm,\n",
    "                                                               accuracy_ada, accuracy_xgb],\n",
    "                           'class_1' : [accuracy_lr, accuracy_dt, accuracy_rf, accuracy_svm,\n",
    "                                                               accuracy_ada, accuracy_xgb],\n",
    "                           'model' : ['Logistic Regression', 'Decision Tree', 'Random Forest',\n",
    "                                      'Support Vector Machine', 'AdaBoost', 'XGBoost'\n",
    "                                     ]}\n",
    "                          )\n",
    "df_models = pd.concat([df_models, df_accuracy], axis=0)\n",
    "df_models.reset_index(drop=True, inplace=True)\n",
    "df_models.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a7e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 2, ncols = 1, figsize = (12,15))\n",
    "sns.barplot(y = 'class_0', x = 'model', hue = 'metric', data = df_models, edgecolor = 'black', \n",
    "            palette = ['papayawhip', 'powderblue', 'plum', 'lightgrey'], ax = ax[0])\n",
    "ax[0].set_yticks([x/100 for x in range(0,105,5)])\n",
    "ax[0].set_yticklabels(range(0,105,5))\n",
    "ax[0].set_ylabel('Value (%)', fontdict = fontdict_labels)\n",
    "ax[0].set_xlabel('Model', fontdict = fontdict_labels)\n",
    "ax[0].set_title('Metrics of Tested Models for Class 0')\n",
    "ax[0].legend(edgecolor = 'black')\n",
    "\n",
    "sns.barplot(y = 'class_1', x = 'model', hue = 'metric', data = df_models, edgecolor = 'black', \n",
    "            palette = ['papayawhip', 'powderblue', 'plum', 'lightgrey'], ax = ax[1])\n",
    "ax[1].set_yticks([x/100 for x in range(0,105,5)])\n",
    "ax[1].set_yticklabels(range(0,105,5))\n",
    "ax[1].set_ylabel('Value (%)', fontdict = fontdict_labels)\n",
    "ax[1].set_xlabel('Model', fontdict = fontdict_labels)\n",
    "ax[1].set_title('Metrics of Tested Models for Class 1')\n",
    "ax[1].legend(edgecolor = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6c176",
   "metadata": {},
   "source": [
    "<div class = 'all'>\n",
    "    <h1>Cross-validation</h1>\n",
    "    <h2>What is Cross-Validation?</h2>\n",
    "    <p>Cross-validation is a critical technique in machine learning used to assess and validate the performance of a model. It involves dividing the dataset into subsets, training the model on some of these subsets, and evaluating it on others. The main goal is to simulate how the model performs on unseen data, providing a robust estimate of its generalization performance.</p>    \n",
    "    <h2>Why Use Cross-Validation?</h2>\n",
    "    <p>Cross-validation is essential for several reasons:\n",
    "        <ul>\n",
    "            <li><b>Assess Model Performance</b>: It provides a more accurate estimate of how well a model is likely to perform on unseen data compared to a single train-test split. This reduces the risk of selecting a model that performs well by chance on a particular data split.</li>\n",
    "            <li><b>Mitigate Overfitting</b>: It helps identify models that are prone to overfitting (learning the training data too closely) by evaluating them on multiple subsets of the data.</li>\n",
    "            <li><b>Robustness</b>: It enhances the robustness of model evaluation by using different data partitions, reducing the influence of outliers or peculiarities in a single split.</li>\n",
    "        </ul>\n",
    "    <h2>How Does Cross-Validation Work?</h2>\n",
    "    <p>Here's how cross-validation works step by step:\n",
    "        <ul>\n",
    "            <li><b>Data Splitting</b>: The dataset is divided into multiple subsets or \"folds.\" Common choices include 5-fold (data split into five subsets) and 10-fold cross-validation.</li>\n",
    "            <li><b>Training and Testing</b>: The model is trained on a portion of the data (e.g., four folds) called the \"training set.\" It is then evaluated on the remaining fold, known as the \"validation set\" or \"test set.\" This process is repeated for each fold.</li>\n",
    "            <li><b>Performance Metrics</b>: For each fold, performance metrics (e.g., accuracy, F1-score, mean squared error) are calculated based on model predictions on the validation set.</li>\n",
    "            <li><b>Average Scores</b>: The performance scores from each fold are averaged to provide a more reliable estimate of the model's performance. Additionally, standard deviation may be calculated to assess performance stability.</li>\n",
    "        </ul>\n",
    "    <h2>Comparing Multiple Machine Learning Models Using Cross-Validation</h2>\n",
    "        <p>When comparing multiple machine learning models, cross-validation is a powerful tool:\n",
    "            <ul>\n",
    "                <li><b>Consistent Evaluation</b>: It ensures that all models are evaluated consistently using the same cross-validation process and evaluation metric(s).</li>\n",
    "                <li><b>Robust Comparison</b>: Models are evaluated on multiple data subsets, reducing the impact of random variations in data splits. This makes model comparison more robust.</li>\n",
    "                <li><b>Model Selection</b>: Cross-validation helps select the best-performing model based on average performance metrics. The model with the highest average metric is typically chosen.</li>\n",
    "                <li><b>Hyperparameter Tuning</b>: It facilitates hyperparameter tuning for each model within the cross-validation loop, allowing for an optimized model selection process.</li>\n",
    "            </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_best_model(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 46)\n",
    "    lr = LogisticRegression(random_state = 41)\n",
    "    param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "    'max_iter': [25, 50, 75, 100, 200, 300, 400, 500],\n",
    "    }\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv = 5, scoring = 'accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    lr_best = grid_search.best_params_\n",
    "    best_model_lr = grid_search.best_estimator_\n",
    "    return lr_best, best_model_lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec3b187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lr_best_model(X,y):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 46)\n",
    "#     lr = LogisticRegression(random_state = 41)\n",
    "#     param_grid = {\n",
    "#     'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "#     'penalty': [None, 'l1', 'l2', 'elasticnet'],\n",
    "#     'max_iter': [25, 50, 75, 100, 200, 300, 400, 500],\n",
    "#     }\n",
    "#     grid_search = GridSearchCV(lr, param_grid, cv = 5, scoring = 'accuracy')\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "#     lr_best = grid_search.best_params_\n",
    "#     best_model_lr = grid_search.best_estimator_\n",
    "#     return lr_best, best_model_lr\n",
    "\n",
    "\n",
    "# lr_best, best_model_lr = lr_best_model(X,y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765920b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9de1a5c",
   "metadata": {},
   "source": [
    "<dl>\n",
    "    <dt><strong>Accuracy:</strong></dt>\n",
    "    <dd>\n",
    "        <p>Accuracy is a commonly used metric for evaluating the overall performance of a classification model. It measures the proportion of correctly predicted instances (both true positives and true negatives) among all instances in the dataset. In other words, accuracy answers the question:</p>\n",
    "        <blockquote>\n",
    "            \"Of all the instances in the dataset, how many were correctly classified by the model?\"\n",
    "        </blockquote>\n",
    "        <p>The formula for accuracy is:</p>\n",
    "        <blockquote>\n",
    "            <code>Accuracy = (TP + TN) / (TP + TN + FP + FN)</code>\n",
    "        </blockquote>\n",
    "        <p>Where:</p>\n",
    "        <ul>\n",
    "            <li><strong>TP (True Positives):</strong> The number of correctly predicted positive instances.</li>\n",
    "            <li><strong>TN (True Negatives):</strong> The number of correctly predicted negative instances.</li>\n",
    "            <li><strong>FP (False Positives):</strong> The number of instances incorrectly predicted as positive when they are actually negative.</li>\n",
    "            <li><strong>FN (False Negatives):</strong> The number of instances incorrectly predicted as negative when they are actually positive.</li>\n",
    "        </ul>\n",
    "        <p>Accuracy provides an overall measure of how well a model performs across all classes. It ranges from 0 to 1, with higher values indicating better performance. However, accuracy may not be the best metric when dealing with imbalanced datasets, where one class significantly outnumbers the other, as it can be misleading. In such cases, it's essential to consider additional metrics like precision, recall, and the F1-score to gain a more comprehensive understanding of the model's performance.</p>\n",
    "    </dd>\n",
    "</dl>\n",
    "<dl>\n",
    "    <dt><strong>Precision:</strong></dt>\n",
    "    <dd>\n",
    "        <p>Precision is a metric used to evaluate the accuracy of a classification model, particularly in binary classification problems. It measures the proportion of true positive predictions (correctly predicted positive instances) among all instances predicted as positive. In other words, precision answers the question:</p>\n",
    "        <blockquote>\n",
    "            \"Of all the instances the model predicted as positive, how many were actually positive?\"\n",
    "        </blockquote>\n",
    "        <p>The formula for precision is:</p>\n",
    "        <blockquote>\n",
    "            <code>Precision = TP / (TP + FP)</code>\n",
    "        </blockquote>\n",
    "        <p>Where:</p>\n",
    "        <ul>\n",
    "            <li><strong>TP (True Positives):</strong> The number of correctly predicted positive instances.</li>\n",
    "            <li><strong>FP (False Positives):</strong> The number of instances incorrectly predicted as positive when they are actually negative.</li>\n",
    "        </ul>\n",
    "        <p>Precision is useful when the cost of false positives is high. For example, in medical diagnoses, you want a high precision to avoid unnecessary treatments for healthy patients.</p>\n",
    "    </dd>\n",
    "    <dt><strong>Recall:</strong></dt>\n",
    "<dd>\n",
    "    <p>Recall, also known as sensitivity or true positive rate, is a metric that measures the ability of a classification model to correctly identify all relevant instances of the positive class. It answers the question:</p>\n",
    "    <blockquote>\n",
    "        \"Of all the actual positive instances, how many did the model correctly predict as positive?\"\n",
    "    </blockquote>\n",
    "    <p>The formula for recall is:</p>\n",
    "    <blockquote>\n",
    "        <code>Recall = TP / (TP + FN)</code>\n",
    "    </blockquote>\n",
    "    <p>Where:</p>\n",
    "    <ul>\n",
    "        <li><strong>TP (True Positives):</strong> The number of correctly predicted positive instances.</li>\n",
    "        <li><strong>FN (False Negatives):</strong> The number of instances incorrectly predicted as negative when they are actually positive.</li>\n",
    "    </ul>\n",
    "    <p>Recall is valuable when missing positive cases has a high cost. For example, in disease detection, you want high recall to ensure that all actual cases of the disease are identified, even if it leads to some false alarms.</p>\n",
    "</dd>\n",
    "\n",
    "<dt><strong>F1-Score:</strong></dt>\n",
    "<dd>\n",
    "    <p>The F1-score is a metric that combines precision and recall into a single value, providing a balance between these two measures. It is especially useful when you want to consider both false positives and false negatives. The F1-score is the harmonic mean of precision and recall and is defined as:</p>\n",
    "    <blockquote>\n",
    "        <code>F1-Score = 2 * (Precision * Recall) / (Precision + Recall)</code>\n",
    "    </blockquote>\n",
    "    <p>The F1-score ranges between 0 and 1, with higher values indicating better model performance. It is particularly helpful when you want to strike a balance between precision and recall, and there is an uneven class distribution or when false positives and false negatives have different implications.</p>\n",
    "</dd>\n",
    "\n",
    "<dt><strong>Support:</strong></dt>\n",
    "<dd>\n",
    "    <p>Support refers to the number of instances in the dataset that belong to a particular class. In classification reports, support indicates how many actual instances are in each class (positive and negative). It provides context for precision, recall, and F1-score values. For example, if you have a large support for the negative class and a small support for the positive class, it indicates class imbalance.</p>\n",
    "</dd>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f77cfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bb1f68c",
   "metadata": {},
   "source": [
    "<h1>Model Training and Evaluation:</h1>\n",
    "\n",
    "<p>Once you have selected the models, the next step is to train them using the training set and evaluate their performance on the test set. This process allows you to compare the models and determine their effectiveness in predicting the presence or absence of cardiovascular disease. Here's how you can train and evaluate the models:</p>\n",
    "<ol>\n",
    "  <li><b>Initialize the Models:</b> Instantiate each selected model with its respective hyperparameters. You can refer to the documentation of the specific models for guidance on setting the hyperparameters.</li>\n",
    "  <li><b>Train the Models:</b> Fit each model to the training set by using the <code>fit()</code> function. This step involves learning the patterns and relationships in the training data.</li>\n",
    "  <li><b>Make Predictions:</b> Use the trained models to make predictions on the test set by using the <code>predict()</code> function. This step applies the learned patterns to unseen data.</li>\n",
    "  <li><b>Evaluate Model Performance:</b> Compare the predicted labels with the true labels of the test set. Use appropriate evaluation metrics to assess the models' performance. Commonly used evaluation metrics for binary classification include:\n",
    "    <ul>\n",
    "      <li><b>Accuracy:</b> The proportion of correctly predicted instances over the total number of instances.</li>\n",
    "      <li><b>Precision:</b> The proportion of true positive predictions over the total predicted positive instances. It measures the model's ability to avoid false positives.</li>\n",
    "      <li><b>Recall:</b> The proportion of true positive predictions over the total actual positive instances. It measures the model's ability to identify positive instances correctly.</li>\n",
    "      <li><b>F1-score:</b> The harmonic mean of precision and recall. It provides a balanced measure of the model's performance.</li>\n",
    "    </ul>\n",
    "    You can use the <code>classification_report()</code> function from scikit-learn to generate a report containing these evaluation metrics for each model.\n",
    "  </li>\n",
    "</ol>\n",
    "<p>By evaluating the performance of each model using these metrics, you can determine which models perform better in predicting the presence or absence of cardiovascular disease. It's important to note that the choice of evaluation metrics should align with the specific goals and requirements of your project.</p>\n",
    "<p>The scikit-learn library provides a comprehensive set of functions and utilities for model training, prediction, and evaluation, making it easier to perform these steps. Remember to document the evaluation results for each model to compare their performance and guide further analysis and selection.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4deeb6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39f788cd",
   "metadata": {},
   "source": [
    "<h1>Hyperparameter Tuning:</h1>\n",
    "\n",
    "<p>Hyperparameter tuning is a critical step in optimizing the performance of your models. Hyperparameters are parameters that are set prior to training and affect the model's learning process. Exploring different combinations of hyperparameters can significantly impact the performance of your models. Here's how you can perform hyperparameter tuning using techniques like grid search or random search:</p>\n",
    "<ol>\n",
    "  <li><b>Identify Hyperparameters:</b> Determine the hyperparameters of the models that you want to tune. These can include parameters like learning rate, regularization strength, maximum tree depth, or number of estimators.</li>\n",
    "  <li><b>Define the Search Space:</b> Specify the range of values or possible options for each hyperparameter. For example, you can define a list of potential learning rates or a range of tree depths.</li>\n",
    "  <li><b>Choose a Search Method:</b> Decide whether to use grid search or random search. Grid search exhaustively searches through all combinations of hyperparameters, while random search randomly selects combinations from the defined search space.</li>\n",
    "  <li><b>Perform Hyperparameter Search:</b> Use the <code>GridSearchCV</code> or <code>RandomizedSearchCV</code> functions from scikit-learn to perform the hyperparameter search. These functions automatically train and evaluate models with different hyperparameter combinations, allowing you to find the optimal set of hyperparameters.</li>\n",
    "  <li><b>Evaluate Results:</b> Analyze the performance of the models with different hyperparameter combinations. Compare the evaluation metrics (such as accuracy, precision, recall, or F1-score) obtained from the hyperparameter search to identify the best-performing model.</li>\n",
    "  <li><b>Select Best Hyperparameters:</b> Choose the hyperparameter combination that yields the best performance on the evaluation metrics. Use this set of hyperparameters to train your final model.</li>\n",
    "</ol>\n",
    "<p>Hyperparameter tuning helps fine-tune your models to achieve better performance. It is important to strike a balance between exploration (trying out a wide range of hyperparameters) and exploitation (selecting the best performing hyperparameters) based on the computational resources and time available.</p>\n",
    "<p>By systematically exploring different hyperparameter combinations, you can optimize the performance of your models and improve their ability to predict the presence or absence of cardiovascular disease.</p>\n",
    "<p>Remember to document the hyperparameters searched, the corresponding evaluation results, and the chosen optimal hyperparameters for each model. This documentation can serve as a reference for reproducibility and future improvements.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068afbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90161fee",
   "metadata": {},
   "source": [
    "<h1>Model Comparison and Selection:</h1>\n",
    "\n",
    "<p>After evaluating the performance of different models and tuning their hyperparameters, the next step is to compare the models and select the best-performing one for your task. Here's how you can perform model comparison and selection:</p>\n",
    "<ol>\n",
    "  <li><b>Evaluate Performance:</b> Review the evaluation metrics obtained from training and evaluating each model. Consider metrics such as accuracy, precision, recall, and F1-score to assess how well the models perform in predicting the presence or absence of cardiovascular disease.</li>\n",
    "  <li><b>Compare Results:</b> Compare the performance of the models based on the evaluation metrics. Look for patterns or consistent trends in their performance. Consider the impact of hyperparameter tuning on the models' performance.</li>\n",
    "  <li><b>Select the Best Model:</b> Based on the evaluation results, choose the model that exhibits the highest performance and meets your specific project requirements. This model is likely to provide the most accurate predictions for your cardiovascular disease classification task.</li>\n",
    "</ol>\n",
    "<p>Remember to consider factors beyond just the evaluation metrics. Take into account the interpretability of the model, computational complexity, and the feasibility of implementation. You may also want to consider the model's robustness, generalization ability, and its performance on different subsets or validation sets.</p>\n",
    "<b>Further Analysis:</b>\n",
    "\n",
    "<p>Once you have selected the best-performing model, you can dive deeper into its results to gain additional insights. Here are some steps you can take for further analysis:</p>\n",
    "<ol>\n",
    "  <li><b>Feature Importance:</b> Analyze the importance of each feature in the selected model. Determine which features have the most significant impact on the prediction of cardiovascular disease. This can be done by examining the feature importance scores provided by certain models or by using techniques such as permutation importance or SHAP (SHapley Additive exPlanations).</li>\n",
    "  <li><b>Feature Relationships:</b> Investigate the relationships between the features and the target variable in the best model. Explore how different features interact with each other and how they contribute to the prediction of cardiovascular disease. Visualizations, such as scatter plots or partial dependence plots, can help reveal these relationships.</li>\n",
    "  <li><b>Domain Insights:</b> Consult with domain experts or medical professionals to gain additional insights into the identified important features and their relationships with cardiovascular disease. Their expertise can provide valuable context and help validate the findings from your analysis.</li>\n",
    "</ol>\n",
    "<p>By conducting further analysis, you can gain a deeper understanding of the factors influencing the prediction of cardiovascular disease and potentially discover additional insights that can be used for future research or intervention strategies.</p>\n",
    "<p>Document your findings and observations from the model comparison and further analysis steps. These insights can contribute to the interpretability of the chosen model and provide valuable information for reporting or presenting your results.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aad5093",
   "metadata": {},
   "source": [
    "<h1>Model Deployment and Monitoring:</h1>\n",
    "\n",
    "<p>If you wish to deploy the chosen model into a production environment, you can follow these steps for model deployment and monitoring:</p>\n",
    "<ol>\n",
    "  <li><b>Prepare the Model for Deployment:</b> Serialize and save the trained model along with any necessary preprocessing steps or feature encodings. This ensures that the model can be easily loaded and used in a production environment.</li>\n",
    "  <li><b>Integrate the Model:</b> Incorporate the model into the production environment. This may involve integrating the model into an existing software system, developing an API for model predictions, or setting up a real-time streaming system for continuous predictions.</li>\n",
    "  <li><b>Perform Quality Assurance:</b> Before deployment, thoroughly test the integrated model to ensure its proper functioning and accuracy. Use a validation dataset or a set of simulated inputs to verify that the model produces the expected outputs.</li>\n",
    "  <li><b>Monitor Model Performance:</b> Continuously monitor the deployed model's performance in the production environment. Track prediction accuracy, response time, and any relevant performance metrics. Set up monitoring systems to raise alerts if the model's performance deteriorates or deviates from expected thresholds.</li>\n",
    "  <li><b>Update and Retrain the Model:</b> Regularly assess the model's performance and consider retraining or updating the model if necessary. As new data becomes available, periodically retrain the model using fresh data to ensure it remains up to date and continues to deliver accurate predictions.</li>\n",
    "  <li><b>Data Governance and Privacy:</b> Adhere to data governance and privacy regulations when deploying and using the model. Ensure that the model is compliant with relevant privacy laws and handle sensitive data responsibly.</li>\n",
    "</ol>\n",
    "<p>Model deployment and monitoring allow you to apply the chosen model to real-world scenarios and continuously assess its effectiveness. Monitoring the model's performance helps detect any potential issues, such as concept drift or degradation in predictive performance, and take appropriate actions to maintain its accuracy.</p>\n",
    "<p>Additionally, consider creating documentation or user guides that outline how to use the deployed model, its input/output format, and any limitations or assumptions associated with its predictions.</p>\n",
    "<p>Remember to establish a feedback loop with domain experts and stakeholders to gather insights, assess user satisfaction, and make iterative improvements to the deployed model as needed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183bd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a7864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
